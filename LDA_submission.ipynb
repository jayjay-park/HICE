{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://github.com/Kaustubh-Tambe/BBC-News_Topic-Modelling/blob/main/Notebook/BBC_News_Topic_Modelling_Project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from gensim) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim\n",
    "# conda install -c anaconda gensim\n",
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (3.4.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyLDAvis in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: sklearn in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from pyLDAvis) (0.0.post1)\n",
      "Requirement already satisfied: scipy in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: gensim in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from pyLDAvis) (4.2.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: setuptools in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: funcy in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: joblib in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: future in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numexpr in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /storage/home/hcocice1/jpark3141/.local/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /storage/pace-apps/manual/packages/anaconda-individual/2021.05/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Genism\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy for Lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preparing stopwords from NLTK & extended_stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /storage/home/hcocice1/jpark3141/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# add more stopwords\n",
    "file = open(\"./extended_stopwords.txt\", \"r\")\n",
    "stop_words.extend([line.strip() for line in file.readlines()])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting it to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./2016-01-posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1654555 entries, 0 to 1654554\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count    Dtype \n",
      "---  ------        --------------    ----- \n",
      " 0   subreddit_id  1654555 non-null  object\n",
      " 1   selftext      580175 non-null   object\n",
      " 2   author        1654555 non-null  object\n",
      " 3   over_18       1654555 non-null  bool  \n",
      " 4   ups           1654555 non-null  int64 \n",
      " 5   created_utc   1654555 non-null  int64 \n",
      " 6   score         1654555 non-null  int64 \n",
      " 7   downs         1654555 non-null  int64 \n",
      " 8   title         1654554 non-null  object\n",
      " 9   num_comments  1654555 non-null  int64 \n",
      " 10  subreddit     1654555 non-null  object\n",
      " 11  quarantine    1654555 non-null  bool  \n",
      "dtypes: bool(2), int64(5), object(5)\n",
      "memory usage: 129.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>ups</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>downs</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>quarantine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_2qupf</td>\n",
       "      <td>Does anyone have the Jupiter Ascending script?...</td>\n",
       "      <td>Nonsuch42</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>1451606400</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[REQUEST] Jupiter Ascending script?</td>\n",
       "      <td>15</td>\n",
       "      <td>Screenwriting</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_2qi58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>historyworkisboring</td>\n",
       "      <td>False</td>\n",
       "      <td>261</td>\n",
       "      <td>1451606401</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>Cristiano Ronaldo: \"We cannot live being obses...</td>\n",
       "      <td>139</td>\n",
       "      <td>soccer</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_2qhqb</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>ReadsStuff</td>\n",
       "      <td>False</td>\n",
       "      <td>47</td>\n",
       "      <td>1451606401</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy New Year /r/UnitedKingdom</td>\n",
       "      <td>17</td>\n",
       "      <td>unitedkingdom</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5_2r344</td>\n",
       "      <td>This thread is for you to promote your blog / ...</td>\n",
       "      <td>ranalog</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>1451606403</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Monthly 'Self Promotion' - January</td>\n",
       "      <td>51</td>\n",
       "      <td>analog</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5_2qpp6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brandhor</td>\n",
       "      <td>False</td>\n",
       "      <td>732</td>\n",
       "      <td>1451606404</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>MLG sells “substantially all” assets to Activi...</td>\n",
       "      <td>285</td>\n",
       "      <td>starcraft</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id                                           selftext  \\\n",
       "0     t5_2qupf  Does anyone have the Jupiter Ascending script?...   \n",
       "1     t5_2qi58                                                NaN   \n",
       "2     t5_2qhqb                                          [removed]   \n",
       "3     t5_2r344  This thread is for you to promote your blog / ...   \n",
       "4     t5_2qpp6                                                NaN   \n",
       "\n",
       "                author  over_18  ups  created_utc  score  downs  \\\n",
       "0            Nonsuch42    False   11   1451606400     11      0   \n",
       "1  historyworkisboring    False  261   1451606401    261      0   \n",
       "2           ReadsStuff    False   47   1451606401     47      0   \n",
       "3              ranalog    False   11   1451606403     11      0   \n",
       "4             Brandhor    False  732   1451606404    732      0   \n",
       "\n",
       "                                               title  num_comments  \\\n",
       "0                [REQUEST] Jupiter Ascending script?            15   \n",
       "1  Cristiano Ronaldo: \"We cannot live being obses...           139   \n",
       "2                    Happy New Year /r/UnitedKingdom            17   \n",
       "3                 Monthly 'Self Promotion' - January            51   \n",
       "4  MLG sells “substantially all” assets to Activi...           285   \n",
       "\n",
       "       subreddit  quarantine  \n",
       "0  Screenwriting       False  \n",
       "1         soccer       False  \n",
       "2  unitedkingdom       False  \n",
       "3         analog       False  \n",
       "4      starcraft       False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit_id          0\n",
      "selftext        1074380\n",
      "author                0\n",
      "over_18               0\n",
      "ups                   0\n",
      "created_utc           0\n",
      "score                 0\n",
      "downs                 0\n",
      "title                 1\n",
      "num_comments          0\n",
      "subreddit             0\n",
      "quarantine            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum()) # selftext has 1074380 Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 580175 entries, 0 to 1654554\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   subreddit_id  580175 non-null  object\n",
      " 1   selftext      580175 non-null  object\n",
      " 2   author        580175 non-null  object\n",
      " 3   over_18       580175 non-null  bool  \n",
      " 4   ups           580175 non-null  int64 \n",
      " 5   created_utc   580175 non-null  int64 \n",
      " 6   score         580175 non-null  int64 \n",
      " 7   downs         580175 non-null  int64 \n",
      " 8   title         580175 non-null  object\n",
      " 9   num_comments  580175 non-null  int64 \n",
      " 10  subreddit     580175 non-null  object\n",
      " 11  quarantine    580175 non-null  bool  \n",
      "dtypes: bool(2), int64(5), object(5)\n",
      "memory usage: 49.8+ MB\n",
      "580175\n"
     ]
    }
   ],
   "source": [
    "# delete Nan from selftext\n",
    "updated_df = df.dropna(axis=0)\n",
    "\n",
    "updated_df.astype({'selftext':'string'}).dtypes\n",
    "updated_df.info()\n",
    "\n",
    "# delete [removed]\n",
    "print(len(updated_df))\n",
    "#count = [c for c in updated_df['selftext'] if \"[removed]\" in c]\n",
    "#print(len(count)) # 15552 rows of [removed]\n",
    "#filtered_df = updated_df[updated_df['selftext'].str.contains(\"[removed]\") == False]\n",
    "#wrong = updated_df[updated_df['selftext'].str.contains(\"[removed]\") == True]\n",
    "#print(len(filtered_df))\n",
    "#print(len(wrong))\n",
    "#df['selftex'] = df['selftext'].map(lambda x: x.lower())\n",
    "#filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          does anyone have the jupiter ascending script?...\n",
       "2                                                  [removed]\n",
       "3          this thread is for you to promote your blog / ...\n",
       "7          ### this thread is for serious discussion of t...\n",
       "10                                                 [deleted]\n",
       "                                 ...                        \n",
       "1654545    i'm currently reading a book on wwi and came a...\n",
       "1654549    joe speaks highly about justin trudeau and say...\n",
       "1654552    i'm probably going to buy $100 into bitcoin, f...\n",
       "1654553    hey y'all, welcome back to my semi-gem ch coll...\n",
       "1654554                                    it's pretty good.\n",
       "Name: selftext, Length: 580175, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df['selftext'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does anyone have the Jupiter Ascending script? I doubt it, but I really enjoyed the movie and would love to see how it was presented in script form, especially since there is so clearly connective tissue missing from the final product. Thanks!\n"
     ]
    }
   ],
   "source": [
    "# converting DataFrame column into List\n",
    "data = updated_df['selftext'].tolist()\n",
    "\n",
    "# Remove url\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "#pprint(data[:1])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Data\n",
    "def sent_to_words(sents, deacc=True):\n",
    "    for sentence in sents:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does', 'anyone', 'have', 'the', 'jupiter_ascending', 'script', 'doubt', 'it', 'but', 'really', 'enjoyed', 'the', 'movie', 'and', 'would', 'love', 'to', 'see', 'how', 'it', 'was', 'presented', 'in', 'script', 'form', 'especially', 'since', 'there', 'is', 'so', 'clearly', 'connective_tissue', 'missing', 'from', 'the', 'final', 'product', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "# Defining the Bigram and Trigram Model\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions for removing stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#creating fuctions for making bigram\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#creating fuctions for making trigram\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "#creating fuctions for Lemmitization\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #corpus will have the words that belong to only these part of speech\n",
    "    \n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using all the Functions Created for Preprocessing\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Removing Stop Words\n",
    "# sometimes Lemma can be equivalent to the stopwords... \n",
    "data_words_nostops = remove_stopwords(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LDA for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are trying to get the optimal model according to the Coherence score(meseaure of Separability) to figure out no of topics ..\n",
    "def tune_model(dictionary, corpus, texts, limit, start, step):\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True,\n",
    "                                           )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(round(coherencemodel.get_coherence(),3))\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run\n",
    "from tqdm import tqdm\n",
    "model_list, coherence_values = tune_model(dictionary=id2word, co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Line Graph for Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=10; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Coherence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should Figure out our maximum Coherence Value!! -> LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we knew that the coherence score is maximum for 6 topics so that will become our optimal model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dominant words for each topics \n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster topics and see the dominant words in graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization libraries \n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Word Cloud\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation for wordcloud\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "topic_words = dict(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3 , figsize=(15,10), sharex=True, sharey=True)\n",
    "#fig.delaxes(ax[1,1])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4909a6f98c8ba87732415580af1fe424aedbcedb4a84f7ee2e27fa7a2ea53689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
