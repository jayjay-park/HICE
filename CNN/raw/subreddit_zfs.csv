,controversiality,parent_id,body,subreddit,id,score,subreddit_id
1120049,0,t1_chvluio,How can I check if disks announces itself as b lets say under OmniOS?Im not looking to run VMs on this server this will be a file server ZFS NFS CIFS virtualized on top of ESXi ..,zfs,chvlyth,1,t5_2ruui
1134451,0,t1_chx16tz,what are you trying to see how little you can get by with? you already were told that a gig.how much will zfs use? all you give it,zfs,chx8hop,1,t5_2ruui
1854587,0,t1_ckco1x8,I always agreed with everything you said in this last comment.But you just proved yourself that the blocks are striped across potentially all disks within a vdev which is what Ive been saying the whole time...  And this increases throughput based on the sheer number of disks no matter the RAIDZ configuration.This is how it works in standard RAID as well not just RAIDZ.Many users do not need IOP performance above  disk.  And if you are particularly interested in IOP performance you should be using striped mirrors anyway not RAIDZ.  I mentioned this in my initial long comment which is at the top right now.,zfs,ckco8jc,1,t5_2ruui
2398614,0,t1_clp5ot5,If this is correct then it makes a resilvering option much more convenient that doing some manual ie. error prone sendrecv.Im also thinking about doing the same thing.So basically you would have a  disk mirror pool with one of them always offsite? And at all times the pool has  or  online disks?,zfs,cm86d9s,2,t5_2ruui
2953768,0,t3_2txfol,when you rsync with two local paths it will always copy entire files.you must have an rsyncd on one side of the transfer to enable infile deltas.,zfs,co3uvsq,1,t5_2ruui
3146864,0,t3_2w73yf,If youre hitting the ARC youll already saturate the link or close to it.If youre doing large mostlycontiguous reads youll do MBsec.If youre doing a lot of small offARC IO  database ops tons of tiny files highly fragmented storage  you arent going to come close to saturating that link  probably not even if you tore everything out and replaced it with eight full solid state disks configured as mirror vdevs.So basically the answer is no.  Your best bet would be to cram as much RAM as humanly possible into the box and hope to stay onARC for as much as possible.  Given that this is a home setup I seriously doubt a ZIL or LARC would really do you much good.gt Would splitting the pool into x disk vdevs help with performance?Wait  I thought you said you had  disks total?  Did you mean ?You absolutely shouldnt have a single  disk wide stripe.  If you do yes you should split that up.  Either three sixdisk RAIDZ vdevs or preferably if you care about performance  mirror vdevs.,zfs,coo63hx,4,t5_2ruui
4535615,0,t1_ct264fx,Too bad there isnt any New England openings.,zfs,ct2ch8w,1,t5_2ruui
4661449,0,t1_cti1gwt,Youre right the post is wrong about async writes not hitting the ZIL.  The transaction records are written out to the ZIL before txgsync and large gtKB data blocks will wind up getting sync written with a pointer to it in the ZIL.However doesnt ZFS effectively sequentialize random IO  ,zfs,cti4rag,1,t5_2ruui
5094412,0,t1_cuztokd,Far more importantly holding sync snapshots.,zfs,cv02frt,1,t5_2ruui
5506936,0,t1_cwfje7q,Sounds like a reasonable plan. ,zfs,cwfk8gi,1,t5_2ruui
5678568,0,t1_cx0z1q6,This is not for everyone however having an identical image if your sdcards could be beneficial to a professional who wants to archive their data. I think most people here are looking at this and t thinking of how hard it is to find anything. But the purpose is to have as near copies of the sdcards as possible which is served better at the block level than file system level. You could even go further and use ddrescue or ddrescue.If you want to dive deep learning about IT forensics applies much to this use case.,zfs,cx0zdjv,1,t5_2ruui
5810018,0,t1_cxh5upu,WD red is an excellent choice. Consider red pro if you can afford it given how many drives youll be using and particularly given that I am not smelling any backups in your future.Youll be tempted by the high storage efficiency of raidz or even raidz. I strongly advise you not to do that though. Performance aside its very difficult and irritating to try to upgrade a striped array and a few years down the road youre definitely going to be wanting to upgrade a little at a time. For that reason I recommend a pool of mirrors. Exhaustive analysis here Since youre starting off fresh you definitely should go with a machine that supports ECC ram. The price difference really isnt that high. This could be an avoton core atom soc or it could be a Xeon ev. This assumes youre buying new. If you dont mind scrounging you could probably save a lot of money digging on eBay or the hardocp forums.If you find some super free or almost free hardware that didnt support ECC... Its okay. Use it. Dont BUY anything that didnt support ECC because the price difference isnt much and ECC is good. But if its a case of this is the hardware I have dont sweat it. There will be people who will tell you that ZFS without ECC is worse than a conventional filesystem without ECC. They are wrong. Your motherboard SATA should be fine. But if you need more ports you need an HBA  host bus adapter  card NOT repeat NOT a RAID controller. SuperMicro makes port HBA cards of excellent quality for about .  LSI cards are very popular with the freeNAS crowd but be very careful  they arent all the right kind and even the ones that are need to be flashed with the right firmware.  Id really recommend sticking with cards that only with the way they should work for the application that there is no uncertainty about.Cases are a tough one. Up to  bays theyre a dime a dozen. But xtb  tb raw. Cut that in half for redundancy and youre looking at only tb which would be  full from day one with your tb of stuff. So you should be looking at bigger cases. SuperMicro makes a really solid  bay chassis for . Probably out of your price range so look at norco rpc or .  TEST ALL BAYS IMMEDIATELY if you get one of those though because the QC hasnt been super consistent on them. If something is bad on your backplane you want to know IMMEDIATELY so you can fix it under warranty and before youre depending on it not a year or two down the road.,zfs,cxhadd7,1,t5_2ruui
6146605,0,t1_cyo9zvu,up i agree. this is all at home tho. I plan on upgrading the server itself. will replace with supermicro something. i just dont know what yet. dual processor probably for sure but rack mount anything means too much noise. trying to decide on Xeon E E E. I want to stay newer than xx but all expensive still. i think am narrowing on a targe of at least as new as ivybridge stuff. i am beginning to believe the memory speeds on this current box is a bottle neck.,zfs,cyoayx8,1,t5_2ruui
6495231,0,t3_44k9cv,gt Are MLC and TLC Samsung SSDs good enough to use as a ZIL and read cache? Will I end up killing the SSDs after a few months of beating them up?My inner pedant demanded I ask you please dont call it a ZIL if you mean a SLOG. The ZIL always exists even if you dont designate a special device a Separate LOG device is its own thing.At least you didnt call it a write cache. That one is sure to set me off on a tenminute rant.THE THEORYFor SLOG heres the most important bit that is commonly ignored ZFS wont use more than  of your RAM for SLOG data unless you recompile it to do so. Thats it.Assume you have GB RAM. Your intent log wont ever be larger than GB. So if you buy a shiny GB SSD for your SLOG all you need is one GB partition on it and you can use the rest as overprovisioning space to enhance your write endurance. A SLOG will absolutely beat the crap out of a SSD but if youre so vastly overprovisioned that every write only uses  of the endurance it would if fullyprovisioned and that takes your expected lifetime from  years to  who cares?My biggest beef with big MLCTLC NAND is that it ends often up dogslow when used as SLOG. Faster than a spinning disk sure but a lot slower than it ought to be or for my applications than it HAS to be. A lot of them use k write alignment in blocks of   or even kb that need to be read before they can be rewritten. Transactions in a transaction group are usually way way way smaller than that. Therefore the SSD controller ends up staying really busy readingfreeing old blocks in e mode because youre writing so much fresh data on busy systems and this can really choke down your appliance synchronous write speeds. Contrary to popular belief there are many synchronous writes to ZFS even if your primary workload is asynchronous.Anyway that MLCTLC has gotta be fast and ideally fairly small so that it doesnt read giant blocks to modify parts of them. Right now there is a dearth of dTrace probes to really peek inside the flash mostly because hard drive amp SSD manufacturers intentionally obfuscate whats going on under the hood so you have to rely on performance testing. And based on mine its often really really worth it to pick up a fast IOPS not throughput small  of your RAM no more needed SLC NAND for SLOG over a bigger slower MLC or TLC NAND right now.USE CASEIn the most typical Oracle IT ZS configuration Oracle uses .TB of RAM per head for a total of TB RAM sixteen writeoptimized supercapacitorbacked GB SLC NAND for SLOG usually mirrored but sometimes striped when needed and  drives for the zpool. The reason for SLC here is clear we have an enormous amount of RAM and need an enormous amount of SSD to drive the SLOG for TB RAM thats TB of SLOG. We purchase LARC SSDs in GB capacity as needed if my LARC hit rate even with that much RAM drops below  but usually we only see those kinds of low hit rates in certain iSCSI environments with heaps of random reads.  That much LARC can take literally days to fully warm up and provide a helpful hit rate in the Teleo amp PaaS Cloud environments.CONCLUSIONTLDR Dont use LARC unless you absolutely positively are sure its going to help your current workload based upon your performance tests. Use a fast small SLC NAND for SLOG if you can.  If you cant afford one then a  RAMsized partition on a bigger MLC device often provides sufficient endurance for home and small business workloads with a caveat the performance is often questionable but better than a ZIL on hard disk.Disclaimer Im an Oracle employee my opinions do not necessarily reflect those of Oracle or its affilaites.,zfs,czr5l49,4,t5_2ruui
6501202,0,t1_czrmf0k,SLOG cannot be shared among pools unless partitioned to do so so I assume your question is about RAM. Thats a great question. I am not certain. Watching the super bowl right now but I will put this on my list to investigate tomorrow!,zfs,czrruld,1,t5_2ruui
6833462,0,t1_d0by64j,Can you let me know if I did this right?  I opened nano pasted your dScript into it saved the file as larc.d  chmoded it to be executable and then did dtrace s lard.dDid I miss anything?,zfs,d0qvh5m,1,t5_2ruui
6914368,0,t1_d101hr8,Ah makes sense. Still doesnt explain why this system thinks thats the scrub time on these zpools though,zfs,d102dtz,1,t5_2ruui
7086227,0,t1_d1j366o,You wouldnt do lz compression on the the full volume? ,zfs,d1jlab8,1,t5_2ruui
7478671,0,t3_4ht25h,What patch did you go from and to? Definitely looks like some post patch changes in ARCLARC. Which is impacting you refeed and thus pounding on the disks. ,zfs,d2sa5t5,1,t5_2ruui
7554672,0,t1_d30dbr9,Bah. Call me when you write it in Lisp ,zfs,d30xhva,1,t5_2ruui
7595605,0,t3_4jbknk,Two big changes youve made is the SATA controller and the NIC either of which on a budget Pentium system could have been implemented with cheaper parts with poor Linux compatibility. Id test it with a known good PCIe SATA andor network cards if have them lying around. ,zfs,d35kymn,3,t5_2ruui
7615861,0,t3_4jm85k,ZFS doesnt care about files for deduplication only blocks. This is a good read for understanding how it all works.,zfs,d37vtmu,6,t5_2ruui
7643505,0,t3_4jzuhh,Thats silent data corruption for you. There wont be any trace of the issue outside of ZFS. This is not necessarily common but it is not something Id be too worried about unless it happens often.,zfs,d3b0x0v,10,t5_2ruui
7706533,0,t1_d2iu4py,Im not sure if I didnt see your response or what...but this is very good information for me.  I hadnt been able to find a formula for that anywhere!  Im not fully clear though as your example appears to be in bytes though the formula you listed in kilobytes.  Did you mean     LARC in bytes  record size in bytes   bytes  ARC Header in bytes  OR LARC in bytes  record size in kilobytes   bytes  ARC Header in bytes  ??  With a GB LARC device in my system  TB drives consisting of  drive raidz vdevs K recordsize ARC is set to GB system has GB RAM formula  I come up with only .MB memory required for LARC which seems way too low   Bytes GB SSD   Bytes recordsize   Bytes   Bytes ARC Headers size in RAM  With formula  I come up with .GB memory required for a GB LARC which seems more likely but still higher than I expected   Bytes GB SSD   Kilobytes recordsize   Bytes   Bytes ARC Headers size in RAM  Assuming formula  was correct and I had a K recordsize instead of my K a GB LARC device would consume TB of RAM so that cant be right.  With formula  a GB LARC and K recordsize would consume GB of RAM which does sound more reasonable.  So assuming  is right and a GB LARC would only consume .MB of RAM shouldnt I just go big on my LARC?  Like  TB SSDs as cache drives my understanding is that they would be used in JBOD which would still only use like GB of RAM.  Whaa??? That cant be right.  Do you have any linksdocumentation that mention that formula?  Id love to actually have something concrete to go off of.,zfs,d3i6jgf,1,t5_2ruui
7766865,0,t1_d3ozyv0,Assuming theyre read heavy databases and that the reads are truly very hot across many tens of gigabytes of blocks that certainly sounds like a likely use case for larc.Honestly though it sounds like a better case for all ssd to begin with. gb or so especially a highly compressible gb or so which you seem to be implying here isnt expensive to fit directly on solid state and will absolutely crush the performance of a rust pool SLOGlarc or no.You should also really consider upgrading the machine itself so you can cram in more RAM if performance is an issue. Skylake i CPUs are cheap and can address gb of RAM. Better yet so can the new generation of Xeon es and they can be had for about the cost of an i.,zfs,d3p14h5,1,t5_2ruui
8213372,0,t1_d52zq7u,It flushes IO and locks the tables while you snapshot it and should ensure that the snap is consistent. While its the simplest mechanism to achieve a fullyconsistent MySQL database it plays merry hell with performance.  ZFS snapshots quickly but I wouldnt do this to an OLTP or OLAP database for that reason.,zfs,d548596,2,t5_2ruui
8271500,0,t1_d5bhmzk,I guess Im just surprised that theres a  overhead for this but if its normal its normal.thanks for the confirmation and sanity check!,zfs,d5bhvxb,2,t5_2ruui
8439699,0,t1_d5wfcwa,Honestly its not a difficult job but its probably one that belongs in its own bespoke tool.  I dont know how widely useful itd be too many people would likely want to remove SOME orphaned datasets but not ALL orphaned etc.Practically its not too difficult as a oneoff.  Sloppy quasicode     !usrbinperl w       use strict        assume were running this script on the target        and checking the source remotely.            my sourcesnaps        populate a hash with pooldataset and all its children         on the source machine            open FH ssh rootbox zfs list r pooldataset |       foreach my snapltFHgt          sourcesnapsnap              close FH               iterate through pooldataset and all its children locally        and get rid of anything that didnt exist on the source       open FH zfs list r pooldataset |       foreach my snapltFHgt          unless exists sourcesnapssnap            system zfsdestroyrsnap                Note that this is SLOPPY QUASICODE itd sorta run but just to for instance one bloody obvious for instance zfs list gives humanreadable not scriptable output and to actually do this youd likely want zfs get with the proper arguments.  Should also use multiargument open etc bloody etc.  But you get the idea.,zfs,d5wi20u,2,t5_2ruui
8717247,0,t1_d6v6jkd,I had a disk fail last week I documented how the spare replacement process worked in that situation.,zfs,d6vamq6,1,t5_2ruui
8844410,0,t1_d7avpcu,You can retain the compression settings on the receiving end so that it is stored as .T  .x compression  check out the man page for zfs and look at the options for send and recv.You can speed up the transfer by compressing and slapping mbuffer in the middle if were talking about WAN. Some info on that,zfs,d7az0yc,4,t5_2ruui
8889804,0,t1_d7gfc9a,We cant give up that much space unfortunately. We can lose  disks to parity but not .,zfs,d7gffpq,3,t5_2ruui
8986035,0,t1_d7rx4hy,Ive updated with some more details Ill see about freeing up some space on those zpools.  Would the slow down occur even if the zpools above  are effectively doing nothing?,zfs,d7rzexo,1,t5_2ruui
9107266,0,t3_54y8t1,Once you have the torrents moved somewhere else using MiB recordsize for big files like movies improves performance quite a lot too.,zfs,d86jpd3,1,t5_2ruui
9938376,0,t1_db2xdub,Thanks for the suggestion. For this specific need I need to restore the snapshots. I put details hereampnbspShort version testing how postgresql behaves when it is snapshotted.  So need to actually get snapshot and see the state postgres is when I bring the VM for the snapshot back to live.,zfs,db2zup5,1,t5_2ruui
10088287,0,t1_dbgtnb7,gt theres no performance penalty for higher ashift values.Small niggle RAIDz zpools with excessive ashift values suffer additional performance degradation because the heads have to write in one place longer spindle or sacrifice throughput for small operations both spindle amp SSD.Back when I worked at Oracle this performance degradation and the associated capacity issues with RAIDz were  along with smallblock database performance  primary reasons for carefully choosing technology that supported e performance in excess of older native b drives.  Excellent b support was still one of the reasons for the performance spindles smaller than TB and probably still is.  Maintaining smallblock performance is a reason for choosing SSDs with smaller native block size MLC kn SLC b over larger native implementations TLC kn it pretends to be kn with good firmware but you burn up PE at a little higher rate.These days SSD speeds and firmware are pretty much good enough that most people cannot see the onSSD write combining performance hit with ashift. Hard drives dont really have that leeway except HGSTs TB models with media access mode... Disclaimer I no longer work for Oracle my opinions then and now do not necessarily represent those of Oracle or its affiliates.,zfs,dbmirno,1,t5_2ruui
10215699,0,t1_dc1yl7v,Youre totally right it is a big waste of space. But compared to a pool of mirrors it isnt actually as big a difference. Pool of  ways would be  pool of mirrors is already a miserable .And youre right to get just one extra disk worth of space youd need to add  disks. But in a use case where this would be considered youd be populating all the available disks at once anyway so it wouldnt matter.And for balancing the vdevs in the pool youre right. If youre adding a vdev every time you need space your data is going to be all mixed up and io isnt great. But again since this use case is very niche a server built w this in mind would have all the disks itd ever get from the start. So itd be balanced.It is literally the solution to a very very niche use case. You need amazing io performance and you also need amazing resiliency from disk failure. Almost no one is going to choose this setup.,zfs,dc209qq,1,t5_2ruui
10340806,0,t1_dcf58kx,There remain a few solid performance amp capacity reasons to split up filesystems but despite those for all users other than the few performance or capacityobsessed youre right.,zfs,dcg6to8,1,t5_2ruui
10485274,0,t3_5q5v64,Id use it for test stuff like operating systems or virtualization.  You could also use the platters to make some neat coasters.,zfs,dcwm56l,1,t5_2ruui
10497203,0,t1_dcxrk9x,Performing a secure erase on the disk usually works well for this.,zfs,dcxyvjc,1,t5_2ruui
10645146,0,t1_dddwrbb,Personally I feel dd is a horrible benchmark tool for zfs. Ive always gotten higher speeds with dd then real life.Zeros compressdecompress too easy IF you use compression. I believe you are not correct?You dont really know if it is being async or sync to the disks. Ive seen it do both using the same syntax. I saw that you used zeros on dd you really didnt write that many to disk since you have dedup. Zfs saw identical blocks and reported that it was written when it actually did not.,zfs,dddyr2i,5,t5_2ruui
10652592,0,t1_dde6509,i disabled dedupe. no game. I also upgraded to ashift.,zfs,ddeqeae,1,t5_2ruui
10997515,0,t1_deec194,Number of people ive spoken to dont understand that Oracle Linux doesnt come with ZFS of any kind and it isnt supported by Oracle. So Id grade umjt comment as useful.,zfs,deej994,1,t5_2ruui
11000413,0,t3_55pwxp,Has anyone heard any updates on this ? The compression ratio looks pretty good but I dont see any updates on openzfs.org,zfs,deev4d1,1,t5_2ruui
11128757,0,t1_detdnk1,Im running on Arch Linux. Im just creating new zpools and rsyncing files over its a bit towers of hanoi as I created a new zpool on my old backup drive and Im migrating files to that and then formatting the old drives as zfs and moving files back over I plan to zfs send|zfs receive for the nd step.Im just going to leave  on ext and make media drives my backup drive and probably home as zfs.,zfs,detfo2h,1,t5_2ruui
11131155,0,t3_5yxf1a,It used to be you needed to leave around  space emphasis on used to. I dont know where it is in the documentation but I believe it is more like . Some people report filling up to  and zfs being fine.That being side under any file system you shouldnt be filling your disks to absolute capacity.,zfs,detpfyg,3,t5_2ruui
11383767,0,t3_62dx40,zvols Normally if you take a snapshot it doesnt cost you any storage space to create it. If you use zvols you need to have at least as much space available as the actual size of the zvols.  LinkYou could use Proxmox as your Hypervisor. Its just a modified Debian. That means its easy and transparent to manage and you dont need to create your storage pool in a VM and you dont have to bother with FreeNAS. ,zfs,dfmbwxq,1,t5_2ruui
