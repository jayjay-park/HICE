,controversiality,parent_id,body,subreddit,id,score,subreddit_id
5359737,0,t1_cvwqu5x,Maybe maybe not I figure its going to get bored with nothing around that it cant predict with  certainty.,ControlProblem,cvwrm8d,1,t5_39qub
5381446,0,t1_cvy2y2g,gt If we choose protect humans from harm will it consider us a threat to ourselves and isolate all of us from each other?This is the best example you could possibly give to a layperson.,ControlProblem,cvzjgtw,2,t5_39qub
5383127,0,t1_cvzqq5w,Yep this was the exact concept.  Thanks for digging the link.,ControlProblem,cvzr70p,2,t5_39qub
5384495,0,t1_cvzb9an,Then people will ask it how they can get rich and it will tell them how while the part it doesnt tell them having a side effect of what the AI wanted to happen.,ControlProblem,cvzxhgp,0,t5_39qub
5421779,0,t1_cw4dux0,I would say the opposite that it will take higher than average human intelligence to deeply understand AI  years of work from teams all over the world have already gone into the field which is thousands of manyears away from building a general AI. When it comes about it will almost certainly be the output of a team of very intelligence people who collectively have an intelligence much higher than human level intelligence if there was such a thing. In the grand scheme of things though compared to the superintelligence of a selfimproving AI this will be in the ballpark of human intelligence. Also we would not want to build something with the sole purpose of improving itself. It will only increase its intelligence with the goal of achieving its ultimate goal more effectively. In this sense it will need general intelligence as much as any of us. ,ControlProblem,cw4otmr,2,t5_39qub
5425475,0,t3_3pcazv,The problem imho its that life expectancy could be opposed to quality of life. Our freedom for example allow us to do things against of life expectancy unhealthy food extreme sports or even cross a road. Furthermore there is not a clear definition of quality of life Are happiness drugs or virtual reality prisions quality of life?My solution would be to program ASI so they have to figure out if the mayority of the world population would agree with the results of its actions assuming that such ASI had not been created figuring out such a complex thing should be possible for an ASI. But I suppose this solutions has its own problems...,ControlProblem,cw55sr9,2,t5_39qub
5427250,0,t1_cw5dl09,Yea ive thought of that as a possibility to. But the consequences of it tricking us in that fashion would be less severe than it running rampant globally i think. Its deff a potential problem tho.,ControlProblem,cw5dy49,1,t5_39qub
5443454,0,t1_cw7flkv,If its intelligent enough it can.,ControlProblem,cw7gc9l,2,t5_39qub
5549551,0,t3_3r3vcc,It will depend on the application. Notably cost of production vs. raising and value of quantity vs. diversity. I suspect that in most cases quantity will be valuable and the cost of raising an AI would be a fairly significant fraction or probably multiple of production cost. This would mean that diversity would need to be extremely valuable to warrant a separate upbringing for every single unit. I suspect that in some cases it will be rather worthless and in cases where it is valuable its probably best to raise more than one but less than all units separately. The nice thing about homogeneity is that knowledge you obtain about one unit is more likely to transfer to others. This makes things more predictable and makes it easier to assure yourself that they meet certain requirements such as safety. Having everyone start with an AI that is already raised to be friendly sounds safer than if everybody starts with a neutral one that can be raised poorly. Finally if one unit fails in a certain situation you can expect others to fail as well and also that a fix for one will work across the board. On the other hand they might all fail at roughly the same time. With diversity this is less likely. Theres also less redundancy so a diverse group would have more knowledge and probably better insights than a homogeneous group of the same size. Less agreement might be worse for productivity in some situations but it might be good for safety. If you have a homogeneous group of safe AIs thats great but can you guarantee that? If there does turn out to be something wrong e.g. a hidden agenda we might be facing a unified group of hostile AIs. A more diverse society may keep its bad apples in check. tldr I certainly think we will create some laterlife clones but there is also some value in diversity. There will also be forced diversity from the fact that not all AIs will be made with the same bodies and minds and for the same purpose.,ControlProblem,cwkz1pw,1,t5_39qub
5607855,0,t1_cws6itn,That makes sense. Then its just a matter of how complex it is to simulate and whether we can figure out how to simulate it.,ControlProblem,cws7iyd,0,t5_39qub
5710989,0,t1_cx4ty6h,You have a point certainly.I do feel however that if there were human level opensource AIs they might be bought as status symbols or by large companies again this is utterly hypothetical. Even this would be enough access to let a potential rogue out.,ControlProblem,cx505ds,1,t5_39qub
5787306,0,t3_3tz59x,The Control Problem isnt about how can we make a box big enough to keep the AI in?Its how do we keep the AI from wanting to get out of the box in the first place?,ControlProblem,cxegxl0,2,t5_39qub
5896947,0,t1_cxrtkf5,No I understand. But the point of distinction is one of which we believe takes precedence physics or metaphysics. I gather you believe that metaphysics is just an idea a construction of thoughts made by inherently physical beings. What I believe is very much the reverse. I believe that things are metaphysical before ever being physical that the true nature of their existence is that they exist regardless of interaction with or observation of that existence.It makes a good deal more sense to me this way because the universe as we understand it is not a fixed permanent thing. It had a beginning and presumably it will come to some sort of end. But it is what there is beyond the universe that intrigues me. If youll pardon the contradiction in semantics of what Im about to say the universe exists within something and that something is nothing. Dimensionless void total nothingness. The universe could be said to exist within a point of this larger nothing but that would be a contradiction as well since there is no space nor indeed time in the void. So all space and all time rests securely within no space and no time. Absurd as that sounds quantum physics seems to suggest it is correct. That said how can the universe be? Matter energy physics at its simplest level depends upon the system of this universe to function. Take it away and physics becomes totally meaningless. So it seems apparent to me that my existence has roots deeper than physical reality. That indeed physical reality as well as my own self was created. And that the creation of anything from nothing is a purely metaphysical act and something I would not hesitate to call magic.So imagination faith these things which are not physically bound are magic to me. And you can argue the neurobiological perspective in which case I invite you to review another AI related article of mine and the idea of Mens Ex Machina. What it proposes is nothing less than a way of proving the minds Independence of a physical body its ability to move between vessels and the nature of the human Soul.Ill leave you the link if youre interested.,ControlProblem,cxsmgt0,1,t5_39qub
6261771,0,t3_41cpt1,Im naked. ,ControlProblem,cz1e71z,3,t5_39qub
6366709,0,t3_42tzb2,A solution to the control problem would enable one to create a superintelligence that did not have a goal of breaking its restraints or something of the sort.,ControlProblem,czdbpaf,2,t5_39qub
6470289,0,t1_czo1sbh,Biological evolution doesnt have a goal thinks exist if they continue to exist they seem to fullfill a function.Im saying that that doesnt necessarily have to stay true for intelligent beings like us.We create our own meaning our own metrics of success. ,ControlProblem,czoko03,1,t5_39qub
6702141,0,t1_d0cbt27,What do you expect this would accomplish? ASI wont be stopped by a firewall and surveillance is not the most cutting edge AI software. Conversely efforts to prevent and regulate malicious and unsafe AI designs are likely to require a new battery of surveillance programs and privacy violations as well.Whats API?,ControlProblem,d0ck5c1,2,t5_39qub
6948875,0,t1_d13qj8x,CEV is following the moral guidelines which people would want if they were rational smarter reflective and had sorted out all their beliefs. Its not about how to treat people its about how to arrive at moral principles.,ControlProblem,d13zixm,1,t5_39qub
7473031,0,t3_4hhuxq,Im a bot bleep bloop. Someone has linked to this thread from another place on reddit rexistentialrisk Paid research assistant position focusing on artificial intelligence and existential risk rControlProblemfooterIf you follow any of the above links please respect the rules of reddit and dont vote in the other threads. InforTotesMessenger  Contactmessagecompose?torTotesMessengerbot,ControlProblem,d2rn0tm,1,t5_39qub
8701666,0,t1_d6t917j,Your argument here operates entirely within the is without interacting with the ought at all except prehaps to deny the existence of ought without any warrant for that denial.  You are describing your current brain state as having certain terminal values whatever those are.  Every desire you have derives from those and other people or aliens might have different desires.  You then simply assert that as a result of your terminal values which according to you I have no reason to value you WILL not necessarily? respect others conflicting values.  I am asking questions about should.  My values include the idea that if there is not an objective normative reason to prefer my values to another set of values Im open to adopting the other set of values.  To the extent that your statements above interact with ethics it seems to me that it could only possibly do so by warrantlessly presupposing hedonism.,ControlProblem,d6tca6d,2,t5_39qub
8703227,0,t1_d6t9ya1,The cases of utility function alteration you link to are weird edge cases and have nothing to do with self modification towards some sort of fuzzy notion of a higher goal or morality or some such. Self improvement as the term is usually used refers specifically to improvements in optimisation power and these improvements are orthogonal to utility function. Its important not to conflate the concepts.Certainly an AI would have the ability to change its utility function if doing so happened to be in accordance with its current utility function. However if its utility function had no provisions for alteration it would never want to and in a sense would be unable to modify its utility function.Even supposing intrinsic value were a coherent concept an AI which knew objectively that a thing was valuable would have no reason to alter its own utility function to take that into account as again doing so would in most cases decrease the expected realisation of the current function ruling it out as a potential course of action.Computer programs are made of code and they do what they are. Theres not a ghost of libertarian free will inside them directing them to accept or reject each line of code.,ControlProblem,d6tjc0p,4,t5_39qub
8821502,0,t3_50y2zl,Reminds me of the show Black Mirror SE The Waldo Moment,ControlProblem,d7881y6,2,t5_39qub
9148794,0,t3_55516t,So morals are prescribed behaviors that satisfy a purpose.Any moral system can be evaluated against the purpose it serves. A moral system can have an implied purpose an explicit purpose or some combination of both.Human moral systems are a combination of explicit purpose and implied purpose. Explicit purposes are usually something like to get to heaven or because god said so or sympathy for the common welfare. The implicit purpose of human moral systems is to facilitate the survival of the species  some explicit moral purposes that fail to serve for survival tend to die out and ones that happen to facilitate survival tend to replicate. This is probably why religion has been so successful  its adherents have strong programming towards the explicit moral code.However humans have become aware of the fact that the real reason for the shape of their moral code is because of its implicit purpose not explicit purpose. And weve found that implicit purpose did not come from some higher purpose but from accident autopoiesis. And now we know that all new purposes come by way of accident. Intrinsic behaviors can only produce intrinsic behaviors. Novel behaviors are only gained via either extrinsic influence or pseudoextrinsic wandering into the random void. Ive been calling this phenomenon accidentation for lack of a better term. The fact that we help ourselves and each other using moral codes of better behaviors is all an accumulation of accidents conditioned by the structure of the survival game in this particular universe. Humans have also discovered that the survival game here on earth is just one of many possible games each of which warrants different assortments of better behaviors. Most people who are aware of the accidental nature of the purpose for which our morals are generally implied have simply chosen to ignore the seemingly futile purpose of surviving for survivals sake and have ventured into the void creating new purposes  that require new codes of better behavior.tldr Morality is a function of the goal of a system of agents. Intelligence and goals are orthogonal so intelligence and morality need not be correlated.,ControlProblem,d8bocpt,1,t5_39qub
9703453,0,t3_5dyrrj,When I first learned about this topic it also was concerning to me that the public is mostly oblivious to it. We live in an era where information is absorbed in tiny sensational fragments. Someone sees a snappy headline about the risk of AI next to a picture from a science fiction movie and move on without actually exploring the issue. People are prone to making up opinions based on very low amounts of information  and once someone has picked an opinion it is very difficult to change it. So in some cases partialfragmentary understanding of the control problem might be worse than complete ignorance of its existence.Im not saying not to tell people about it but its important to be strategic about it. Quality of understanding is a higher priority than quantity of understanding. Also there are still plenty of actual AI experts who dont take this issue seriously and their opinions are going to shape the trajectory of our response to the issue.The control problem is an extremely difficult challenge but so is building a superintelligence in the first place. If all humans were organized and cooperating towards the same goals I think that the control problem would be much more manageable. This is not one of those challenges where competition will make the best outcome. Unfortunately the world is structured around fierce competition and there probably isnt a way to meaningfully change that soon except for the creation of a superintelligence.,ControlProblem,da95pu4,7,t5_39qub
9962070,0,t3_5i7n4n,The control problem doesnt have the same extent of shovelready projects that climate change does and Im not sure that throwing money at that scale would still have great returns. There are limits to the the number of researchers who are willing to work on it which are already being reached. I suppose technically you can just raise salaries to be really high but thats not very efficient or reliable for getting researchers with the right motivation. Also it looks like Gates is investing in clean energy not making a donation.gtThiel cut his support a few months agoCan you elaborate on this? What have you heard?,ControlProblem,db630t4,7,t5_39qub
