,controversiality,parent_id,body,subreddit,id,score,subreddit_id
37318,0,t3_1uixmz,I attended their short course in person and got to hang out with them for a couple of days. These two guys are awesome. I cant say enough how wonderful they are. ,statistics,ceiohpd,4,t5_2qhfi
52166,0,t1_cek8voo,I work as an actuary  predictive modeler.  How hard would it be to make a transition into biostatistics?  The field has always interested me more than applying statistics to generate insurer profits.  I have a personal passion for the work Dr. Pande is doing at Stanford with the Folding  Home project and somewhat regularly read up on the articles published by their group.Are advanced degrees required to break into the field or would I be fine with a Bachelors with very limited experience in physicschemistrybiology?,statistics,cekdb27,1,t5_2qhfi
53188,0,t1_cekfrzu,deleted,statistics,cekhhng,7,t5_2qhfi
99160,0,t1_cepp74e,Sorry I want to ask few more questions.. What would be the difference between actuarial science and data science?. What languages do you suggest? What are the most commonly used in this field?. When you mentioned graduation you mean undergraduate? And if you arent following an actuarial program you would self study to take those tests I assume?,statistics,ceppx03,1,t5_2qhfi
110830,0,t3_1vd4cg,Z scores have the advantage in that they are simpler to use. Unfortunately they are rarely practical. Generally Z scores can only be used when the population you are studying has a known variance. However that isnt very common. T scores take this added measure of uncertainty into their calculations. This allows you to use them on data where the population variance is unknown which is most populations people generally work with. Unfortunately this can also make the derivation of T scores slightly more confusing.,statistics,cer1u9b,7,t5_2qhfi
225935,0,t3_1wo6kj,No.  If you want to call the distribution concentrated at zero a Dirac delta distribution this is silly but whatever.  Multivariate normal distributions can be degenerate with singular variancecovariance matrix.  This is just the univariate special case.,statistics,cf45ycg,1,t5_2qhfi
227226,0,t1_cf4a0rw,If I threw rock then scissor would lose since it would have  losses and  wins .  If I threw paper then rock would lose.  If I threw scissor then  paper would lose.  We go buy the total wins  total losses so there would be a losing group in that scenario.,statistics,cf4aslh,1,t5_2qhfi
228983,0,t3_1wg5c1,The power of a test to detect a difference depends on the standard error and this in turn depends on the sample size and the standard deviation. I wrote a blog post about the difference between standard deviation and standard error last year which might help.,statistics,cf4hd0l,2,t5_2qhfi
250048,0,t3_1wyvo1,At this point I would look at descriptives rather than do hypothesis testing. Calculate means and standard deviations for each question separately for the two groups of stores and see what stands out. I know this isnt statistically appropriate for an ordinal variable but it should help you quickly see whats going on and then you can go to better methods.,statistics,cf6o6sx,1,t5_2qhfi
267437,0,t1_cf8b9rl,Lots of models with a lot of simulation counts and a few data sets. Thank you for the assistance.Edit Just to clarify since I am having some issues this is the code you meant?gtsinkdevnull xread.bugsoutputxread.bugsoutputsinkThis is my errorgtError in filefile ifelseappend a w  gt  cannot open the connectiongtIn addition Warning messagegtIn filefile ifelseappend a w  gt cannot open file devnull No such file or directory,statistics,cf8had8,1,t5_2qhfi
282721,0,t1_cfa0u85,I see I havent put as much thought into it. Thanks for the heads up!,statistics,cfa2gf6,1,t5_2qhfi
309552,0,t1_cfcupq9,ampampamp Regression toward the mean gtgtIn statistics regression toward or to the mean is the phenomenon that if a variable is extreme on its first measurement it will tend to be closer to the average on its second measurementand paradoxically if it is extreme on its second measurement it will tend to have been closer to the average on its first. To avoid making wrong inferences regression toward the mean must be considered when designing scientific experiments and interpreting data.gtThe conditions under which regression toward the mean occurs depend on the way the term is mathematically defined. Sir Francis Galton first observed the phenomenon in the context of simple linear regression of data points. However a less restrictive approach is possible. Regression towards the mean can be defined for any bivariate distribution with identical marginal distributions Two such definitions exist. One definition accords closely with the common usage of the term regression towards the mean. Not all such bivariate distributions show regression towards the mean under this definition. However all such bivariate distributions show regression towards the mean under the other definition.gtHistorically what is now called regression toward the mean has also been called reversion to the mean and reversion to mediocrity.gtInteresting Francis Galton | Regression analysis | Internal validity uLikeLeeHood can delete DeletionampmessageBdeletecfcupve. Will also delete on comment score of  or less. | FAQs | Mods | Magic Words | flag a glitch comment reportampmessageWhat seems wrong optional description goes hereAAAAReply no. A,statistics,cfcupve,2,t5_2qhfi
398904,1,t1_cfm2ygf,deleted,statistics,cfm41lv,1,t5_2qhfi
401905,0,t1_cfmf1oo,The Diehard tests are applied to the output of the PRNG which is similar to what your data is like. However the Diehard tests are most easily applied to the PRNG values before any transformation most PRNG generate values in the interval  actually  to some very big integer divided by that integer and that apply transformations such as inverse CDFs to that data to generate the samples for another distribution. As such it is a little harder to test the validity of a PRNG further downstream after the application of a series of transformations. Most of these tests as may be clear assume the data follows a U  in theory. ,statistics,cfmf8tr,2,t5_2qhfi
406414,0,t3_1yqota,rHomeworkHelp ,statistics,cfmw3hj,2,t5_2qhfi
417227,0,t3_1yuu5v,assuming no bias in missing birthdays a vanilla two sample ttest works.,statistics,cfo0fax,3,t5_2qhfi
440944,0,t3_1z4jtw,You have to divide  by . and then multiply that number of attacks over  seconds with the damage. Also in your calculation if your hit triggers the bleed effect you dont do normal damage  is that correct? Assuming the answer to my question is no you should do this.  .,statistics,cfqgvo0,1,t5_2qhfi
444645,0,t3_1z5kmw,You dont always have to agree with the reviewer!A cox proportional hazards model will let you compare the hazard rates between the two groups which refers to the risk of developing the first incidence of cancer.There are extensions to the cox model that will allow you to include multiple events per person but in general it wont let you look at the rate as you have defined it.,statistics,cfqup78,5,t5_2qhfi
459620,0,t3_1zct6u,Regression By Example Source graduate level course in predictive modeling,statistics,cfsibaw,1,t5_2qhfi
488401,0,t1_cfvnlzm,deleted,statistics,cfvs9fb,1,t5_2qhfi
498940,0,t3_1ztz7g,UC Riverside is wonderful a little off the beaten path east of civilization but the UC system is world renowned. If I remember right Bob Rosenthal recently retired from there. He was the man when it came to metaanalyses. Not exactly stats related more researchrelated but great regardless. ,statistics,cfwzeai,1,t5_2qhfi
524995,0,t1_cfzu9rp,Yes whats the error message?,statistics,cfzy4ox,2,t5_2qhfi
547285,0,t1_cg29s7d,gtAfter thinking about this Im a little less Bayesian.How very Bayesian of you. ,statistics,cg2he19,19,t5_2qhfi
562239,0,t3_20koto,deleted,statistics,cg46l0c,0,t5_2qhfi
578393,0,t3_20rblj,difficult to actually prove the desired result if its true This a common misconception. When you are doing hypothesis testing you always necessarily state the null hypothesis as being true. You never accept the alternative hypothesis or null hypothesis you can only reject or fail to reject. Pvalues could change with differences in experimental conditions or outliers however I dont there is a way of reporting this change visually. You would just report that with N samples a pvalue of . for X was calculated when samples were increased to  or reduced to  maybe use bootstrapping a pvalue of  for X was calculated. Someone else could have another viewpoint but plotting pvalues seems like a waste of time. A pvalue is just an area under a curve and plotting this visually would not illustrate if it is a false positive or not. Setting a type I error before hand of say . is the experimenter deciding that a  chance of a false positive is OK. It is common to set a type I error rate before hand otherwise you are letting the results of the study dictate your hypothesis. For example if you got a pvalue of . and decided to set alpha to .. The major issue with the application of frequentist statistics is the obsession over pvalues in my opinion. ,statistics,cg60n7m,2,t5_2qhfi
578911,0,t3_20qq2d,The first thing you should figure out is what kind of behaviors youre interested in picking out. I mean if the info you want at the end of the day is exact account balances then nothing you try to do will recover that. If youre interested in roughly the maximum how many have a balance over  how many have a balance under  etc well good. Youre going to judge your model based on how closely it matches those exact quantities and not how well it matches exact dollar values because you cant get that info.If you can get for just one month actual account balances and work from there that is going to be better than having no information. Two months is better! Then what you would do is roughly speaking fiddle around and see if these is some distribution that is a good fit for the data generate some data sets based on this and then check whether the generated data matches somewhat the kinds of features youre interested in looking at. Suggestions a lognormal distribution seems like a reasonable first choice. Perhaps it should be somehow truncated in some way. A power law could be possible I dont like them because lognormal is usually a better choice and people usually fit them stupidly. A hierarchical model could be considered but that gets very complicated. I bring up the first two because rather helpfully some very smart people already wrote a software package for R for fitting power laws and lognormal distributions and comparing them and very helpfully wrote up extensive documentation with code you can just copy and paste and run to make the comparison with the only necessary modification being to change how the data set is read in. See here Anyway once youve picked out your model the lognormals parameters if you did that will somehow depend on your sample mean and variance or some other statistics which you can obtain and you can generate things in the future based on that. And hopefully you can get a second month of data to compare your predictions with to see how awful your model is  it will likely be very rough. What you will do generally is generate a lot of data sets of the same size as your original and look at the distributions of properties youre interested in. ,statistics,cg62rk4,1,t5_2qhfi
583022,0,t3_20sgj7,In addition to the ones listed elsewhere khan academy is always a good resource with well explained videos which have examples.,statistics,cg6jl8c,1,t5_2qhfi
596771,0,t1_cg7e13t,Fair enoughYou are PROBABLY right on that part... the theory really appeals to me but I also want to keep it practical.  Pure math leads to very few immediate applications unless one if VERY clever so I do not want to get stymied by experiences that are too rigorous ,statistics,cg83umi,1,t5_2qhfi
602306,0,t1_cg8olr1,Sounds all correct.  This is not a CFA it is a measurement model combined with a causal model otherwise called a latent variable structuralmodel.  Thats some deep quant shit you are in good work.,statistics,cg8qhoq,1,t5_2qhfi
604713,0,t3_211ixg,Your math is correct  .Im guessing the German lottery probability is not a percentage like you have it there. The number you have would make sense if it was a probability not a percentage.  Otherwise the expected value of a lottery ticket would be below .,statistics,cg90caq,1,t5_2qhfi
626176,0,t3_21awwu,I forgot one point as well. Are the errors estimated outofsample? I.e. are the authors split the dataset into training and test subsets and use the later one to compare models? Actually the AIC is asymptotically equal to cross validation error but no there seems to be no test associated with AIC.,statistics,cgbg3wc,2,t5_2qhfi
635401,0,t1_cgchfyh,Yeah its less about the way to write it and more about drawing attention to this occurring because of the means being the same. Will probably end up just using the standard way and not drawing too much attention to it.,statistics,cgchuj4,1,t5_2qhfi
657611,0,t1_cgex9os,Thanks for the advice. Ill make sure to take a look at that book.,statistics,cgf0qbw,1,t5_2qhfi
669600,0,t1_cgg049c,Hmm interesting so you say that this is not confidence interval. Then how do I explain its working. I have to write about it and thus fully understand it. The reason I said confidence interval is because it is using the  standard confidence interval equation.gtFor example you could decide instead that about  of gaps should be anomalously high and define the cutoff you use as the th percentile which is another arbitrary yet somewhat reasonable thing to try.Can you please elaborate a little bit on this I dont completely understand what you meant.thank you,statistics,cggds7x,1,t5_2qhfi
680311,0,t3_21y5c7,You might want to take a look at for an introduction to contrasts including orthogonal contrasts. ,statistics,cghloha,2,t5_2qhfi
688796,0,t1_cgcy0b7,Im going to stick with R I ended up ordering Kruschkes book Doing Bayesian Data Analysis which looks to be a pretty good guide for the beginner.The reason I have no experience with R or Python is not because I chose the wrong career path but because Ive used SPSS for data analysis which has a graphic user interface instead of entering code.,statistics,cgikg0w,1,t5_2qhfi
765747,0,t1_cgr1noh,Thank you very much for the help. So what indicators do you think would be useful to look for say for figuring out if a variable is significant?,statistics,cgrbk62,1,t5_2qhfi
766002,0,t1_cgrbk62,The way youve phrased your question makes it directly a question about pvalues which I have already talked about. ,statistics,cgrclni,0,t5_2qhfi
767031,0,t3_22wt4q,I am not sure whether this will help but here it goes. If the publications you collected contain information about the intercept which is the risk of the reference group you can use the risk ratios to estimate the risk of the other groups. Once you have the risks for the other groups you can use any value of your choice or the risk of any group of your choice to create a new risk ratio.,statistics,cgrgszd,1,t5_2qhfi
773497,0,t3_230l9t,What exactly do you mean when you say that it matches up to standard error?,statistics,cgs790k,1,t5_2qhfi
776537,0,t1_cgnru5u,Out of curiosity why do you recommend python AND R? I was always under the impression that the folks that used python only did so because that was the language they were most familiar with. Instead of relearning a new language in r they just stick with python. ,statistics,cgsjos9,1,t5_2qhfi
785286,0,t3_233oys,You should not use Levenes test as a basis for deciding when to assume equal variances in the ttest.If theres any serious levels of doubt about the reasonableness of the equalvariance assumption dont assume it. Use something like WelchSatterthwaite any time youre not confident in assuming equal variance. The result level robustness and power is better than testing and then operating conditionally on the outcome.,statistics,cgtjhei,2,t5_2qhfi
839846,0,t3_23q68w,Not really. Moments dont pin distributions down especially well and with just summary statistics very different distributions might be possible.You might be able to find a family that lets you say this distribution has the same summary stats... but so might something that looks quite different to that one.,statistics,cgzqrfw,2,t5_2qhfi
858746,0,t3_23y436,I think you would need to know how many people dine at each restaurant.  If you are comparing nominal figures you are inherently assuming the same number of dishes served per place. So a place that serves  plates per service period would be expected to have more illness than a place that serves  in the same time even if their food safety standards are exactly the same.  Also you are dealing with only those sick enough to go to the hospital.  It is likely that for each person who reports there are some that dont.  This does not mean you cant make inference just that you need to know that you are not likely getting a representative sample.  Now on to the statistics.  I would look into some kind of negative binomial regression.  According to your question you are going to need to decide what food hygiene problem means.  This is the part that needs to be well defined by you as you will later be testing the null hypothesis that restaurant i has no systematic health issue.  Economist use value of statistical life to measure costs and benefits associated with public health and social well being but there may be some well defined criteria from the CDC or your county state health department.Edit spellingGood Luck.,statistics,ch1w7hu,3,t5_2qhfi
901510,0,t3_24dd35,Ha I used to play that! Im taking into account the info in your other comment because thats necessary too and Im assuming that the items you get are independent do not depend on the other items you get.It makes it complicated that you get  items. We cant actually find the exact probably without knowing how likely it is that you get  items total how likely it is that you get  items total etc... This is because the probability of getting the certain item twice in one treasure chest depends on how many items you get out of that treasure chest. Somebody correct me if Im wrong but I think this is how youd want to think about itFor example say you get  items out of a treasure chest. Then the probability of getting the certain item twice is      .Now say you get  items out of a treasure chest. There are multiple ways to get the certain item twice now YYN YNY or NYY. We have to treat them differently.YYN The probability of getting the certain item on the first try is . The probability of getting the certain item on the second try given that it was there on the first is . The probability that you dont get it on the third GIVEN that you did on the first two is  because you can only get it twice. Thus this yields      for the combo YYN.YNY The probability of getting the certain item on the first try is . The probability that you dont get it on the second try is . The probability of getting the certain item on the third try is . So the probability of this combo is     . NYY The probability of not getting it on the first try is . The probability of getting it on the second try is . The probability of getting it on the third try is .So IF you already know that you get  items the probability that you get two of this certain item is            See how it gets complicated?,statistics,ch6r8yk,1,t5_2qhfi
908000,0,t1_ch7g67d,Ok that makes sense and clarified some things for me. I was going to ask about how much the positivenegative categorization would correlate with proanti comments. I can imagine seeing both positive and negative comments being labeled pro.,statistics,ch7hueb,1,t5_2qhfi
910191,0,t1_ch7c86d,Sorry I have a follow up.  I see the Adult Weight log kg chart and understand that concept.  However what would that chart look like if it was just Adult Weight kg without the log of the weights?,statistics,ch7qt9v,1,t5_2qhfi
922003,0,t1_ch8xx1n,If data is normally distributed then datameandatastandarddeviationdata is distributed N.  ,statistics,ch935qo,1,t5_2qhfi
960675,0,t1_chdhil4,whoa just checked on my phone and its also an alien head in a box. ,statistics,chdhkie,3,t5_2qhfi
963130,0,t1_chdjjd2,Thats what I read it as first haha,statistics,chdrmfe,3,t5_2qhfi
985930,0,t1_chgbazi,Interesting. Thanks I appreciate the info.,statistics,chgcwcf,1,t5_2qhfi
1013909,0,t3_25kjxv,Should you wish to end up in biotech sector then id say that R is the way well python and SQL are nesessary aswell. ,statistics,chjjede,1,t5_2qhfi
1078454,0,t1_chqqnpl,Use real examples or think of real examples and interpret them whenever you see a formula.,statistics,chqvgcq,1,t5_2qhfi
1158633,0,t1_chztl38,I was an analyst at a major multinational and I had to beg them to let me use SAS because they did everything in Excel. Did you know there is a row limit in Excel?  I didnt. At least not until that job. ,statistics,chzyphu,2,t5_2qhfi
1191054,0,t3_27r5wv,deleted,statistics,ci3m1xz,3,t5_2qhfi
1196523,0,t1_ci41fr8,Yes and thats why perfectionists end up dying of heartdisease because if you want it done right you have to be prepared to do it all yourself.In the end its up to you what you want and what you are willing to do for it. Some people want a piece of paper some people want grades and some people want understanding. Personally Id tell you to be happy with the last one as it has the longest shelf life. Grades and assignments come and go but your comprehension of the concepts doesnt and really thats what youre in school for. Everything else is just ego padding. ,statistics,ci486rx,2,t5_2qhfi
1200088,0,t1_ci4kl6j,Haha yeah Ive been here for years and Im not entirely sure what gold is myself. Mostly qualityoflife stuff I think Thanks again for all your help ,statistics,ci4mmn6,1,t5_2qhfi
1215819,0,t1_ci6eblv,What exactly is a hypothesis test? ,statistics,ci6ecbp,2,t5_2qhfi
1219834,0,t3_282aho,In addition to what others are saying I feel Math opens up a lot more career options in case you ever change your mind by the time you graduate.,statistics,ci6ulg9,6,t5_2qhfi
1221740,0,t1_ci71lbx,My degree has a large programmingcomputer component so I should be good there. Ive heard biostats is a booming business but I dont know how much I would enjoy that. Banking seems pretty dull to me and not something that would interest me. I know that the sports world needs statisticians which is where Im really leaning....but no real clue on how to break into the sports arena. ,statistics,ci72bk0,1,t5_2qhfi
1225676,0,t3_284zqu,I see Python and R as suggestions but considering this is intro stats those might be overkill. Libreoffice is a free version of Microsoft office and their spreadsheet program can do it,statistics,ci7i95q,4,t5_2qhfi
1248670,0,t1_cia1eke,OK but your target variable if Im understanding your model is a  binary flag right?  Say se   ra   for example.  Your model is trying to predict the  binary flag.  If that is true then what you are doing is predicting that outcome with the model and the sensitivity and PPV and C stat are all measures of the predictive power of the model granted that only the C stat is a general measure across all possible score values.But again you are less interested in prediction than you are description.How are you deciding which variables are the most powerful predictors?  I would look at the Wald chisquares  the larger the chisquare the more important the variable is to the prediction.,statistics,cia3aa2,1,t5_2qhfi
1283712,0,t1_cie13ik,I admire your civil tone a standard that I have not lived up to recently in a lot my comments. Thanks and cheers.Re ANOVA that was my mistake ANOVA is really just the GLM anyway as is multiple regression so I tend to use a lot of these terms interchangeably which is obviously not good practice.OLS just solves for parameter estimates it has no feature selection step exactly. So it is up to the user to specify the features.I agree that exploration of ML algorithms could be helpful and I should have said so in my post I have the nasty habit of just highlighting where I disagree with people rather than highlighting points on agreement its an irksome quirk of my training personality or both. In particular I would refer the OP to Zelazos late searly s metaanalysis of the AnotB error which is in many cases analogous to the task OP is analyzing. Zelazo used a neural network to do the metaanalysis so a case could be made for it here too. Pushing for the use of PCAICAetc might be a bit harder since there isnt prior art to work from. In fact Im not aware of a single metaanalytic study using those approaches so its likely that some basic statistics papers would have to be published on this first.Personally if the OP were to be interested in pursuing some more complex techniques Id recommend a hierarchical Bayesian metaanalysis not nearly as hard as it sounds. But given the quality of reviews that OP encountered and alas that are standard in psychology there is a generational war going on these are unlikely to result in a published paper in anything less than a year or so of back and forth waiting on the availability of reviewers with the requisite expertise etc.,statistics,cie18qt,1,t5_2qhfi
1292216,0,t3_28vfd2,Ive had a few ideas to do pattern recognition with handwritten alphabet characters. The idea is to decompose handwriting into categoriesAll of the as in category All of the As in category All of the bs in category etc...Divide up each letter on a pixel grid and code it to be  or   if no writing is present  if writing is present. So one may look likeThe above would look like a b except this is an example and there would be many more pixels resolutions and wouldnt exactly look like this.From here you can use digital software to determine where the center of this picture is maybe calculate the row and column sums then you can find the heaviest frequency row and column total.The center will be a point where you can line up the recurrence of each letter. Then when all of category  letters lined up you can see the mean of each pixel and understand the variability also. You can create some distance matrix to see how a new letter deviates away from the mean  call this score D.So in terms of pattern recognition. The idea would be to use a logistic model  if the person had written this letter  if the person had not written this letter  or any other type of classification really. One factor would be which category is it based on the above blocking per letter then the scoring D.Something like that,statistics,ciezopw,2,t5_2qhfi
1337816,0,t3_29eb1f,Is neither an option? Im not sure why a stats specialization would require either of those courses. See if you can substitute another course in analysis real or complex measure theory probability stochastic methods etc.,statistics,cik4c9h,3,t5_2qhfi
1369580,0,t1_cinyjpz,I have heard that working withfor MDs can be a real chore but isnt there some aspect of your post that is meant for educating users?  I understand if its not.  My most recent post was at a midwestern med school and the environment was so toxic and hostile Why are you asking me about that?  You should know that already. that i just up and quit one day a phd and  years of experience and i just walked like i was  again.  Im still curious if the fuck you im not sharing what i know attitude is a midwestern thing or just unique to this school.,statistics,cinyr90,1,t5_2qhfi
1372157,0,t3_29t1sx,You can for the data in excel using linest but youll need a column for each variable.  So you need additional columns filled with XY G and H.   All the columns in the model have to be next to each other.  Next to your data select  rows by J columns J  number of variables in your model   for the constant or intercept term.    Type in linest select the fxyz data then the x data then type .  Then hit F and then ctrl  shift  enter.  The coefficients will be in reverse order  no idea why starting with the intercept term on the left and then the coefficient for the first column.  The second row is the standard error for each coefficient.  The leftover rows have the rsquared SSESST etc.  read the help documentation and start with a simple y vs a single x until you get the hang of it.   ,statistics,cioadjs,1,t5_2qhfi
1409271,0,t1_cisttl4,Hm. I didnt have a formal derivation so I should stop and think about it some more.,statistics,cisxjt3,1,t5_2qhfi
1436136,0,t1_ciwa81h,I have never used S or Splus.  The code is similar enough to R to translate well?,statistics,ciwao9o,1,t5_2qhfi
1454383,0,t3_2at270,I think Id simulate this. Its probably amenable to analysis by a clever person but Im not feeling at all clever today.,statistics,ciykyuj,1,t5_2qhfi
1463768,0,t1_ciz2d7s,I agree with you that a thoughtless statement of you can make statistics say anything is rather childish and is certainly not a valid critique against databased arguments.  However in certain arenas it is definitely the case that one should not be swayed by statistics without looking extremely closely at all aspects of the study. The application that always comes to my mind is survey results. The experimental design is often more important that the statistics done after the collection of data in particular how you word the prompts. Very slight changes in the wording of a prompt can greatly affect the outcome of a poll In the political arena pollsters with agendas can often exploit this fact without really doing any statisticalmathematical slightofhand. If someone put a gun to my head and told me to tell a convincing lie with statistics this is probably the method I would choose. ,statistics,cizrcbu,1,t5_2qhfi
1465415,0,t1_cizyma5,I put this into google pca skewed data outliers and it was the first link...,statistics,cizyrgq,6,t5_2qhfi
1466651,0,t3_2avrmt,IMHO nobody knows how to teach statistics including me.The suggestions by other posters are plausible but cause problems.More applications?  Real Applications TM are very messy.  They require a lot of subject matter knowledge which students dont have to understand what the real scientific business whatever issues are and many need a broad knowledge of statistics to choose an appropriate procedure.  Real Applications are something you can do after you have learned a lot of statistics.  Beginners cant understand them.  Hence everybody uses dumbed down applications  applications stripped of all of their complications outliers and other issues with the data already cleaned up scientific questions ignored  but all dumbed down applications demonstrate is that if you put numbers into a statistical method then numbers come out.  But using Real Applications would slow a course down to a glacial pace.Use more computers?  Sounds great until you confront the problem that students dont grok computers either at least not beyond mousing around or nowadays gesturing around and editing Facebook.  So you have to teach computing as well as statistics both tasks very hard.  Doing both simultaneously is extremely hard.  The solution here is to dumb down the computing.  Just use calculators.  Or only expect students to closely follow a handout.  Or various other strategies.  But then students dont really learn computing.  And it still can slow down the statistics.My main complaint is that we dont really try to shake up the students.  Statistics is extremely counterintuitive.  Probability is extremely counterintuitive.  They are not counterintuitive in the same ways.  There is some overlap but not that much.  But most teachers of statistics dont really challenge students to give up their intuitions.  There isnt even a good book on the subject.  I can point to lots of bits and pieces in lots of books but they are all over the place.And I mean really basic things.  Intuitively investigation is about finding whodunnit.  Statistics is about questioning whether there is a point to explanation at all maybe it is just chance variation.  This makes most peoples head hurt.  So we soft pedal it in statistics teaching.  But then people dont get the point and misuse statistics.  Statistics is the methodology I can use to prove my ideas are right.  My slogan about this is that many users of statistics believe P lt . means statistics has proved that every idea I have ever had is correct.  They could care less what the null hypothesis is.Maybe other disciplines have issues too.  I only have thought much about statistics.  And the more I think about teaching statistics the harder it seems.  When you see very smart people scientists with big grants and hundreds of publications making basic errors you see that we have a long way to go.,statistics,cj04c6i,1,t5_2qhfi
1517464,0,t1_cj6ho9j,ampampamp Akaike information criterion sfw gtgtThe Akaike information criterion AIC is a measure of the relative quality of a statistical model for a given set of data. As such AIC provides a means for model selectiongtAIC deals with the tradeoff between the goodness of fit of the model and the complexity of the model. It is founded on information theory it offers a relative estimate of the information lost when a given model is used to represent the process that generates the data.gtAIC does not provide a test of a model in the sense of testing a null hypothesis i.e. AIC can tell nothing about the quality of the model in an absolute sense. If all the candidate models fit poorly AIC will not give any warning of that.gtInteresting Deviance information criterion | Bayesian information criterion | Focused information criterion | HannanQuinn information criterion Parent commenter can toggle NSFW NSFW toggleampmessageBtogglensfwcjhojz oror delete DeletionampmessageBdeletecjhojz. Will also delete on comment score of  or less. | FAQs | Mods | Magic Words,statistics,cj6hojz,1,t5_2qhfi
1519941,0,t1_cj6sgla,logistic regression? what is the dependent variable here? just high or low sales?,statistics,cj6svcy,1,t5_2qhfi
1566163,0,t1_cjch995,The important bit is learning general rules about how to deal with spatial data rather than learning one tool arcGIS which isnt particularly focused on statistics. There are a lot of good R tools for working with spatial data. ,statistics,cjclbjs,1,t5_2qhfi
1578153,0,t3_2cc2r2,My admittedly limited understanding is that a ttest would be best here.  ANOVA is best for multiple variables but a ttest is best for two variables.,statistics,cje3kxe,2,t5_2qhfi
1589203,0,t1_cjfcdmq,gap minder,statistics,cjfhnoc,2,t5_2qhfi
1630151,0,t3_2cw9vc,If you are going to transform a correlation you might want to use the transform described herewhich has the advantage that the sampling distribution of the correlation is a Studentt distribution.From that you can get the sampling distribution of the mean transformed correlation and from that you can get standard error a confidence interval and a pvalue.,statistics,cjkn8z9,2,t5_2qhfi
1643402,0,t1_cjm8pxs,Excuse my ignorance ASA?,statistics,cjmbcne,4,t5_2qhfi
1675868,0,t3_2dkfgi,First of all you are going to make mistakes in any job that revolves around programming and data handling  its inevitable.  If mistakes in a certain position have the potential to cause significant harm to your employer validation strategies should be in place.That said there is a certain amount of checking your own work that has to be done.  People can recognize a mistake for what it is  sometimes its due to confusing data or a complicated plan and sometimes it is due to carelessness.  People generally dont mind the first but the second can be a source of aggravation.  In either case own up to your mistakes correct them and move on.,statistics,cjqelr8,1,t5_2qhfi
1689671,0,t3_2dqswz,You cant buy a better name. Within reason of course. No need to break the bank for a masters degree.,statistics,cjs5aq9,0,t5_2qhfi
1695352,0,t3_2dt55s,Regression to the mean just says that following an extreme event a less extreme event tends to happen. So for example if you get a run of  blacks the next run of blacks will tend to be less than . Events near the average  black  red per  spins are far more likely to occur than extreme events like the  black case.The Gamblers fallacy occurs when for example after a run of  blacks a red is assumed to be the most likely outcome for the next spin. It assumes that the next event depends on the past events. The problem is the probability of the next spin being black or red is exactly the same as it was for each individual spin before that one.Basically if you witnessed  blacks coming up on a roulette wheel you could say that its not likely the next  spins will have  blacks in it but theres no guarantee of how many redsblacks there will be or what order they come in. You could literally get  more blacks before a red and Regression to the Mean still happened.I should be careful here because its not really useful for prediction getting an edge so Im not trying to say it is. You basically can only say that an extreme event happened and a less extreme event is more likely. Regression to the Mean doesnt imply causality as in an extreme event doesnt cause the less extreme event to occur. Events that are close to average occur far more frequently than extreme events is really all you can take away from it.,statistics,cjsv2a4,2,t5_2qhfi
1710994,0,t1_cjsxdi3,Imagine after five spins you have  reds and  black. Thats  red.Now imagine that the next  spins give you  red and  black. The percentage of reds across all  spins is now ..Notice that the percentage of red has fallen from  to . even though there was a majority of red in the second batch.Both the first batch of five spins and the second batch of  spins had a majority of red but the percentage still fell.In other words in order to get the long term tendency to move towards  you do not need the wheel to do the opposite of what it did before.,statistics,cjutznm,2,t5_2qhfi
1725679,0,t1_cjwlcey,Sure but theres a difference between small and very small  when someone talks of small samples I imagine something like  observations. Doing inference on  observations is extremely rare. But in any case I expected OP to read the linked answer and not take my word for it. ,statistics,cjwoq26,1,t5_2qhfi
1742557,0,t1_cjyt80n,ampampamp Fishers exact test sfw gtFishers exact test    is a statistical significance test used in the analysis of contingency tables Although in practice it is employed when sample sizes are small it is valid for all sample sizes. It is named after its inventor Sir R. A. Fisher and is one of a class of exact tests so called because the significance of the deviation from a null hypothesis e.g. Pvalue can be calculated exactly rather than relying on an approximation that becomes exact in the limit as the sample size grows to infinity as with many statistical tests. Fisher is said to have devised the test following a comment from Dr Muriel Bristol who claimed to be able to detect whether the tea or the milk was added first to her cup see lady tasting teagtInteresting Hypergeometric distribution | Exact test | Ronald Fisher | Barnards test Parent commenter can toggle NSFW NSFW toggleampmessageBtogglensfwcjyti oror delete DeletionampmessageBdeletecjyti. Will also delete on comment score of  or less. | FAQs | Mods | Magic Words,statistics,cjyt88i,1,t5_2qhfi
1766149,0,t1_ck1rjp9,My point exactly in my post elsewhere on this thread.And as someone who has taken quite a few of your data science courses I say bravo. Theyve been interesting challenging and very informative. Keep up the good work.,statistics,ck1s25p,1,t5_2qhfi
1790247,0,t1_ck4k1l0,I dont think theres anything necessarily wrong with small samples. It makes it harder to know if youve violated assumptions but if you havent then it still works. Of course the big problem is that youre probably massively underpowered.  The zeroes are a big problem.,statistics,ck4t8ov,0,t5_2qhfi
1806077,0,t3_2f65cw,Is it better than John Hopkins course?,statistics,ck6sue2,1,t5_2qhfi
1813038,0,t1_ck7n7y9,I have this one,statistics,ck7neza,1,t5_2qhfi
1870888,0,t1_ckemg4n,The data I will have will be bacterial growth with time for each well from which the length of a certain phase of the growth is obtained it is the distribution of this time parameter that I am interested in. History affects certain aspects of bacterial behaviour so there is no way to my knowledge to definitively return the wells to their original state in order to repeat the measurement.I think the initial bacterial count of the various wells would be Poisson distributed and so perhaps erroneously that so too would this time parameter.Since there is no dependent variable here or relationship to be established how would linear regression be applied to this type of problem?Cheers for your response.,statistics,ckemvsm,1,t5_2qhfi
1900425,0,t3_2gdr8h,Sounds like you can use linear regression to predict attitude using various demographic data as predictor variables. This seems to be the best way to accomplish your goal. Alternatively you could compute partial correlations between each of your demographic variables and your dependent variable attitude score. What software do you have access toare proficient in using?,statistics,cki6v2t,2,t5_2qhfi
1917115,0,t3_2gl551,I graduated with a BA in philosophy. Taught myself a bunch of stuff got a job as a Data Analyst lateral promotion then got an MS in mathstats a few years later. Now Im a Data Scientist at a highly respected consulting firm. I only did one year of math in college and it was all calculus.  I never took a formal stats class prior to grad school. It can be done.,statistics,ckk76rv,12,t5_2qhfi
1932437,0,t3_2grhw4,What a coincidence! Why is this so?,statistics,ckm1mgp,-2,t5_2qhfi
1942735,0,t3_2gvwsu,A lot of this is psychologyspecific but some of it is more universal. Some of these are pretty technical and some are more general. Additionally many of these are arguing the bigger fight rather than against the more egregious misuse youre describing. But in those theres likely to be stuff of relevance.The most thorough treatment though also difficult this one will have direct quotes including several from Fisher himself to support you on your points of concerngt Hubbard R. amp Bayarri M. . P values are not error probabilities. Institute of Statistics and Decision Sciences Working Paper  .An easytoread article by a famous statistician author of Cohens kappa  the measure of interrater agreementgt Cohen J. . The earth is round plt. . American psychologist  . American Psychological Association.Othersgt Anderson D. R. Burnham K. P. amp Thompson W. L. . Null hypothesis testing problems prevalence and an alternative. The journal of wildlife management . JSTOR.gt gt Berger J. O. amp Sellke T. . Testing a point null hypothesis the irreconcilability of P values and evidence. Journal of the American statistical Association  . Taylor amp Francis.gt gt Gardner M. J. amp Altman D. G. . Confidence intervals rather than P values estimation rather than hypothesis testing. British medical journal Clinical research ed.  . BMJ Group.gt gt Gibbons J. D. amp Pratt J. W. . Pvalues interpretation and methodology. The American Statistician  . Taylor amp Francis Group.gt gt Goodman S. N. . Toward evidencebased medical statistics.  The P value fallacy. Annals of internal medicine  . Am Coll Physicians.gt gt  Schervish M. J. . P values what they are and what they are not. The American Statistician  . Taylor amp Francis.gt gt Hubbard R. amp Lindsay R. M. . Why P values are not a useful measure of evidence in statistical significance testing. Theory amp Psychology  . Sage Publications.,statistics,cknarxu,2,t5_2qhfi
1976176,0,t3_2hc2n2,Someone more experienced than I should come along but theoretically you cannot. A simple random survey as defined allows for all possible outcomes to be equally likely to occur. That means that the draft that was picked in  theoretically had the exact same likelihood of occurring as any other combination picked if lottery was random. The results of this one lottery does not include enough information to determine if it was truly random or not which is on a sliding scale already since nothing is truly random ignoring quantum mechanics. To determine if a  sided die is random again on a sliding scale then you have to introduce replication.,statistics,ckrbuwm,4,t5_2qhfi
1993111,0,t1_cktdbiw,ampampamp Tfidf sfw gttfidf short for term frequencyinverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus   It is often used as a weighting factor in information retrieval and text mining The tfidf value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus which helps to control for the fact that some words are generally more common than others.gtInteresting Vector space model | Information retrieval | Okapi BM | Gensim Parent commenter can toggle NSFW NSFW toggleampmessageBtogglensfwcktdcx oror delete DeletionampmessageBdeletecktdcx. Will also delete on comment score of  or less. | FAQs | Mods | Magic Words,statistics,cktdc8x,1,t5_2qhfi
2001075,0,t3_2hnq41,The highest pvalue I am able to obtain is  with the numbers          and . These numbers were carefully picked to raise the pvalue  not at all random.,statistics,ckubumt,3,t5_2qhfi
2049519,0,t3_2ia7p1,It might be nice to have some axis labels.,statistics,cl0c7fx,3,t5_2qhfi
2059442,0,t3_2ieyli,I think what you want to start with is the probability of detecting a difference from fair dice if one exists. This would be the power of your test. So typically two dice will sum to  in ths of all dice rolls. What would be a meaningful deviation from  that you would want to detect? For instance if you would give the dice the benefit of the doubt at . then you shouldnt need to do millions of rolls.,statistics,cl1lm6x,12,t5_2qhfi
2064414,0,t1_cl26brs,There isnt a specific number of trials that will give you  confidence that the dice are weighted.  If you just roll the dice twice and you get eight both times theres only about a  chance that would happen.  Hence if that scenario were to play out you could be  confident that the dice are weighted.Note that this doesnt mean that if you ever roll two eights in a row then the dice are probably weighted.  This would have to be a selfcontained test that you perform just once.,statistics,cl28dih,2,t5_2qhfi
2069399,0,t3_2ijz11,A consequence of assuming normal errors is that the least squares estimates are equal to the maximum likelihood estimates.  This is useful for hypothesis testing.  It is important to note that the least squares estimate is still the best linear unbiased estimator even when the errors are not normally distributed.  This is the famous GaussMarkov Theorem,statistics,cl2v6te,2,t5_2qhfi
2070286,0,t1_cl2xonq,Ah ok I think I see what happened. Heres basically what you need to know and eventually explain in polite reviewer language. Cutoff scores arent going to be the same thing as an LPA.  Two variables are not ideal for running an LPA because youre basically looking for  groups from two variables. You can do it sure but it wont look the same as your median splits.  Cutoff scores dont presuppose a normal distribution nor do they require an elimination of any outliers. Just run a quick histogram and see how it turns out because LPA will grab any scores that cluster together on each end and make it into a class.  You dont have to include what the reviewer asks for. Youre allowed to say We ran it your way but it came out all fucked up.Heres my attempt at reviewer language. We graciously thank reviewer  for their suggestion of using an LPA to form our four highlow groups. However we originally constructed these highlow groups because our data included a handful of outliers and were not uniformly normally distributed. This original categorical grouping allowed us to explore an interaction between grouping variables with a similar number of subjects within each level. When examined the same data with an LPA we found that the nonuniform distribution of our variables formed a number of uneven groups. These groups did not fit into our apriori interests but could perhaps be part of a future study that focuses on natural groupings within different levels V and V. However these grouping are not more or less valid that the median splits that we used to examine the data.Alternatively you could also just extract the  latent profiles rerun the analysis and see how it turns out. Part of the reviewers suggestion is that you might get something better with LPA which is plausible. ,statistics,cl2z917,1,t5_2qhfi
2088821,0,t1_cl599qj,deleted,statistics,cl5c2zu,1,t5_2qhfi
2095265,0,t3_2iw5x0,yep Kmeans is the solution. Interestingly a guy recently used kmeans to find an ideal OKCupid match for himself ,statistics,cl65kit,1,t5_2qhfi
2104033,0,t1_cl6h43m,You could in principle reverseengineer clinical significance from the bit error rate no?I dont think the key idea here is to throw information out. Thats just used in the example to make things intelligible. I think the key idea is that any causal link can in principle be used to transmit information.,statistics,cl79nwo,1,t5_2qhfi
2119805,0,t3_2j7wsu,Definitely add a coffee variable somewhere in there!,statistics,cl99ukn,3,t5_2qhfi
2169883,0,t1_clfjl2e,Understood  you should use variable selection techniques like backwards elimination forward selection Cp or Lasso because your variables might be highly correlated.,statistics,clfn5mz,1,t5_2qhfi
2172929,0,t1_clfuahu,It is not analysis that is typically or considered before a judge or signed off by an expert witness.  Ive never seen it used and Ive been a participant in  or so federal matters with respect to voting cases and am considered an expert in the statistical analysis of voting and fraud detection.,statistics,clg14vt,1,t5_2qhfi
2241291,0,t3_2ksmkh,Id recommend picking up a copy of Lean Analytics Itll give you a solid idea of what stats are important for your company. It all depends on what stage you company is in. How does the signup funnel look? Are users getting activated? How long are they staying? You dont necessarily want to show stats for all those things in a daily report but those questions will help you figure out what stats are important. If people are downloading the app then not uploading any pictures or liking anything then you want to track and share the percentage of new users that upload a picture within a few days for example.All this stuff is covered in Lean Analytics in much more detail than I could possibly explain via Reddit.,statistics,clopzke,2,t5_2qhfi
2270817,0,t1_clrmfsi,Wow thanks for the great response. Data science seems to be one of those buzz words that gets a bunch of hits to your linkedin profile but from what youre saying is that its really a unique skill set that makes the difference.,statistics,clse548,1,t5_2qhfi
2283527,0,t3_2le25z,Breimans paper on parametric vs. nonparametric modeling.EDIT  Here it is  Statistical Modeling The Two Cultures,statistics,cltylyd,4,t5_2qhfi
2286277,0,t1_cltylyd,This is a cool paper but I dont like how he puts down data based models...and declares that it is seemingly wrestling with futility ,statistics,cluatz3,2,t5_2qhfi
2289658,0,t3_2lcl64,A bunch of meaningless fluff.,statistics,clupuvx,1,t5_2qhfi
2293967,0,t1_clv6rcp,Or you can just do it in two words maximize utilityMuch like your post though thats not terribly useful advice because the trick is in knowing how to maximize utility. Which is what all that statistics crap is good for.,statistics,clv9152,0,t5_2qhfi
2297013,0,t3_2lj98d,Data mining. I know controversial but it has proven very accurate in related applications like insurance predictions for a while now. I took a summer course on social science statistics at UMich fantastic program and had the pleasure of taking courses under UPenns Bob Stine who made it very clear the applications and value. Yes you need to be careful but model building with tools is just as valuable as old purists who tweak their models by eye and intuition anyway but somehow pretend it is less risky than using refined tools. I also learned about the use of JMP is SAS which makes understanding underlying models so much more interesting.I have been applying these lessons as a consultant for governments all over the world for a few years now with great success.,statistics,clvmk93,2,t5_2qhfi
2311780,0,t3_2lp93m,This site on ordered logistic regression in SPSS may help. Poke around there for more.,statistics,clxg9lv,1,t5_2qhfi
2322954,0,t1_clyi99j,Its true...Ive interviewed phd candidates for an internship position that cannot give the assumptions of simple linear regression or give proper interpretations of coefficients. ,statistics,clytx62,1,t5_2qhfi
2357731,0,t3_2mckma,Couldnt you model this with Bayes?,statistics,cm34k0g,0,t5_2qhfi
2400406,0,t3_2mwul9,Im not a big ANOVA person but to the best of my understanding treatments are a specific type of factor i.e. one that is manipulated by experimental control. The other factor could either be a second treatment if your experiment involves two separate treatments or a controlIn the former case you might have an experimental design where you assign exercisenoexercise to some groups and highlowprotein to different groups. You could look at each treatment independently without doing two experiments. You could also look at the interaction of exercise and protein  i.e. whether the two together have some effect that is greater then their combined individual effect.To illustrate the latter in your the example the dependent variable could be body weight. We might know that males have higher bodyweight overall and that males are more likely to have highprotein diets. The two way analysis would give us an estimate of the effect of a highprotein diet on bodyweight regardless of the gender of the individual. So gender is a control that isolates the effect of the protein in the diet from the gender of the individual.Those examples are completely made up but I hope they help. In the shortest terms a factor is a categorical variable malefemale highlow protein which is used to measure whether a treatment occured.,statistics,cm8ecqs,2,t5_2qhfi
2406883,0,t1_cm8fcpm,gt That being said this is not a career choice to make lightly. The exam track that actuaries take is very difficult and requires years of commitment to finish. You need to decide if taking  exams well into your s or s is something youre willing to do.I quit the field because of exams. Passed P was prepping for FM and decided that I spent enough time not having fun in college,statistics,cm977b4,1,t5_2qhfi
2418964,0,t1_cma8ic6,Its good as long as you keep in mind that its the stats equivalent of a book by a politician  it may explain the situation better than anyone else could  but it has an agenda.,statistics,cmaoyf5,2,t5_2qhfi
2430800,0,t3_2nbgdu,I would imagine it has to do with how you did your recoding.If you look here link you will see at the top that the person ran frequencies but it also printed the command to the output window. the same is probably true of most recoding commands. What you have to do is find each oneif they are in the output window and paste them to an spss syntax file. Then run the syntax file by clicking the big green arrow.IF you do not see anything in the output window that looks like recoding syntax.. then you are probably out of luck.,statistics,cmc5jti,2,t5_2qhfi
2458288,0,t3_2nnoqt,Not sure what you are asking about. Are you referring to an equivalence test? Such as does drug A reduce inflammation the same as drug B? ,statistics,cmfjoeg,1,t5_2qhfi
2496813,0,t3_2o6hu9,Dont forget random chance. Getting famous takes lots of skill but also quite a bit of luck. ,statistics,cmkgzps,5,t5_2qhfi
2505275,0,t1_cmljhhx,Perhaps a little uncharitable but Im from Philly its in my blood. But I think it is especially critical that you cite your sources because you are in pedagogical role here even if it is only a blog. Andrew Gelman uses examples quite often on his blog and he has plenty of citations. Furthermore you have a PhD youre a professor and youre trying to sell your book. This might only be reddit and this might only be a blog but Im still holding you to a higher standard. The other issue I have is that I have not seen this hypergeometric distribution approach for a simple two occasion markrecapture experiment except for the Wikipedia talk page and your blog. Its seems a bit odd that the one thing you do link to outlines the same somewhat obscure method for this problem and that the Wikipedia page itself has no citation information. Aside from the fact that the method doesnt seem to perform all that well and so I question whether it should be taught to students it seems odd to me that you should replicate this approach when markrecapture methods have such a rich literature.Sorry dude but youre treading in my territory. Markrecapture is my bread and butter. So forgive me if I take offense that you are not giving serious weight to the assumptions of markrecapture or even make passing mention of the history of the topic beyond citing a wikipedia page that could really use some work. If I were a reviewer and somebody tried to pass off an abundance estimate using this approach while only citing a wikipedia page I would almost certainly reject it.,statistics,cmlkt5g,2,t5_2qhfi
2505591,0,t1_cmllwl0,At the time I bought it I was doing lots of simulations for parasitic disease modeling.,statistics,cmlmaqc,1,t5_2qhfi
2529344,0,t3_2omexf,I agree with belariuss suggedtion. Also consider volunteering as an intern doing something technical not even necessarily statistics at a research center or local business. If you have a local tech startup community that might be a good place to ask around.,statistics,cmopz9q,2,t5_2qhfi
2544717,0,t3_2otwjk,rdatasets,statistics,cmqqc0b,2,t5_2qhfi
2575064,0,t1_cmujgdw,gt There is no value in running a replicate trial as there is no variability in measurement here.Provides an answer. But I dont believe anonymous users can give the same answers twice. Not in any trials I have ever seen.There are  things. Can you group them? Maybe one of those is useful.,statistics,cmup6hj,1,t5_2qhfi
2596994,0,t1_cmxiozp,Thank you! This is exactly what I want. ,statistics,cmxkba8,1,t5_2qhfi
2616350,0,t3_2pny20,I did mostly R in my stats masters with  SAS course. R is used internally in the oranisation I now work but government projects and  data sets that contain confidential information health records tax returns all have to be accessed with SAS. So SAS has been cruital for work.,statistics,cn03fwq,2,t5_2qhfi
2651861,0,t1_cn4nwb3,I used an R package that I wrote that works with discrete random variables. One of the things it can easily do it give the distribution of the sum of a bunch of random variables.,statistics,cn4q95v,1,t5_2qhfi
2685558,0,t1_cn94947,Full playbyplay. See nfldb,statistics,cn94f47,0,t5_2qhfi
2702038,0,t3_2r0alk,Here is a oneoff video that I think might fit the billJohn Rauser  Statistics Without the Agonizing PainIt would be great to find more products like this.,statistics,cnb7mdc,25,t5_2qhfi
2715968,0,t1_cncrzo9,Wow.That might just do the trick.,statistics,cncsmf6,0,t5_2qhfi
2721218,0,t1_cndcqop,To do theory you need linear algebra.  An easy example... You use linear algebra to find the conditional distribution of Y|X when Y and X are both multivariate normal and YX has a multivariate normal distribution.Linear algebra us useful for more than least squares minimization.  Consider LASSO Ridge regression quantile regression.  Quantile regression in particular has a very complicated proof involving linear algebra to show consistency of the estimates.,statistics,cnde2u1,2,t5_2qhfi
2727403,0,t1_cne2hb4,Hey neat its close to what Im after but there are some issues.The biggest issue is that I dont have an actual survival curve.I can only estimate a survival curve using my data which has DOBi and Vi at a single point in time. This estimated survival curve might not be a decreasing function meaning the hazard function will be negative and I will start to worry that the techniques are invalid.,statistics,cne3ehm,1,t5_2qhfi
2741813,0,t1_cnfj52m,That said my SO is a microbiologist so I cant help but feel it as a side project would be infinitely helpful to them!,statistics,cnfqe1k,1,t5_2qhfi
2765374,0,t1_cni6gyr,Agreed. It sounds like you mostly need aggregates which SQL can do easily. And your data appears to be well structured so a relational database would work.People have mentioned dplyr I want to point out that it contains function that interact with databases through R. I dont know more than this but its worth checking out. Once you have your subsetaggregate I would recommend manipulating the data in R rather than Excel. You mentioned creating graphs  R has very powerful graphics packages such as ggplot and ggvis that will produce graphics that are hand over fist better than what Excel can do.,statistics,cnieup6,2,t5_2qhfi
2771630,0,t3_2rr1on,Some thoughts Independent ttests are robust to deviations of normality you can run individual ttestsMannWhitney U with a SidakBonferroni correction for a conservative way to control for familywise error rate. ANOVA can be robust for similar sample sizes and similar variances but small sample sizes take away protection from the violation of assumptions. You can use is KruskalWallis H a nonparametric test if the distributions of your samples have a similar shape. If youre going for nonparametric test Id suggest to do pairwise comparisons with an independent samples median test.,statistics,cnj4hg0,3,t5_2qhfi
2784352,0,t3_2ryypi,This is called p hacking. Youre just fishing for the best model. Form a hypothesis and test it. Youre inflating alpha by guessing until you find the best fit. ,statistics,cnkkkal,0,t5_2qhfi
2863481,0,t1_cntj8jv,Cooleo. So I really just have to worry about the software interface and the tasks. Thatll do.,statistics,cntkutc,1,t5_2qhfi
2902780,0,t1_cny0l7i,I see. You could possibly find something in generalizability theory that can help you. There are many types of ICCs under generalizability theory and if I recall correctly you can even estimate the reliability of a single item or rater. Shavelson has some classic resources on generalizability theory if youre curious. ,statistics,cny1vrh,1,t5_2qhfi
2907671,0,t1_cnylogi,Thanks for the input.  Makes me wish I had access to the source data and SPSS....if thats even still used anymore.,statistics,cnylxyu,1,t5_2qhfi
2911130,0,t1_cnyv7eo,You want a wellknown advisor who people think highly of so that his recommendation will be worth something.,statistics,cnz04cp,2,t5_2qhfi
3044727,0,t3_2v14lw,When you are talking about a likelihood there is always an assumed distribution for the residuals. Residuals are the ei in the relationship Yi  BXi  ei B can be a vector of regression coefficientsFor least squares the assumed distribution of ei is normal with constant variance  mean and independent from each other. I butchered the notation assume ei from this point on is the difference between the observation and predicted valueMinimizing the least squares sum ei  is maximizing the normal likelihood under the above conditions.It is true that if you keep adding covariates to the model the R goes up which also indicates that the sumei  goes down and the likelihood the likelihood increases.The term maximizing the likelihood is often used when you assume a particular model for example EYi| X  xi  B  Bxi and you want to find the B and B that maximizes the likelihood which is the same that minimizes the least squares under the above conditions.However the term maximizing the likelihood is used when there is a determined model it isnt really used when you are trying to find the model because as you have said doing this results in overfitting.The deviance works very similar so it isnt directly used as a term of model selection.For model selection there is AIC BIC which have advantages in different scenarios R adjusted and a number of other criterion for model selection which incorporate a penalty term for higher number of parameters.Another way to check for overfitting is to employ a cross validation scheme assuming you have enough data to train a model and enough data to test the trained model. If you are overfitting then a trained model will perform horribly on the test data.,statistics,codl2rj,3,t5_2qhfi
3068474,0,t1_cofx1nt,Good question.. this is for my bioinformatics class too. Figured maybe the beginning is an intro to basic concepts we will be using. ,statistics,cog1l33,1,t5_2qhfi
3139387,0,t1_condv26,A statement that is seemingly contradictory or opposed to common sense and yet is perhaps true.Its paradoxical because it is counterintuitive that a trend present is several groups of data could reverse when combining them.,statistics,cone7f5,10,t5_2qhfi
3139626,0,t1_coney19,Yeah philosophers and other nonstats people probably would. Gotta make stuff sound fancy.,statistics,conf3he,-9,t5_2qhfi
3153332,0,t3_2w9ixr,Not a professional statistician but Im an undergrad senior stat student. May I ask why youre going for a math degree? I would say go straight for stats youll take plenty of math courses for it anyways. If youre going to try to learn a language I would say try R it is used a lot in the academia and its friendly for beginners.,statistics,coou9t5,-8,t5_2qhfi
3175982,0,t3_2wij3z,While an inherently distressing topic this was a very enjoyable read. I had never actually considered making high school teachers take comprehensive high school tests but its such a sensible idea.,statistics,cor6tzv,5,t5_2qhfi
3208790,0,t3_2wvonj,It seems as if you are taking the difference between the sum of uniform random variables which you might be able to specify from the convolution of IrwinHall distributions. Just going by intuition this distribution seems right since from a combinatorial standpoint there should be more ways to get a different of  than   then  but that support falls away as you move from . A quick test would be to raise the maximum value of the cards or add higher value cards to the deck and see if you get the expected outcome.,statistics,coul95d,2,t5_2qhfi
3238264,0,t1_coxa7ts,Well impact factor compares a journals impact to ALL other journals including Nature and Science and the like.If the top journal for a given field is in an obscure field with  researchers total the impact factor is going to be meaningless since it will have to be incredibly small because only  researchers might be reading it citing it etc.In this case theres  journals in the field as counted by SCImago and likely many thousands of researchers so you can look at the journal ranking and get a better feel. This particular journal is in the th out of all  social  science journals sorted by impact factor  which is more suggestive.Than you can look at  how old the journal is Volume  was published in Unfortunately its still hard to get a good feel of the value of a journal just from those raw numbers. It ranks th  in the Hindex for the same  journals which implies that the impact factor may not reflect how important the journal is and of course it may be one of the very top journals in its own littlesubgenre and still be obscure in the larger categories of applied psychology or social psychology.Its interesting though the journals website says its got a . SJR ranking but SJR from  says it has a .. But the journal cites Thomas Reuters rather than SJR directly. Dont know what the discrepancy means..The whole field of journal ranking is very confusing and controversial. A review article that is merely a summary of existing knowledge might have many of dozens of authors and eventually cited many hundreds or even thousands of times and yet not contribute in any way to new science is still counted in the various journal ranking schemes as every bit as important as a new paper with realworld importance equivalent to Einsteins paper on Special Relativity. Journals are accused of encouraging this kind of review article just so they can boost their impact factor ranking and authors are happy to have their name appended just so they can be part of a  citation article..If I recall correctly there are actually science journals devoted specifically to the topic of journal impact factor and related subjects. I dont know what their journal impact factor is however.,statistics,coxniku,3,t5_2qhfi
3261780,0,t1_cozy402,Good point.  If  foreignborn STEM workers with advanced degrees create jobs why would not  U.S.born STEM workers with advanced degrees create the same number of jobs?  Dr. Matloff has raised this point several times.  By the way all of the R code for this analysis can be found on GitHub at  .,statistics,cp03f7g,2,t5_2qhfi
3278334,0,t1_cp17q9m,Its difficult to know the exact motivations for those who write these studies. Since she released her data to me I think that Zavodny believed in her study or at least thought it more likely that she would receive good press as have her study seriously critiqued. From having gone through the replication I can think of possibilities for her thinking. She tested  through  but those numbers were not quite so favorable to the desired conclusions. Then she or an aide on the study came up with the rationale for excluding the  recession. Those gave good numbers so she quit looking. I may be giving her too much credit though. As can be seen at  she happened to have selected the best numbers out of the  spans with  or more years from   spans from  when she did the study.Another major writer of such papers along with his two coauthors declined to release their data to me. I therefore suspect that they know that their data would not stand up to scrutiny. You can hear him describe why he left San Jose out of his data at this link,statistics,cp1yfy4,2,t5_2qhfi
3298248,0,t3_2xw28t,Lets say just for example that  of people are doctors and  are garbage men in the general population.Harvard graduates are significantly more likely to become doctors than the average person.  Why?  Because they have college degrees.  They are also more likely to have an above average work ethic.  Harvard has one of the best medical schools in the world so it is understandable that somebody might be more likely to go there if they want to be a doctor.  Lets say that a Harvard graduate is  times as likely to be a doctor as a random person.  So  of Harvard graduates go on to become doctors.Much more importantly Harvard graduates are significantly less likely to become garbage men.  Most garbage men do not have college degrees yet alone an Ivy League education.  Students who go to Harvard are more likely to be from wealthy families.  So say Harvard graduates are  times less likely to be a garbage man than the general population.  So  of Harvard graduates are garbage men.  There are now more Harvard doctors than garbage men.You are right.  The above numbers do not even begin to come close to show to what extent you are right.  Harvard grads are more than twice as likely than the general population to become doctors.  More importantly they are probably hundreds if not thousands of times less likely than the general population to become garbage men because there is almost no overlap between the population of people with Ivy League educations and those who will become garbage men.,statistics,cp48696,2,t5_2qhfi
3334791,0,t1_cp83ir8,Stata is still more or less the standard in economics although R is popular too. If a program is only teaching one software package I think it should be Stata.,statistics,cp8e4fz,1,t5_2qhfi
3344536,0,t1_cp9ddoz,Kind of reminds me how NFL cheerleaders are paid,statistics,cp9i2ud,1,t5_2qhfi
3372309,0,t1_cpcmpws,If data binning is not common in the field and is being used solely to find significant results then it probably isnt the best method.  If you can find a theoretically useful way to bin data then at least there is a good justification.  Otherwise a logistic regression can help answer your question without needing to attempt explaining binning.,statistics,cpco2ao,1,t5_2qhfi
3414536,0,t1_cph4dyx,gt and require that there is reasoning put behind why a particular model is assumed before estimationI think this is key. The point of science is to erode our uncertainty. The more difficult the subject being investigated the more likely we will have competing ideas. The ideas that survive are those that are plausible and can e replicated. Plausibility here is the theory or even narrative that plugs those findings into what we already know. Stats alone do not play a singular role. A well thought through statistical method is essential but it is a part of the broader scientific method. Often the big problems we see at least in my neck of the woods is poorly done experiments that produce biased data. No statistical model no matter how perfect can undo these sorts of problems.,statistics,cphhcmb,2,t5_2qhfi
3439977,0,t1_cpkc6ks,How are the data organized? Is it a regular grid of points? or is it a trail of points? If the latter how much of the sea floor does it cover     ?You might look at Gaussian process regression which is a kind of smoothing model. GP regression is generalization of local smoothing models such as LOWESS. A web search should turn up a lot of hits.,statistics,cpkdsy3,1,t5_2qhfi
3442314,0,t3_2zln89,Theres a different lower bound for different sample sizes and different tests ... and ties can alter it again.I can give an example or two however. Consider a WilcoxonMannWhitney with n nThere are only  choose    arrangements of the two group labels to the  observations so the smallest possible one tailed pvalue in that case if there are no ties at all is .Now consider a signed rank test on paired data with n pairs. There are  possible assignments of pairmembers to groups for each pair the original order or swapping the twoSo the smallest attainable onetailed pvalue in that case is This happens with any nonparametric test capable of being rendered as a permutation test many rankbased tests are really permutation tests performed on the ranks.This is generally only an issue with really small sample sizes. Once you get past sample sizes around  its pretty much a nonissue unless youre using tiny significance levels like ridiculous numbers of multiple comparisons which suggests more thinking is needed about what youre really trying to achieve,statistics,cpkneez,2,t5_2qhfi
3469836,0,t1_cpns07z,You should always test for the parallel regression assumption because it is critical for the interpretation and often violated. There are several tests for this and you can usually just pick one that your statistical software supports. You can find the names and interpretation of some tests for Stata here and that should also help if you use something else.Edit fixed link.,statistics,cpnsb8a,3,t5_2qhfi
3473704,0,t1_cpo63hs,Yeah dont plan to drop out.  It leaves a bad taste unless youre in desperate circumstances.  If needed you could switch from fulltime to halftime as you progress in the program but dont plan to drop out.  That said a lot of PhD programs include tuition remission free school and if youre lucky you can get a stipend.  Maybe reapply next year if youre concerned about location or see if your boyfriend can move with you and pursue an opportunity where the PhD program is,statistics,cpo87lk,6,t5_2qhfi
3484658,0,t1_cpp79f2,for A lot of it seems to be educating the public and scientists.  Gigerenzer tries to do this for the most part in plain English.  There are a lot of sites dealing with some of the issues raised in this article.  One particular good one is ,statistics,cpph5b7,3,t5_2qhfi
3539451,0,t3_30ttnz,Watches and Statistical Formulas.... Can Watches be used as Infographics for Statistical Formulas?Measurements of Central Tendency for Grouped Data,statistics,cpvpxdt,-2,t5_2qhfi
3756719,0,t1_cqkha93,gt One is how many xbars you draw. CLT talks about this going to infinity. This is the quote above that I am strenuously objecting to. Do you see what is wrong about it?,statistics,cqkhs4i,1,t5_2qhfi
3837928,0,t3_34d44e,deleted,statistics,cqtr9b8,1,t5_2qhfi
3840807,0,t1_cqu2msp,this  applications for undergraduate summer internships close in January.,statistics,cqu33ie,5,t5_2qhfi
3869716,0,t1_cqxanyx,Close! Generalized additive models are an extension of general linear models which are basically linear regression with link functions so they can fit nonnormal noise distributions that add smooth additive by default there are no interactions between the independent variables terms. Ive seen both splines and smoothing kernel implementations.Basically you can use them for basically anything youd use regression for when youre expecting nonlinearities but dont have a  strong reason to suspect its exactly quadratic or whatever.,statistics,cqxdqo5,1,t5_2qhfi
3926095,0,t1_cr3sol6,Theres no hard rule saying that you need a minimum sample size of  you just need more data points than parameters so your model is identifiable. Any problems that arise from having too small a sample size for repeated measures ANOVA will be worse if youre using a Friedmans test.I mean you might not probably dont have enough data to reliably estimate your model  detect meaningful differences but just changing the test youre using doesnt magically fix that.,statistics,cr3t6yo,2,t5_2qhfi
3930922,0,t3_35fs84,A related question is there a good source of public freely available and preferably interesting data online?,statistics,cr4czu7,3,t5_2qhfi
3932252,0,t1_cr4czu7,rdatasets,statistics,cr4ifxk,2,t5_2qhfi
3980183,0,t3_361ax3,For a nonscholastic approach check out the public version of Tableau start making your own visualizations and get feedback. Visualization is almost a discipline unto itself IMO  its more like an art. ,statistics,cr9z2l5,2,t5_2qhfi
3985135,0,t1_crah5ir,Yeah ARIMAX is just ARIMA with predictor variables ARIMA  X. ARIMA is the noise part of the model all the autocorrelation structure and trend stuff and the X part is just like regression you are used to assuming your data is stationary,statistics,crajhhc,2,t5_2qhfi
4003246,0,t3_369pos,Is this like USA and NL have  men and  women while DE is  and youre worried about a gender effect on the response?Try some basic plots to see if there is an obvious difference.You can test specific variable and see if it is significant or if you have enough data break the data set into two and analyze each part separately.,statistics,crclnmd,1,t5_2qhfi
4003478,0,t1_crclqfy,hrmm  I must confess I had the same issue I still mostly only dabble with it but the ideas are quite neatAn accessible entry point depends what your background looks like and what part of this stuff you are interested in. To me the neat part involves cramer series a series expansion of the CDF of a statistic around its limiting distribution in the tails of a distribution though the stuff relating this to statistical mechanics is also cool but I know very little about it. If you PM me Im happy to try and help.,statistics,crcmlxm,1,t5_2qhfi
4075828,0,t1_crjoris,Medical statistics without p value hacking is like something without a thing which is commonly in that thing.,statistics,crkvcsz,2,t5_2qhfi
4125311,0,t3_37w2bk,Also heres the post about it on Slate Star Codexgt For me the takeaway from this affair is that there is no onesizefitsall solution to make statistics impossible to hack. Getting rid of pvalues is appropriate sometimes but not other times. Demanding large sample sizes is appropriate sometimes but not other times. Not trusting silly conclusions like chocolate causes weight loss works sometimes but not other times. At the end of the day you have to actually know what youre doing. Also try to read more than one study.,statistics,crqie3d,2,t5_2qhfi
4137877,0,t1_crrs6df,This is very helpful thank you. Guess I should start doing my research!,statistics,crrxq7c,1,t5_2qhfi
4151138,0,t1_crtc60y, for survival analysis. Rates arent a good measure of attrition since they can be so sensitive to recent hiring rates firstyear attrition is usually much lower than that of employees in other tenure groups.Source Im in Googles HR Analytics team,statistics,crtfzd3,1,t5_2qhfi
4197481,0,t3_38xqck,It is the constant in a linear regression.  They have a dataset the define a model and then they fit constants to that equation.  The fitted constants are represented by that symbol.,statistics,crypiuu,3,t5_2qhfi
4214932,0,t1_cs0m8ii,Agreed. ,statistics,cs0oq62,1,t5_2qhfi
4233184,0,t1_cs2pu5j,Agreed outside tech and academia R is not commonly used. SAS is king in the business world. I had no idea rapidminer was so popular! Python is also the fastest growing of the languages so its definitely worth learning. ,statistics,cs2rqlr,-4,t5_2qhfi
4276691,0,t3_39ylbp,Try and download and use SAS University edition it should let you try out all the functionality of SAS Base. It is free for anyone but a little complicated to install if you are not computer savy.,statistics,cs7p9oo,4,t5_2qhfi
4289616,0,t3_3a1v7g,This is exactly what I did. I would say that the content of the course matters far less than the topic of instruction. Stats teachers especially at the grad level vary greatly in their abilities. So ask around. If you are serious you want a ballbuster who will make you work hard but is available to help you. Once you have a handle on some upper level stats and have done the work to really learn it it should be doable to expand your knowledge base on your own or with minimal input from professors. As a second suggestion Ask IO grad students. Third suggestion Multivariate.,statistics,cs95tfy,1,t5_2qhfi
4342841,0,t1_csf5vlu,Its like changing from ferenheight to celcius same thing really,statistics,csf63ku,1,t5_2qhfi
4353881,0,t1_csg9xx4,Oh man this reminds me of one of my favourite boss is an idiot stories. I was providing my former boss some results of some metric that is composed of several components adding up to a total value. I rounded the values to  decimal points for legibility.He was going through those results and asked me why the total score doesnt exactly add up to the sum of components scores. I replied that this was simply due to rounding error.His response Well if its an error you have to fix it.I just stared at him speechless.,statistics,csgeyd8,8,t5_2qhfi
4358290,0,t3_3atrdv,gt I wish we said something like not merely random or probably not zero.Unfortunately neither of these terms is likely to be correct in anything beyond the simplest cases. Theyre also too long for jargon terms.I dont much like the use of significance either but were stuck with that one. Id be happier spending my time convincing people to stop wasting time doing them for the large fraction of the time theyre used when something else should be done instead.gt Type IType II error It is hard to remember which is which because the terms dont convey any clues as to what they mean. I wish more informative metaphors were used such as false hit and false miss.Theres already the very widely used false positive and false negative without knowledge broad enough to be aware of those its probably better to refrain from suggesting terms and muddying the waters even further gt Power Statistical power refers to the probability that the null hypothesis will be rejected provided that the null hypothesis is false. The term is not selfexplanatory and requires memorization! I wish we used a better term such as true hit rate or false null rejection rate. While were at it  and  are not much better. False hit rate or true null rejection rate and false miss rate or false null retention rate would be easier to remember.Pshaw! Bah humbug! Power is a perfectly clear term once it has been defined exactly once. Unlike Type III errors which I agree are a problem because the terms dont remind you which they are power to discern a difference is pretty damn good at suggesting what power is. If you have to write half a sentence to get a term that is obvious on first reading false null rejection rate?? Youve  got to be kidding me! people will make up a shorter term precisely to avoid constantly writing phrases like that   word phrases are the kind of thing which causes people to come up with singleword jargon terms in the first place. You cant think sufficiently complex ideas about things if youre writingsaying a great long phrase every time youre dealing with the concept because you have too many things to keep in your head.No power is perfect as jargon  suggestive of its meaning concise productive. The whole point of jargon.While a few of the remainder are fine I disagree to various extents with much of the rest,statistics,csgwuqz,3,t5_2qhfi
4422580,0,t1_cso4n3s,Thanks I agree that the monte carlo way should work but maybe I should research more before wasting time on the bazillion samples. Hopefully someone has worked out a closed form solution for p in the MVN and GMM setting. The reason I think the second approach would be better is because i dont really have lots of spare heldout data to estimate this CDF and I want to avoid taking a beating from my dissertation advisor and commitee.,statistics,cso5s8s,1,t5_2qhfi
4430412,0,t3_3bsqfs,This is majestic and also scary. Here are some other highlights from this infographic    Each year about  people die from entering a freeway going the wrong way!  Around  people are killed each year by a vehicle on a private driveway.  Speeding only saves the driver  minute for every  miles.   of all fatal accidents in the US are caused by aggressive driving habits such as speeding and reckless driving. September is the worst month for road rage incidents. Perhaps its the changing of the seasons?,statistics,csp55gs,0,t5_2qhfi
4453710,0,t1_css20ir,I dont have a clear picture of what the problem youre solving is about but suppose were looking to find the rate  of some events. In the simplest model the rate is fixed so  is our only parameter. If we then want to capture a yearly variation we might for example use the more flexible model      sin t   with the three parameters   and  and a constant       year.Its possible that I have misunderstood your question though.,statistics,css2o2b,2,t5_2qhfi
4469403,0,t1_cqlws2s,Wil Im back  months latter.as the project evolved we changed a few things. Now i have a excel file with a few columns  my dependent variable and others we judged important.This is a small part of it where Phase is my dependent variable.What i ve done so far using R i split my set into a training one and a test one. and tried to create models from itThe first model I simply used the glm commandbinomial and what happened was this.   It predicted using each variable inside my columns which i think is not right.What i did after was changing all my variables inside my columns to numbers and the result was better i added a weigh to each column of my model. But I dont know if that is right since i dont know if i can simply give wach variabel a number.How should i proceed now?gtThank your,statistics,csu1k66,1,t5_2qhfi
4545765,0,t1_ct1ve3v,,statistics,ct3mbn9,1,t5_2qhfi
4565696,0,t1_ct5r8j2,Im not sure how your universityhospital is set up but Im a biostatistician at a large research hospital. We do all of the stats for the research projects residents and med students do. At the  universities often the stats and biostats departments also have free consulting. Im mean the following in the nicest way possible please consult a biostatisticianstatistician with your project. The simple truth is youre busy and you wont have all the right domain knowledge. Sure you may learn how to do a x way ANOVA but what if that isnt the correct model given your data? The reality Ive found is medical professionals learn the students ttest and then think it can be applied to anything. Investigate resources and use them. If they dont exist come back here and give us a detailed summary and let us help you decide. Nothings worse than misapplying methods.  And it wouldnt be your fault. Youre busy learning how to save lives. We were busy learning ways to crunch numbers. Thats our thing. Of course still learn statsread dalgaard and then ISLR intro to statistical learning using Rand many many more I would recommend getting the basic understanding of survival analysis down so you understand why you cant use ttests on such data. You say that you wont be using learning methods. The fact is the methods in there arent the mystical machine learning methods you hear about Netflix using per se. There are many methods covered that really should be getting used in basic medical research. Ive rambled long enough. ,statistics,ct64cbq,3,t5_2qhfi
4570730,0,t3_3dn2l8,guygal who just downvoted do you have advice on what I did wrong in my post to have earned the downvote? Id be happy for literally any advice in this situation.,statistics,ct6r2ka,3,t5_2qhfi
4571837,0,t3_3dnmim,I think you were castigated because it was actively derailing the discussion... What was the point of the statement? To prove that when the person said that no studies have shown adverse effects of GMOs they were quite likely marginally overstating?,statistics,ct6w2bv,2,t5_2qhfi
4577810,0,t1_ct7khtw,Can I ask a follow up Q So when people talk about running multivariate regression like PLS but predicting against a single value say concentration that technically isnt multivariate but multiple regression?As a further example lets say I have a dataset and I do a PCA on the data to see which are different that is multivariate but then if I use that same data to predict against a single Y either categorical or quantifiable using PLS it is a multiple regression and no longer multivariate regression?Im asking because we do this a lot at my job and there are people who can push buttons in a simple software and they call themselves experts its mostly chemometric data. I dont dabble in this too much so call myself a novice and I am a glutton for correct nomenclature!,statistics,ct7n0np,1,t5_2qhfi
4594025,0,t3_3dvqb6,I think the Kalmon Filter will best suit my needs.. I like how the predicted trajectory is a function of the governing equations Fk state transition model and Bk controlinput model. The only thing I see challenging is how I can quantify tortuosity as the noise and a function of time. I believe I could quantify this by using tortuosity by an integral of square of derivative of curvature seen below sorry dont know how to format an equation in reddit..Would love to hear your thoughts on this approach! Thanks so much for the help!,statistics,ct9o3s4,1,t5_2qhfi
4610171,0,t3_3e2tgo,The Bureau of Justice Statistics. I think they even do a special report on hate crimes which includes religiously motivated attacks.,statistics,ctbp0re,1,t5_2qhfi
4613639,0,t3_3e45qh, for CampB,statistics,ctc4ob0,1,t5_2qhfi
4668640,0,t3_3ewm6c,Sorry for the typo in the title.,statistics,ctj1a59,1,t5_2qhfi
4699422,0,t1_ctmwd1z,Would you consider these books entertaining though? I have no trouble finding resources to learnimprove my skill sets but I was seeking ones that have some entertainment value.,statistics,ctmwgkq,1,t5_2qhfi
4787115,0,t1_ctxe3r6,The math checks out. Probability of . . for the top  or .  . for the top  though itd be slightly less since any cards you mulliganed away could be drawn later in the game.Basically this should almost never happen. Or do you mean that your combo consists of  different cards each of which you have multiple copies of in your deck? I only played Hearthstone for a few games a long time ago so I cant remember the deckbuilding restrictions and I definitely have no knowledge of the metagame.Edit Forgot to say what I actually came here for. Regardless of the actual statistics just from basic understanding of game design and customer service I think its a lot more likely that youre just a victim of confirmation bias andor negativity bias You should start keeping stats on your games save your gamelogs if theyre available from Hearthstone. Itll give you a better understanding of whats going on and make you a better player!,statistics,ctxye1m,1,t5_2qhfi
4787443,0,t1_ctxtbzm,The model has not changed.  Again why would you get two different values for one parameter?  The only reasons could be a different objective function or different optimization.  These do not change the model.  They only change how you FIT the model.  How is that a misunderstanding?  Maybe you are confused about how you fit models?  That is not inference that is just math.EDIT An example of changing your model could be something like adding a gain term.  This means your new model now has one more parameter than your old model.  Model selection would fit both models and select the one that covers more of your variance with less bias.,statistics,ctxzvlb,1,t5_2qhfi
4855630,0,t1_cu40j28,That paper you linked was fascinating. Thanks for posting it.,statistics,cu6l7az,1,t5_2qhfi
4859165,0,t1_cu7090n,Im incredibly pedantic sometimes so thanks for being receptive to it. With that cleared up for me thank you I guess I just want to bounce some thoughts off of you because this is an interesting study that I think would be great to see run over more trades more on that in a second and different data sets. Your result is also counterintuitive so Im really fascinated. My first thought would be to change up the ttest to something nonparametric because the distributions of accuracy look somewhat skewed. Something like a Mann Whitney U would work or even just the method whose name I forget right now where you randomly pick one accuracy from each group and build a histogram of the accuracy differences. It might just be that the test is biasing the results with its assumptions.Heres my second thoughtWhat if you made a scatterplot of the training mean and std. dev for a whole bunch of random splits along with a single point for the whole data set? Im wondering if that wouldnt show you something interesting. My hypothesis here is that the splitting up and only getting the normalization from the training is doing a naturalprobabilistic outlier filtering. If the training data is more often of a lower standard deviation then the training data benefits near the mean and you only lose a high std. dev point or two.When you include the full std. dev of all the data it might be increasing your std. deviation of the data and making it harder for the SVM to find a more discriminatory boundaries.,statistics,cu718kf,2,t5_2qhfi
4892136,0,t1_cuad6fz,Could you explain what you mean by this? The prior is supposed to reflect the state of knowledge before any data has been collected typically based on the nature of the experiment such as assigning equal probability to two or more hypotheses if we have no reason to believe one more than the other or if we diplomatically want to give them all an equal opportunity to prove themselves. What would make the prior change during the course of the trial? Data from other trials?,statistics,cub6ycc,2,t5_2qhfi
4920687,0,t3_3i96fi,Is she afraid of all buses or just longdistance bus rides? Id guess that a much larger portion of buses operate within cities and towns rather than travelling on highways. Personal vehicles on the other hand are seem much more likely to travel by highway than buses how many people do you think take buses from the suburbs to cities?.Cars are still likely more dangerous but I wonder if the difference in risk would be as stark if you were to take those relative proportions into consideration. ,statistics,cueshtz,1,t5_2qhfi
4961306,0,t1_cujwq7v,Depends on your personal interests.  Since its for a bachelors youll probably use statistics to solve a problem.  What do you like?  Statistics is nice because you can work on pretty much any kind of topic in any field.,statistics,cujwtv3,1,t5_2qhfi
4992715,0,t1_cul776o,gt you keep the original variable even if it isnt statistically significant and the interaction variables are significant.Thats because weird things happen when you start including interaction terms and you need the individual components for a well specified model. Additionally when you start including interactions it can sometimes remove significance from the other variables by construction. I can understand where youre coming from though Ive thought about your exact question before. I realized its not feasible to do what youre suggesting when you think about how the coefficients are interpreted. Its entirely driven by theory. Plus it makes you a better analyst by thinking about these things than throwing stuff at the wall to see what sticks.This post helped me a lot in understanding interactions,statistics,cunt45q,2,t5_2qhfi
5009958,0,t1_cupgt8m,Yeah not sure why people continue to say R is too difficult to learn.  R Commander is a great way to use it without having to learn to code.  Leads to a natural progression to An R Companion to Applied Regression from Fox as well to learn the code itself or a more combined book like your Discovering Statistics with R.,statistics,cupw17l,3,t5_2qhfi
5045583,0,t1_cutxtdo,That makes sense unfortunately.Those percentages are pretty ominous considering the total population is around  million people.,statistics,cuu6nax,1,t5_2qhfi
5063306,0,t1_cuw2hgs,deleted,statistics,cuwbj9e,3,t5_2qhfi
5064264,0,t1_cutsqr3,While I do agree from purely academic standpoint that he should not do NYU I think the excellent universities in New York have really strong relationship with the industry providing a bridge towards high salary jobs. Its medium risk high reward situation since k is not really that much for a young and talented person in that field.,statistics,cuwfp5h,1,t5_2qhfi
5066252,0,t1_cuw4fjp,Financier detected.,statistics,cuwoav5,2,t5_2qhfi
5090674,0,t3_3kqfeq,It depends. A subject called Introduction to Statistics might not involve anything more mathematically complex than arithmetic say up as far as assuming you can take square roots on a calculator. Some versions of introductory courses are more mathematical and require at least some algebra.,statistics,cuzm9ut,2,t5_2qhfi
5233808,0,t3_3ml1or,There are so so many basic tutorials for R for logistic regression but there are almost none regarding within subjects generalized linear mixed logistic regression which is way more useful for a lot of folks. Sigh. Thanks for the tutorial hadnt seen some of these packages.,statistics,cvgv4c4,1,t5_2qhfi
5277737,0,t3_3n63d4,Hey I am a UIUC  alum with a B.S. in statistics and economics. Been working at healthcare and retail companies in the last  years. Minoring in CS is great because you will have to do a lot of coding in SQL Python C SAS and R. The analytics  job opportunities for college grads are booming. Companies want people who are analytical and eager to learn new things. ,statistics,cvmai56,1,t5_2qhfi
5278887,0,t3_3n9jci,deleted,statistics,cvmft0x,1,t5_2qhfi
5288809,0,t3_3ng24s,Bayes  ,statistics,cvnphpo,-1,t5_2qhfi
5320922,0,t1_cvrssi2,So if you have two players that both generate  points in the same position at the same salary but have vastly different ownership levels... you dont care which player you have? ,statistics,cvrt686,2,t5_2qhfi
5350300,0,t1_cvvj3t1,Thanks Ill keep that in mind!,statistics,cvvk76f,1,t5_2qhfi
5446355,0,t1_cw7gw5e,Ha just avoiding doing the exact same thing for my sampling and surveying course P,statistics,cw7tom2,1,t5_2qhfi
5510308,0,t1_cwfweny,if this is a clinical test then I would expect all testing being described in a SAP and a clinical significant difference in a protocol too. Without this in place is there value in such an analysis?,statistics,cwfzpzl,2,t5_2qhfi
5538311,0,t1_cwjhquw,By being a business analyst and some of them are really good.  One time I watched a BA essentially back his way into linear multiple regression without realizing that he was discovering something that has been in common use for  years.  Sure he had no idea that there were underlying assumptions or such things as sums of squares and degrees of freedom but it was impressive enough for him to get a permanent spot on my talent radar screen.,statistics,cwjkgar,2,t5_2qhfi
5545489,0,t1_cwkg0jt,Ill take the opportunity to ask because Im probably missing something. Even if you have a nonconjugate prior isnt one pointwise multiplication all it takes to go from prior and likelihood to posterior?,statistics,cwkgw83,1,t5_2qhfi
5578836,0,t1_cwokspm,If you look at historical results against the probability predicted by the odds youll find that barring some small aberrations the probabilities are accurate. Since the probabilities are created prior to the event and the event is supposedly independent from the betting line then the conclusion should be that the betting line is a good predictor of the outcome. This is all just another example of how the average of  people guessing how many jelly beans are in a jar will be pretty damn accurate. ,statistics,cwolugo,2,t5_2qhfi
5719248,0,t3_3tgydu,removed,statistics,cx611ub,1,t5_2qhfi
5762440,0,t3_3u2eab,. Data Skeptic  my favorite regarding the math side of data science. Linear Digressions  general talks about machine learning. IBM Analytics Insights  corporate spiel about industry trends and their products. OReilly Radar  interesting discussions with people regarding technology in general. Software Engineering Radio  occasionally has a few podcasts on big data machine learning etc.. Data Informed  mostly blogs but can filter for specific BI podcasts. Freakonomics  stories of data science in action for entertainment,statistics,cxbdudk,13,t5_2qhfi
5809836,0,t1_cxh9dgq,so like chi square? lol ,statistics,cxh9k28,-2,t5_2qhfi
5814540,0,t1_cxhj9hh,This is very helpful. Thanks!,statistics,cxhuj3h,1,t5_2qhfi
5814778,0,t1_cxhtw03,deleted,statistics,cxhvlc2,1,t5_2qhfi
5836981,0,t1_cxkqwdr,Thank you for the reply!The second of those issues I feel I could resolve by tightening to window to  through  or something. I just wanted to make sure I didnt make something useless haha,statistics,cxkr76n,2,t5_2qhfi
5850400,0,t1_cxlrluf,Yep but remember you have to account for possible ties which are pretty frequent.,statistics,cxmikxy,1,t5_2qhfi
5899137,0,t1_cxsvs4w,Its really not that bad.  Basically write the function you want to minimize in a certain way and then let the optimizer find the minimum for you.,statistics,cxswsed,2,t5_2qhfi
5901214,0,t3_3w3o9a,Maybe this is off topic but why in Excel? Unless there is a built in function you would need to do it from almostfirstprinciples,statistics,cxt6l9z,4,t5_2qhfi
5906263,0,t1_cxttxx3,Im glad to hear youre on top of these issues. You certainly have made me feel better! I think the bigger issue is with introductory courses. My soapbox was more of an aside than specifically addressing your course. Its particularly disconcerting to see students enter my field plant ecology and say things like Ive never even heard of R and I thought all populations had normal distributions because thats all I ever learned to do.  ,statistics,cxtueqv,7,t5_2qhfi
6071681,0,t3_3ykki9,Reading your post and your comments predicting probability of default is a supervised learning problem regression and classification belong to supervised learning tasks. For background I am the Data Scientist of an online lending business and my experience tells me that you should look for stateoftheart supervised Machine Learning algorithms Ive never seen Cox proportional hazards model in the industry honestly. Keep in mind that Machine Learning is an active research topic and everyday improvements are made different models different libraries packages and so on but for practical purposes say applying your model in the lending industry I would recommend you to work on scalable sateoftheart implementations this is implementations that are extremely fast and efficient with memory and CPU. If your needs does not require you to be that scalable maybe youre just doing homework you can still look for classic ML supervised algorithms such as SVMs logistic regression and so on  but I would recommend you to try Random Forest first is fast and robust. Finally you should study this tutorial  . If you need help implementing your model feel free to send me a private message. ,statistics,cyfhh3t,1,t5_2qhfi
6225689,0,t3_40ur28,Tobit models are the standard for work with censored data in my field.Im not familiar with imputation in such circumstances it might be fine but I would go with your gut and try for a widely used approach like Tobit.,statistics,cyxak7d,5,t5_2qhfi
6291318,0,t1_cz4mcbb,gt SAS has better upkeep. Thats because its a commercial product not freeI dont think thats why.,statistics,cz4r355,2,t5_2qhfi
6328865,0,t3_42a6v7,In my line of work confidence intervals have two uses. They can easily be visually compared with other confidence intervals and communicated. If they overlap or cross  there probably isnt a significant difference. Theyre also helpful for learningteaching people about prediction intervals. The first reason is probably why theyd be reported.,statistics,cz90to7,3,t5_2qhfi
6340867,0,t1_czaddbl,Youre right I shouldnt have phrased it that way.  It does sound like I might be skeezy.If someone can coach me through running it in R that would work as well but I have  experience with R so Im going to guess that theres a bit of a learning curve.  I actually tried working it out in R  for a few hours but I couldnt get things started.  But yeah in the long run I should probably make the switch to R.,statistics,czadwqz,2,t5_2qhfi
6368575,0,t3_42vau6,inadmissibility of the sample mean in  or more dimensions ie. JamesStein estimator,statistics,czdjcsk,16,t5_2qhfi
6424206,0,t3_43ojd1,Check out ,statistics,czjsswn,4,t5_2qhfi
6443586,0,t1_czlnsp8,gt Intention here is important.Yes absolutely and what do we have?On one side we have people intentionally looking for problematic language in the software code. Which is done exclusively in one language. Noncompliant code is then marked as needing to be fixedreplaced.On the other hand we have millions of developers from all over the world most of whom had to learn that particular language because theres no alternative today. Who most probably unintentionally write code pieces of which can be found problematic when compared against the full lexical richness of said language. And whose code as the result is considered requiring corrections.Now let me tell you this for the first group of people the problematic part clearly doesnt matter. If it did its not that hard to check against at least the most widely spoken languages. That is if one really wants to get rid of offensive language for the sake of not offending people with occasional wrong word. Instead they go on a crusade only in the language they so conveniently were born with and only in software which so conveniently exposes its internals for the whole world to see and work on.For the second group of people its actually offensive because they are being reminded they are secondclass people even when their code works its still not good enough because they are not fluent enough English speakers. Its like always reminding your foreign coworker about their accent even when it doesnt matter in the slightest. So many people not only mastered foreign language to a usable degree but also spent their free time and expertise to make their contribution to a project  who would dare police their language like that and turn a beneficial act into a schooling session? If anything thats exactly the kind of thing which would keep people away just like your coworker Boris will tell you to fuck off and stop talking to you if you decide its a good idea to notify him every time he speaks with a Russian accent.So what was there with intentional being the thing that matters? To me it looks like the first group of people should be given a boot up their ass and the second thanked for their contribution and left alone.,statistics,czlt1ny,4,t5_2qhfi
6448048,0,t3_43ybhi,Im confused is that cover letter supposed to be a joke?,statistics,czm9o0g,1,t5_2qhfi
6468405,0,t1_czn70g0,Yeah I guess depending on where you land theres the opportunity to move from there. Is there a certain industry youre most interested in business intelligence healthcare marketing environment govt  to just name a few? I feel like not all companies are created equal when it comes to statistics and related fields. Even at the masters level I have friends who are in some monotonous jobs or are focused more on report creationexcel work or things that arent really stats heavy so I think its a concern at every level thats not a PhD. From what Ive heard it can be harder at the entry level BS level but I dont have much experience with that.. On top of job searches perhaps ask around to job shadow for a day at some places of interest? Personally Id go for a MS stats degree over an MBA... But thats bias since I ended up doing that.  I also have an undergrad BBA degree so Ive considered that route before.. also IO psych which is kind of a statsslanted psychology industry before landing on my masters in biostats. I believe that it was the strongest degree type out of the three and could get traditional jobs the other degrees could get if I really wanted to go down those paths but I know that there are jobs such as my own that I couldnt have if I took the MBA or IO psych paths. Not that any are bad just it depends on what type of industry you want. I wanted research  in the healthcare industry. For differences between MBAs traditional vs analytics types  its probably more seen in different course catalogs and it is a newer trend. The degree as a whole is trying to be refreshed I feel. Personally I think if you do an MBA program either be out of school for a few years AND go to the best school possible.. I think in MBAs school brand recognition and their networks matter far more.TLDR depends on what industry you wanna be in. I prefer MS stats but Im biased. ,statistics,czodnhy,1,t5_2qhfi
6762616,0,t1_d0iq09w,If you want to do a fullon class I suggest the Data Science Specialization class from JHU on Coursera Just do the audit version not the certificate and its free. The whole class uses R and goes from basics of R programming to graphing analyses etc... It introduces a lot of useful packages too. Theres an R package called Swirl also from JHU that can help you get down the basics as well.,statistics,d0itk7a,1,t5_2qhfi
6786148,0,t1_d0gcrtb,If you are just getting means and whatever then it is perfectly fine. If you work with any type of data where you have to mix data sets mangle text do any complicated calculations or advanced statistical methods excel just isnt up to par. Especially if you have big data.,statistics,d0lhwj8,1,t5_2qhfi
6806596,0,t1_d0nq91q,Did any of your calc classes include multivariatevector calculus? E.g. things dealing with double and triple integrals.If not take another calc class or two calculus is very important for statistics. It shouldnt be too hard to pick up the rest of the necessary calc since youve already got a good calc background.If so start taking probability and statistics courses in your schools math department if you can. The mathematical way read the right way of understanding probability and statistics is based on probability distributions like the normal distribution defined by their probability functions. As such you can use calculus to obtain a myriad of information from them! For instance among many other things within the first one or  courses youd likely be able to answer at least the Spearmans coefficient question the Bernoulli process question and the MLE question.If you dont have room in your schedule to do the stats course you could get a textbook and try learning on your own. There are tons of excellent resources. Hogg Tanis and Zimmerman is pretty good for an introduction though Im sure theres better out there.,statistics,d0ntmyq,2,t5_2qhfi
6836536,0,t1_d0r7jpb,It mostly comes down to checklists like the CONSORT checklist which have their own problems but research definitely benefits from them. On the other hand I disagree that pvalues are the least bad thing to separate good from bad research. They shouldnt even enter the equation.,statistics,d0r827m,1,t5_2qhfi
6839268,0,t3_49dc8t,I am VERY excited about this statement which is long overdue. Pvalues are not generally the issue. The issues are generally due to hypothesis testing as a form of scientific decision making. Getting rid of pvalues wont necessarily get rid of this idea but its progress.The same discussion happening in rmath Its interesting that they seem to be getting the interpretation of pvalues pretty wrong for the most part. It shows that good statisticians are in more need than ever.,statistics,d0rj97u,3,t5_2qhfi
6853782,0,t3_49n80m,Haha I dont think hes been way off very often. Care to cite some examples?,statistics,d0t6ney,2,t5_2qhfi
6886904,0,t1_d0wvnn8,deleted,statistics,d0wy4hg,1,t5_2qhfi
6908733,0,t3_4ad1u2,When it comes to statistics typically it is trying to figure out what range or possibilities are consistent with some data. Suppose Im making observations of some unknown sided dice and counting the times it lands on . I may only perform  dice rolls and my task is then to consider the number of sides that the dice had that seems reasonable given the data.  You can probably calculate a range of possibilities using an analytical approach. Im not particularly good at it and when I do succeed at it I often have a hard time explaining it to others. What I am good at is programming and I know that my computer can simulate lots and lots of dice rolls very quickly. My approach then is to simulate dice rolls of each possible number of sides and consider the range or sides most consistent with my data obtained from the actual dice with unknown number of sides.Intuitively if I had a high number of landed s out of  I can probably assume it is a dice with a low number of sides and if I dont see  even once it probably has a high number of sides. The exact number may not ever be determinable but you can certainly use simulation to help in understand what range of sides makes sense given the data.,statistics,d0zfarj,1,t5_2qhfi
6949886,0,t1_d13yj96,No.  This is mostly preprocessing and the basics of getting data into python and making it ready for analysis.  This is what you will be spending most of your time on so learning python and data cleaning together makes this book a great combo.  Also odds are one of your interview questions will be how to solve one of the common data cleaning problems. So for an introduction into the theory of data science this is a poor first book.  For a way to get into what you will actually will be doing in data science this book covers the required skills that statistics texts never cover as they assume your data is already in the proper form to analyze it.,statistics,d143nwj,2,t5_2qhfi
7031214,0,t1_d1d5ljz,deleted,statistics,d1dccdh,2,t5_2qhfi
7040052,0,t1_d1e7qro,This is a big deal that is news worthy because it shows more and more foundations and companies are getting behind R.,statistics,d1echk2,2,t5_2qhfi
7047956,0,t3_4c5bgj,Even though you know for a fact that at least  observations are positives the confidence interval is calculated for all samples. Your estimate does not have a confidence interval the sample proprortion distribution does.For example you could draw another sample from the population with even  positives.,statistics,d1f8s5w,4,t5_2qhfi
7051987,0,t1_d1fp28o,Im hired!,statistics,d1fp97c,1,t5_2qhfi
7181236,0,t1_d1ua1cr,Yes I considered this but dont have much experience with signal processing.  I there any specific approach you could suggest?  Thanks.,statistics,d1ufysi,1,t5_2qhfi
7194113,0,t3_4dzp9y,Ive been reading Freedmans Statistics e and I really like his style  its very readable and approachable.  You can get this edition for about .,statistics,d1vwtrh,2,t5_2qhfi
7206139,0,t1_d1x7c8k,Not sure about cointegration but yes you can measure correlations between two stationary time series usually in the form of CCF. Spectral coherency might be important too but Im not sure if spectral analysis is common in finance.,statistics,d1xa412,1,t5_2qhfi
7237597,1,t3_4ejqmx, up votes and no comments on a sketchy looking link in a pretty small sub. Yeah seems legit. ,statistics,d20v223,2,t5_2qhfi
7279598,0,t1_d25mc9a,Yes however from the same evidence after  hours the first method  day would give pbeing killed by lightning in the next day   the second method  hours would give pbeing killed by lightning in the next day   pbeing killed by lightning in the next  hours      ,statistics,d25n3za,1,t5_2qhfi
7282170,0,t1_d25tzlr,No problem man Im sitting waiting for this concerto to begin but when I return for the evening Ill lay you out a bit of the axiomatization of probability theory and explain some of the philosophy as well. Then we can discuss what problems measure theoretic probability can and cannot address and then which problems the frequentist view are naturally fitted and to which the Bayesian view are more naturally aligned. We can then discuss the differences in them for certain problems and chat a bit about when and why they sometimes rarely disagree. At that point we can address your question with a bit more solid ground to stand on.,statistics,d25xneu,1,t5_2qhfi
7317822,0,t1_d28snvo,Any advice for what to saynot say when sending a stranger an email or LinkedIn message? I like the idea of receiving advice from someone with the kind of job that I want but I want to make sure I make a good impression.,statistics,d29zj2o,1,t5_2qhfi
7463564,0,t3_4hkvre,I need to find this out too. Isnt that what an ANCOVA does ,statistics,d2qk7jx,2,t5_2qhfi
7470239,0,t3_4hoz84,I was thinking of going down the path of integrating the normal distribution function from  to our production point. I could do this discreetly assigning a probability to each demand amount and finding profit based on a given order level. Then do the sum product of both columns. However this proves very tedious when dealing with larger production amounts or when performing this decision over several products. ,statistics,d2rbk4z,1,t5_2qhfi
7570278,0,t3_4ixcx8,What experience do you have outside of school work?What field do you see yourself in?Where do you live?What other offers do you have?Depends on those things firstly. I dont think its a big deal if you continue school you can pivot from there without much issue. Experience with data is better than classmates with nothing or a  month internship. Im sure there are transferable skills. ,statistics,d32pcfs,1,t5_2qhfi
7590437,0,t1_d34ycd1,Theres a book called Fightnomics which goes through the factors that affect win rate.Short version Reach matters Height does not Age matters a lot Missing the target weight high or low matters Punchingkicking frequency matters Downtime since your last fight matters Everyone does worse against a southpaw even southpaws There is a home cage advantage,statistics,d34zuec,4,t5_2qhfi
7642585,0,t3_4jw2g0,CramerVon Mises is a set of distribution. Its used to test goodness of fit of your empirical cdf  I believe vs a completely specified cdf.You subtract your empirical cdf from your proposed cdf and multiply it by some weight function.This is a page from my notes. The CramerVon Mises goodness of fit test was introduced as a flawed test thats fallen out of practice.,statistics,d3ax52i,3,t5_2qhfi
7873236,0,t3_4n1qkb,The only mathematical prerequisite for Bayesian statistics is arithmetic.But if you can program in Python or R thats very helpful.,statistics,d410vtb,2,t5_2qhfi
7924557,0,t1_d46s19m,For what its worth WAIC and LOO are closely related,statistics,d46sn6c,1,t5_2qhfi
7954270,0,t1_d49zws0,You cant but that wasnt my question.,statistics,d4a4utv,2,t5_2qhfi
8103701,0,t3_4q6o1o,Huh. I really liked it. Its been a while so I dont remember everything but I know I sped through it in a few days.,statistics,d4qxgcm,2,t5_2qhfi
8124245,0,t1_d4stleh,removed,statistics,d4t8ku2,1,t5_2qhfi
8194908,0,t1_d51rrd0,No they measure the likelihood that the given outcome or a more extreme outcome was observed under the given the null hypothesis.For example I am  feet tall and live in America.  Lets say the probability for an American being more than  feet tall is  given the distribution of heights in America.  Now lets say I move to Japan the chance of seeing someone taller than me is only  because people are shorter on average.My test is identical how many people are taller than me but the background distribution my null hypothesis is quite different and gives me different pvalues.NOTE These are not actual pvalues but they illustrate the point that pvalues depend on the model of the null hypothesis,statistics,d51wyw3,1,t5_2qhfi
8357730,0,t1_d5m6u2h,Normally ha! you would be right in the sense that the ttest is robust to small departures from normality with smallish sample sizes. In this case though with a small range of responses and bounded distribution makes it inappropriate.,statistics,d5m9nzw,4,t5_2qhfi
8412500,0,t3_4uuudo,Im not sure what you are asking for. Questions like how old are you? what language do you speak? and can you read? dont really require any kind of statistics to answer. ,statistics,d5t3uac,1,t5_2qhfi
8436406,0,t1_d5qi5xn,dont listen to this cunt,statistics,d5w39e5,-2,t5_2qhfi
8493830,0,t1_d6391h0,How big would my sample size have to be? What test would I use in that scenario?,statistics,d63a1an,1,t5_2qhfi
8496302,0,t3_4w32o5,Representation learning maybe,statistics,d63l7ou,2,t5_2qhfi
8554194,0,t1_d6amapw,It really doesnt matter at all... You get the same results in R SPSS STATA and SAS.,statistics,d6auk3v,1,t5_2qhfi
8570790,0,t3_4x6kgw,deleted,statistics,d6cxi9q,1,t5_2qhfi
8600027,0,t3_4xihny,I found this site from Duke a great reference when doing my Time Series modeling course a few quarters back ,statistics,d6glh8v,2,t5_2qhfi
8812255,0,t1_d76xem1,INDividual...INDependent... Sounds right to me lol,statistics,d77438s,4,t5_2qhfi
8883717,0,t3_51xewl,Yes it is correct. Another way of putting it is There is a decrease of pp. i.e. percentage points in ...,statistics,d7fp3mf,3,t5_2qhfi
8893093,0,t3_51zajn,Yes I did that and highly recommend it. Most statisticians are not good enough with programming and most computer scientists dont know enough statistics to understand well statistical machine learning methods. A dual major will prepare you best for data science. Btw the thing about domain knowledge being more important is completely untrue. Domain knowledge can be learned on the go which is not the case for basic statistical and CS skills. See quote from Tukey gt    The best thing about being a statistician is that you get to play in everyones backyard.,statistics,d7gtnc7,5,t5_2qhfi
8897172,0,t1_d7habh2,I think please correct me if Im wrong! sigma will always be positive because I parameterized all the coefficients as lognormals which must all therefore be positive. Since the servicei variables are all  or  it should always give me something gt.Probably should have used something else other than lognormals but it was just a toy example as I stated elsewhere.However Ill try using column vectors inner products and multivariate normals and see if that gives me a speed boost! Seems like it should.Based on the responses Ive received so far it sounds to me like getting a decently fast model is partly to do with little tricks in the parameterization acceptable simplifications and using no more distributions than you need to. There is no secret formula I was missing  its more like a lot of little things to do with tidying up my code and my model.,statistics,d7hb9rr,1,t5_2qhfi
8940114,0,t1_d7lamkx,I did use a logit regression with different methods of data input  my findings were inconclusive unfortunately,statistics,d7mgx8u,1,t5_2qhfi
9010431,0,t3_53ooc0,deleted,statistics,d7uww0i,1,t5_2qhfi
9014908,0,t1_d7va1wd,gt Are the  results even significantly different from one another? Who knows...Statisticians compare the results from various models all the time in the statistics literature but Ive yet to see anyone do a test of whether the results of models fit to the same data are significantly different. What would the null hypothesis be precisely?,statistics,d7vg9ui,3,t5_2qhfi
9042710,0,t3_5430u5,Just crush the gre or gmat and you will be fine. Also if you can do volunteer work and network it helps tremendously.,statistics,d7ysf2h,0,t5_2qhfi
9051169,0,t1_d7zspli,Example using R ,statistics,d7zt1k9,2,t5_2qhfi
9089161,0,t1_d83zwut,Youre using predicted values as the data right?  Not actual selling values?  And  are more than  standard deviations away from the mean?  You dont see an issue with this?  ,statistics,d84dcj5,1,t5_2qhfi
9151159,0,t1_d8bmmtr,Sounds like you should be OK. ,statistics,d8bz50a,1,t5_2qhfi
9158323,0,t1_d8cutl6,What pvalue are you looking at?I suspect that he chisquare uses  Yates correction and the pvalue doesnt.Youve got a big discrepancy so its likely something like that. But you can have small discrepancies sometimes too. The reason is because the pvalue says Whats the probability of getting this data if the null hypothesis is true and the confidence intervals says If I had this data how big would the intervals be? If you have something like means thats not a problem but for things like odds ratios and correlations the variance is correlated with the magnitude so you get different standard errors. Andy Fields SPSS book has a section on this.,statistics,d8cvv1o,10,t5_2qhfi
9176240,0,t1_d8f3xx0,The sums would not give the correct likelihood values if substituted for the means nor would they allow you to estimate the covariance matrix correctly which requires centered variables i.e. variables with the mean subtracted.,statistics,d8f5m8i,1,t5_2qhfi
9232237,0,t3_56szqd,A quick preface Following Andrew Gelman and Jennifer Hill I dislike the terminology of fixed effects and random effects since its not always clear what someone means when they use one or the other term. I think its more useful to think about these models as multilevel models or not.Based on what seems to be the most common use of the phrases a generalized linear model that only has fixed effects is not a multilevel model. It just has one set of coefficients that relate the independent variables to the dependent variable. Adding in random effects makes it a multilevel model. So I dont just repeat myself heres a comment I wrote about this a while back.Okay so if you have data with one independent variable and one dependent variable and you use a generalized linear model with an intercept the model will have two parameters namely the slope and the intercept.If you have repeated measures for each subject and so you want to add in random slopes and intercepts then you would have two parameters for each subject a slope and an intercept plus the fixed effect slope and intercept plus the variances and covariances of the random effect parameters.The same basic idea applies to an interceptonly model which is what it sounds like you are describing. It also applies to models with more than on independent variable in which you will have additional slopes intercepts and covariances.,statistics,d8m953y,5,t5_2qhfi
9240902,0,t1_d8lrjsd,I tried your approach suggested above with a few sample questions. I guess it worked with most of the questions. I had missed some of the questions though.,statistics,d8ncnvs,1,t5_2qhfi
9283624,0,t1_d8sp5aa,Everyone knows that anecdotal evidence is as good as collected data. Thats the first thing I learned in my stats degree. ,statistics,d8srm54,3,t5_2qhfi
9284741,0,t1_d8rmdv1,Yeah for Bachelor theses we usually collected a really wide range of variables since multiple people would work with the same dataset just with different parts of it. In my case she suggested that I would just throw a variable that had nothing to do with my theoretical rationale into the model as a moderator see what happens and then adjust hypotheses if necessary .I didnt do it and my grade suffered for it...,statistics,d8swpe1,1,t5_2qhfi
9304177,0,t3_57tjhy,Im a PhD in stats on the academic track but Im involved in our departments professional development activities. I think the job market will still be hot in the next decade from what Ive heard from industry folks.Do you have a stats MA you dont identify the discipline? I ask because I wonder if you wouldnt be better off with a stats MS. Given the opportunity cost of a PhD if your goal is maximizing lifetime earnings a masters gets you in the field faster. Im not too familiar with the salary differences so perhaps Im undervaluing the additional benefit of a PhD. The bureau of labor statistics could be useful on this point.,statistics,d8vde83,1,t5_2qhfi
9559387,0,t3_5btv1x,Udacity,statistics,d9re2qu,2,t5_2qhfi
9576382,0,t1_d9t7jxf,Hmm. Actually he failed worse than a bunch of models that did predict Trump. He might have been the best at one model class but that whole class failed. So his results are just well meh like most others this time.,statistics,d9thgdn,1,t5_2qhfi
9576389,0,t1_d9t492b,Nope. He reported both a proper forecast the probability that each candidate will win and a Nowcast which gavethe probability that each candidate would win if the election were held that day. The actual forecast spent a lot of time hovering around .,statistics,d9thhhs,2,t5_2qhfi
9587163,0,t1_d9urhvn,Thanks. Im not exactly sure what his point is but I think hes saying that  treats the actual probability of one candidate winning as a set value instead of a random variable itself.  Because of this the actual probability of any outcome is closer to  than the expected probability that  is claiming as actual probability for Hillary.  In other words dont tell people that your  predicted probability is the actual probability when it is just the expected probability.  Not sure if its valid argument though.,statistics,d9utav6,3,t5_2qhfi
9655640,0,t1_da35cyb,No because you dont do two analyses. First you match on set of  variables using propensity scores and get weights or whatever you are doing. Then you add the variables as covariates in your analysis. The idea is that propensity scores work if your covariates predict assignment and covariates work if your covariates predict the outcomes. So you do both and as long as one of these is correct youll get unbiased estimates. ,statistics,da39bql,2,t5_2qhfi
9656221,0,t3_5dab84,The two are equivalent statements,statistics,da3bwqp,2,t5_2qhfi
9669558,0,t3_5dj932,found your favorite subject and read everything about it until you have a question,statistics,da4z4ro,22,t5_2qhfi
9669971,0,t1_da50i57,deleted,statistics,da50z0f,2,t5_2qhfi
9712908,0,t1_da9mmz4,gt I see another problem with Frequentist statistics though but I might be wrong. Lets say we have some data x seen as a realization of X. Lets also assume that the sample mean sX is an estimator for a parameter theta were interested in. While we can construct a confidence interval CIX for the sample mean I dont think we can conclude that thats also a CI for theta.The CI is for theta not for a point estimate sX. Your definition is wrong its Ptheta in CIX  . evaluated using the sampling distribution of the interval CIX.,statistics,daabqh0,2,t5_2qhfi
9714221,0,t1_daagwns,What you said is why Im having trouble. The actual chances odds are close to  but are definitely not. Players are matched against similar scored players so that the match is close to . Players dont usually know the history of their opponents and instead they mark their predictions on starting conditions like being the White player in Chess.,statistics,daahkzh,1,t5_2qhfi
9715547,0,t3_5e7qn1,It depends on the employer but in my experience most wouldnt be able to tell what the difference between an applied program is vs. a theoretical one. ,statistics,daanh1u,1,t5_2qhfi
9724626,0,t3_5eds1c,I think it would be difficult to prove.But then I think the premise is illogical.  If Vegas is going to assume that kickers will miss PATs a little more often it is going to be a fraction of a point difference per team per game.  So it should have no effect on the spread.MAYBE it could affect the totals.,statistics,dabrub6,4,t5_2qhfi
9725535,0,t3_5eesa4,In principle the null hypothesis can be any specification about parameters.  Anything at all.,statistics,dabvvxp,5,t5_2qhfi
9735059,0,t1_dad19gm,Your point on the definition of the segments makes sense but we are now left with the question of whether or not the use of paper vs electronic ballots is correlated with the segments that Nate identified.  If electronic ballots were more often used in counties with high percentages of minorities and no college degree then his analysis showing that the effect of paper ballot goes to nonsignificant near  b weight when the segment variables are added to the analysis is actually statistical evidence that is consistent with a hack.In other words he ran Step I and Step III of a mediation analysis but because he skipped Step II he misinterpreted the results.I dont actually think there was a hack.  This is more a critique of incomplete analysis and then broadcasting I know the truth.,statistics,dad23yq,1,t5_2qhfi
9764198,0,t3_5ez0y4,log odds ratio as it gets so little love from epidemiologists.,statistics,dagnkob,2,t5_2qhfi
9792278,0,t1_dak3o5e,Yeah but how would one go about detecting it? ,statistics,dak3zys,2,t5_2qhfi
9847657,0,t3_5gcg3o,removed,statistics,dar67ts,1,t5_2qhfi
9854819,0,t1_darvpu4,score on y publication on x? but publication is categorical.,statistics,das3td7,1,t5_2qhfi
10012960,0,t1_dbbn8f1,EXACTLY!With survival analysis you can do the analysis you asked about  longitudinal evaluation dichotomous or more complex outcomes and include a host of covariates with statistical significance tests confidence intervals etc.  The real deal.If the person is lost to follow up you dont have data on the terminal outcome but you do know how long you followed that person without observing the outcome.  So you can analyze their available data and the status lost to follow up does not bias the modeling.And if you study everyone to a same fixed timepoint and at that point the study is over no one is lost to follow up and you can still apply survival analysis.PM me for more information and examples I use survival analysis in a lot of my modeling it is very powerful.,statistics,dbcpubk,2,t5_2qhfi
10020762,0,t1_dbdbdvd,Time is a fixed factor. What software do you use?,statistics,dbdqeii,1,t5_2qhfi
10035265,0,t1_dbf6nty,I recommend a book called Mind On Statistics. It was written by Bob Heckard and Jessica Utts and their intent was to create a fun intro to stats course that focuses on simple pointandclick analyses you might find in MiniTab SPSS MS Excel etc. Youll learn applied level Ttests regressions ANOVA etc. The book has been used for years and there should be really cheap used copies out there. If thats too basic and you want to use that linear algebra then I like Applied Linear Statistical Models by Kutner Nachtsheim Neter and Li. That book really goes into the basics of running a regression and examining all the diagnostics which is really important.Theres also plenty to learn out there for free using tutorials for R and free access to the SAS or Matlab manuals for example. There are hundreds of course notes and lessons out there on the web on YouTube. Even Wikipedia is pretty decent for some stats subjects.,statistics,dbfmdx6,1,t5_2qhfi
10112078,0,t1_dbpk79s,Example?,statistics,dbpm34f,3,t5_2qhfi
10123809,0,t1_dbr3ph3,yes maybe i wasnt aware that only published scientifc work was postable here.,statistics,dbr50ai,0,t5_2qhfi
10184526,0,t1_dbyepmx,Yes. Especially in academia R is ubiquitous.  The entire point of a doctoral program is to be doing new and interesting analyses and SPSS is spectacularly illsuited to that task. Source learned R in my PhD program nobody in the department ever used SPSS.,statistics,dbyh0ks,2,t5_2qhfi
10265372,0,t3_5mzgwi,Im not entirely certain about how you defined your problem you should probably draw a diagram to demonstrate the flow and therefore possibilities of failure.Also I believe this question is more appropriate in rHomeworkHelp so Ive flagged it for the mods but repost it over there and PM me and Ill be happy to help.,statistics,dc7mwg4,1,t5_2qhfi
10290330,0,t1_dc9inr3,Thanks I tried that. Actually what ended up working was opening quartz before I tried to run commander. Maybe I was trying to use it too soon free updating my program and my computer just hadnt caught up yet somehow ,statistics,dcagqq5,1,t5_2qhfi
10339737,0,t1_dcezett,Thanks a lot thats a lot faster than what I was trying to do.,statistics,dcg2ggj,1,t5_2qhfi
10359458,0,t1_dciamdh,Are there any specific papers you can recommend?  ,statistics,dcib01a,1,t5_2qhfi
10394713,0,t1_dcm98hn,The Cost Per Conversion calculation I can get easily from my analytics tools. What I need some help with is running the two sample ttest on the difference in cost per conversion.  ,statistics,dcmb2j4,1,t5_2qhfi
10453587,0,t1_dcsxou4, Matplotlib  Imagemagick for the gifs TikZ for the nd pick Tensorflow for RMSPROPgradient descent,statistics,dct0m5u,3,t5_2qhfi
10489449,0,t1_dcwo3se,I have to second the notes about interactions. But no yet mentioned also dont forget to interact the variables with themselves.Another item not yet mentioned is whether the type of wood is available? If it is include type of wood as a categorical in the regression.Lastly if you or your audience is a stickler for understanding what a  increase does you should log the data.,statistics,dcx37sg,2,t5_2qhfi
10545321,0,t1_dd3dtbf,Multivariate more than one outcome analyzed at a time think repeated measures.Multivariable more than one predictor in a model i.e. multiple regression.,statistics,dd3fhgi,2,t5_2qhfi
10580785,0,t3_5rdp7h,We should have a blanket ban on Data Science Central.  Theyve been involved in some very questionable things in the past like fake female accounts  Plus they post a lot of articles that are frankly crap,statistics,dd7br5e,1,t5_2qhfi
10601621,0,t1_dd9h3ii,Pardon my wording. I mean R has the widest range of domainspecific libraries. Matlab and python have a lot but R has them beat.,statistics,dd9h5lo,18,t5_2qhfi
10612198,0,t1_ddahv0d,There are multiple different groups of observations that are in their own regressions and Im interested in being able to compare the significance of the residuals aggregated across these regressions. So in a group with a less good fit the magnitude of a residual should be scaled to accommodate that it is less significant than a residual of the same magnitude in a better fit group.I would imagine that this means Im not so concerned about the normality of the distribution as Im only interested in using it to scale the residuals relative to each other.Edit Youre completely correct z score was the wrong terminology for this. Sorry for the confusion.,statistics,ddakflw,1,t5_2qhfi
10632566,0,t1_ddcivd4,This actually sounds perfect.  birds  stone. ,statistics,ddco2p3,1,t5_2qhfi
10685691,0,t1_ddi0mpb,The standard error estimates how far our average is from the true average of the distribution. In this very abstract discussion though its not exactly clear what the true average means if anything at all. Ideally the true average is what youd get if you continued collecting data for lets say Sample  for two billion years. But of course thats assuming that nothing important changes for the next two billion years. For instance if these are the dates of major railroad accidents there probably wont be very many of those in two billion years. In fact in a case like that Id be very skeptical about assuming our current data points are identically distributed which throws a lot of what were doing into question.Alternately suppose the data of sample  came from an experiment that can be repeated. Then we could repeat it many times and calculate the average many times. The average of those averages should be close to the true mean and the standard deviation of all those averages is the standard error. The outcome of any given experiment for instance the one we performed is expected to be about  SE away from the true mean typically. But again when were dealing with historical data it might be profoundly impossible to repeat this experiment or even conceptualize what that would mean.      ,statistics,ddi5cz4,2,t5_2qhfi
10699230,0,t3_5szubf,Good question that I often wrestle with. I do marketing analytics of specifically targeted customers. We hold a control group and look for the difference in behavior. But we always do the confidence interval stuff and I wonder why the debate? The target group average sales was clearly and measurably  more than the control average.. I look forward to the discussion in this thread. I guess the confidence interval is useful for answering are the two groups results truly different and attributable to our marketing efforts or is the target group average about in the range of what the control groups average could have been anyway. ,statistics,ddjjo1r,2,t5_2qhfi
10720355,0,t3_5tbxca,Thank you for answering!,statistics,ddlq4fm,1,t5_2qhfi
10757638,0,t3_5tvdcq,So you are basically confusing the purpose of the bootstap.   The choice is not between the mle and bootstrap as one is an estimation tool and the other is used in this context for quantifying the uncertainty of parameter estimates.  So you could use a bootstap to obtain a CI for your mle estimate.,statistics,ddpkjae,24,t5_2qhfi
10767543,0,t1_ddq9v04,Nit picky but D  p values are difficult to compute for all but standard cases. ,statistics,ddqlbzh,1,t5_2qhfi
10840009,0,t3_5uzmg1,Im rocking a ThinkPad T running elementary  OS. It works well  most of the software I use works well under Linux R MATLAB LaTeX etc. and Ive definitely noticed a step up from the old HP beater I used to use.,statistics,ddy2jah,2,t5_2qhfi
10871524,0,t1_de1a09w,Could be difficult to tell but if you have enough data a central limit theorem might help to treat it as Gaussian,statistics,de1bjde,1,t5_2qhfi
10947803,0,t3_5wcrp6,The confounding variable is someone else who threw both items that hit Sheldon at the same time thus discounting the unexpected nature of that event.,statistics,de96sk5,2,t5_2qhfi
10968914,0,t3_5wmn9w,You cant statistically. If you have a distribution and the variance of your data you can estimate a confidence interval. You can estimate things like we can be  sure that fraud is less then X. Theoretically the upper limit to fraud in a day is all credit card spending for that day.,statistics,debd6ll,0,t5_2qhfi
10992798,0,t3_5wytei,I like Cowpertwait.  Its a really good beginners book with minimal math.  But you should use the forecast package for all time series work.  If you want something a bit more advanced Forecasting Principles and Practice is great free and is taught using the forecast package.,statistics,dedzyzo,4,t5_2qhfi
11042268,0,t1_dejlyl7,. Draw points. Draw lines. Make points fit line. ???. Profit,statistics,dejme3i,2,t5_2qhfi
11053407,0,t1_dekrzr3,gt Why would you need IT for data science?I dont think uaxapu is suggesting you need a degree in IT. Just that Data Scientist tend to have more IT knowledge in their toolbelt than a statistician necessarily would.,statistics,dekvv62,2,t5_2qhfi
11063070,0,t1_delooi4,hey thanks for offering the help! how are tests and homeworks like for the online MS in applied stats at penn State? how are classes taught? is it fully online or are you expected to visit the campus at all? ,statistics,delzbkb,2,t5_2qhfi
11082516,0,t1_deo5jw0,Well not a faulty part per se rather there is expected user variation i.e. differences in technique yeah only problem is the whole user base currently consists of only two users...we dont know the absolute value...all we have is the result from these two people testing the same sample one time each. So the absolute value would be the average. ,statistics,deo6yzi,1,t5_2qhfi
11129419,0,t3_5ytvke,Without needing to dive into the technical aspects of statistics we can discuss a point often glossed over when using statistics for public policy debates. This is one of the first things we covered in our experimental design discussions back in school. Statistical methods like linear regression or ANOVA which help try to predict or classify a event are only valid in the context of a controlled experiment. When public policy researchers use these techniques theyre at a greater than average chance of spreading a post hoc ergo propter hoc logical error. ,statistics,detid8o,1,t5_2qhfi
11130615,0,t3_5yu787,To answer that sort of question you need data. Were not usually a source of data but if you had some suitable data we might be able to help you figure out if there was a biggerthanchance difference in signal use or whether it was just noise. ... On the other hand getting suitable data is not easy the number of other variables you need to be able to control for is large so youd probably need a randomized controlled experiment and then you have the problem that people behave differently when they know theyre being observed.From my personal experience I think women are more likely to use turn signals ... but even if that perceived difference held up where I am Id expect theres variation across and probably even within countries.anecdotes are not data,statistics,detn8pr,2,t5_2qhfi
11200509,0,t1_df1e3f2,If you number every widget such that you can say that there are  different ways of selecting two then there are also way more  than  different ways that a box could only have one widget. Because a box could have one widget because you removed the first two but it could also have one widget because you removed the last two...Edit Actually there are three different ways of removing two widgets from a box then. If you reflect that in your calculation you get    and the complement .,statistics,df1kb2q,1,t5_2qhfi
11207528,0,t1_df1zzgd,Thank you!,statistics,df2czi5,1,t5_2qhfi
11217006,0,t1_df37wbr,Im not looking for a professional consulting. Just someone interested.,statistics,df3fmw0,1,t5_2qhfi
11287463,0,t1_dfbdmwp,Can confirm PhD works.,statistics,dfbf1od,1,t5_2qhfi
11321973,0,t1_dffb6oi,Include the circle and asterix seen in the image into the boxplot instead of making them outliers. i.e. changing the maximum whisker.,statistics,dffbrhj,1,t5_2qhfi
11332142,0,t3_61pxx8,Fourier transforms are used in time series. ,statistics,dfgh98w,22,t5_2qhfi
11348705,0,t1_dficf7g,Both the relative numbers and the absolute numbers are important  both comparisons are meaningful but they provide different information. Say that men born in Outer Mongolia in the Canadian armed forces were  times more likely to suffer SHSA than men born elsewhere. That sounds bad  so maybe we should divert our resources to focus on men born in Outer Mongolia. But then if I tell you that there are only  men in total born in Outer Mongolia in the whole of the Canadian armed forces. So probably in that case we shouldnt spend all of our resources on this highrisk group.Certainly the average experience is worse for women than for men  times worse. But you are right  that doesnt necessarily mean that  times the number of counsellors are needed for women than for men. You have to keep both the relative risk and the absolute risk in mind when apportioning resources.,statistics,dficsc9,5,t5_2qhfi
11361484,0,t1_dfjs0jm,gt Asymptotic relative efficiency.Ha ok I do remember learning that in grad school at some point the pi definitely ring a bell. Thanks a bunch for the refresher! Brings back memories. gt It may not be a particular issue for what youre doing a lot of the time but if the variables youre dealing with may be quite skewed so the pairdifferences may be quite heavy tailed it might sometimes be a serious issue.Yeah you can do definitely do better if you tailor the difference in means test to the particular distribution of the paired loss differences. I picked the Ttest more for ease of exposition than for optimality perse.,statistics,dfjsx5f,1,t5_2qhfi
11366730,0,t1_dfkdy07,Right but from what I can tell theres no indication this is the only class of distributions for which the inequality holds.,statistics,dfkeatc,1,t5_2qhfi
11470049,0,t1_dfv2kme,So thats what the p in p value stands for. Pure luck ,statistics,dfw4hjn,1,t5_2qhfi
11480807,0,t1_dfxc5cu,Yeah its unusual. But I figure its better to have high standards than excessively low standards. Anyways try googling the followingCenter of Actuarial ExcellenceFind those schools and research their Statistics programs. Its always hard to gauge what the best statistics program is. I know Purdue is quite good and I imagine Univ. of Illinois is too.,statistics,dfxchma,2,t5_2qhfi
11603855,0,t1_dga1gj2,This is somewhat true but biostats is not limited to clinical trials. Statistical genetics health claims data gut microbiota and medical imaging data come to mind as biostat topics that have a huge amount of data.,statistics,dgbb1il,2,t5_2qhfi
11627737,0,t1_dge0az0,No. I just found it interesting. Then I thought you were suggesting its never too late for me to learn a new language. I wasnt disagreeing with you nor suggesting OP should learn SPSS.,statistics,dge0lmw,-5,t5_2qhfi
11630677,0,t3_65zccb,R without a question.,statistics,dgeclou,133,t5_2qhfi
11737110,0,t3_67gykw, Applied Predictive Modeling by Max Kuhn and Kjell Johnson Introductory Time Series with R by Paul Cowpertwait and Andrew Metcalfe Time Series Analysis With Applications in R by Jonathan Cryer and KungSik ChanI also recommend Introduction to Statistical Learning and Elements of Statistical Learning but theyre available online for free so no need to hoard P,statistics,dgqfjad,1,t5_2qhfi
11809061,0,t3_68f7xv,It wont be a negative. But it probably wont affect anything too much. A more positive thing you can is get an MS in stats. That being said this will probably be the last time you get to seriously learn philosophy. I took an undergrad degree in math and economics and that was a lot of fun! So the philosophy major would be more of a personal choice rather than a practical one do you want to learn philosophy and spend the credit hours and time doing it or would you rather get out and get a job? ,statistics,dgyljh5,3,t5_2qhfi
