,controversiality,parent_id,body,subreddit,id,score,subreddit_id
2455578,0,t3_2nnr2k,The Global Vectors for word representation GloVe introduced by JeffreyPennington et al. is reported to be an efficient and effective method forlearning vector representations of words. Stateoftheart performance is alsoprovided by skipgram with negativesampling SGNS implemented in thewordvec tool. In this note we explain the similarities between the trainingobjectives of the two models and show that the objective of SGNS is similarto the objective of a specialized form of GloVe though their cost functionsare defined differently.,statML,cmf7mcd,1,t5_355ac
2535371,0,t3_2oqac1,Jean Lafond LTCIOlga Klopp MODALXCRESTINSEE EricMoulines LTCIJospeh SalmonLTCIThe task of reconstructing a matrix given a sample of observedentries is knownas the matrix completion problem. It arises ina wide range of problemsincluding recommender systems collaborativefiltering dimensionalityreduction image processing quantum physics or multiclass classificationtoname a few. Most works have focused on recovering an unknown realvalued lowrankmatrix from randomly subsampling its entries.Here we investigate thecase where the observations take a finite number of values corresponding forexamples to ratings in recommender systems or labels in multiclassclassification.We also consider a general sampling scheme not necessarilyuniform over the matrix entries.The performance of a nuclearnorm penalizedestimator is analyzed theoretically.More precisely we derive bounds for theKullbackLeibler divergence between the true and estimated distributions.Inpractice we have also proposed an efficient algorithm based on liftedcoordinate gradient descent in order to tacklepotentially high dimensionalsettings.,statML,cmpicle,1,t5_355ac
2638988,0,t3_2q5esw,Otto Fabius JoostR. vanAmersfoortDiederik P. KingmaIn this paper we combine the strengths of RNNs and SGVB creating theVariational Recurrent AutoEncoder VRAE. Such a model can be used forefficient large scale unsupervised learning on time series data mapping thetime series data to a latent vector representation. The model is generativesuch that data can be generated from samples of the latent space. An importantcontribution of this work is that the model can make use of unlabeled data inorder to facilitate supervised training of RNNs by initialising the weightsand network state.,statML,cn31qs2,1,t5_355ac
3075894,0,t3_2ve3gg,Alexandre deBrebissonGiovanni MontanaWe present a novel approach to automatically segment magnetic resonance MRimages of the human brain into anatomical regions. Our methodology is based ona deep artificial neural network that assigns each voxel in an MR image of thebrain to its corresponding anatomical region. The inputs of the networkcapture information at different scales around the voxel of interest D andorthogonal D intensity patches capture the local spatial context while largecompressed D orthogonal patches and distances to the regional centroidsenforce global spatial consistency. Contrary to commonly used segmentationmethods our technique does not require any nonlinear registration of the MRimages. To benchmark our model we used the dataset provided for the MICCAI challenge on multiatlas labelling which consists of  manuallysegmented MR images of the brain. We obtained competitive results mean dicecoefficient . error rate . showing the potential of our approach. Toour knowledge our technique is the first to tackle the anatomicalsegmentation of the whole brain using deep neural networks.,statML,cogtbra,1,t5_355ac
3313996,0,t3_2y4184,Yingbo Zhou DevanshArpit IfeomaNwogu VenuGovindarajuTraditionally when generative models of data are developed via deeparchitectures greedy layerwise pretraining is employed. In a welltrainedmodel the lower layer of the architecture models the data distributionconditional upon the hidden variables while the higher layers model thehidden distribution prior. But due to the greedy scheme of the layerwisetraining technique the parameters of the bottom layers are fixed aftertraining. This makes it extremely challenging for the model to learn thehidden distribution prior which in turn leads to a suboptimal model for thedata distribution. We therefore propose a novel method for training deepautoencoders where the architecture is viewed as one stack of two or moresinglelayer autoencoders. A single global reconstruction objective isproposed and jointly optimized such that the objective for the singleautoencoders at each layer acts as a local layerlevel regularizer. Weempirically evaluate the performance of this joint training scheme and observethat it not only learns a better data model but also learns better higherlayer representations which highlights its potential for unsupervised featurelearning. In addition we find that the usage of regularizations in the jointtraining scheme is crucial in achieving good performance. Our proposed workcan thus provide a platform for investigating more efficient usage ofdifferent types of regularizers especially in light of the growing volumes ofavailable unlabeled data.,statML,cp60v10,1,t5_355ac
4310975,0,t3_3addln,Ridho RahmadiPerry GrootMarianne HeinsHans Knoop TomHeskes TheOPTIMISTICconsortiumCausal modeling has long been an attractive topic for many researchers and inrecent decades there has seen a surge in theoretical development and discoveryalgorithms. Generally discovery algorithms can be divided into two approachesconstraintbased and scorebased. A disadvantage of currently existingconstraintbased and scorebased approaches however is the inherentinstability in structure estimation. With finite samples small changes in thedata can lead to completely different optimal structures. The present workintroduces a new scorebased causal discovery algorithm that is robust forfinite samples based on recent advances in stability selection usingsubsampling and selection algorithms. Structure search is performed overStructural Equation Models. Our approach uses exploratory search but allowsincorporation of prior background knowledge to constrain the search space. Weshow that our approach produces accurate structure estimates on one simulateddata set and two realworld data sets for Chronic Fatigue Syndrome andAttention Deficit Hyperactivity Disorder.,statML,csbknhn,1,t5_355ac
4414484,0,t3_3bljyx,Tomohiko MizutaniThe successive projection algorithm SPA works well for separable nonnegativematrix factorization NMF problems arising in applications such as topicextraction in documents and endmember detection in hyperspectral images. Oneof the reasons is that it is robust to noise. Gillis and Vavasis showed inSIAM J. Optim.  pp.   that a preconditioner can furtherenhance its noise robustness. The proof rested on the condition that thedimension d and factorization rank r in the separable NMF problem coincidewith each other. However it may be unrealistic to expect that the conditionholds in separable NMF problems in actual applications in such problems dis usually greater than r. This paper shows without the condition drthat the preconditioned SPA is robust to noise.,statML,csn8wll,1,t5_355ac
5750392,0,t3_3twrjw,Guillaume AlainAlex LambChinnadhuraiSankar AaronCourvilleYoshua BengioHumans are able to accelerate their learning by selecting training materialsthat are the most informative and at the appropriate level of difficulty. Wepropose a framework for distributing deep learning in which one set of workerssearch for the most informative examples in parallel while a single workerupdates the model on examples selected by importance sampling. This leads themodel to update using an unbiased estimate of the gradient which also hasminimum variance when the sampling proposal is proportional to the Lnorm ofthe gradient. We show experimentally that this method reduces gradientvariance even in a context where the cost of synchronization across machinescannot be ignored and where the factors for importance sampling are notupdated instantly across the training set.,statML,cx9w44i,1,t5_355ac
6527389,0,t3_451bc7,KacperChwialkowskiHeikoStrathmannArthur GrettonWe propose a nonparametric statistical test for goodnessoffit given a setof samples the test determines how likely it is that these were generatedfrom a target density function. The measure of goodnessoffit is a divergenceconstructed via Steins method using functions from a Reproducing KernelHilbert Space. Our test statistic is based on an empirical estimate of thisdivergence taking the form of a Vstatistic in terms of the log gradients ofthe target density and the kernel. We derive a statistical test both fori.i.d. and noni.i.d. samples where we estimate the null distributionquantiles using a wild bootstrap procedure. We apply our test to quantifyingconvergence of approximate Markov Chain Monte Carlo methods statistical modelcriticism and evaluating quality of fit vs model complexity in nonparametricdensity estimation.Donate to arXiv,statML,czuhfso,1,t5_355ac
6680405,0,t3_475avj,Tejal Bhamre TengZhang AmitSingerThe problem of image restoration in cryoEM entails correcting for the effectsof the Contrast Transfer Function CTF and noise. Popular methods for imagerestoration include phase flipping which corrects only for the Fourierphases but not amplitudes and Wiener filtering which requires the spectralsignal to noise ratio. We propose a new image restoration method which we callCovariance Wiener Filtering CWF. In CWF the covariance matrix of theprojection images is used within the classical Wiener filtering framework forsolving the image restoration deconvolution problem. Our estimation procedurefor the covariance matrix is new and successfully corrects for the CTF. Wedemonstrate the efficacy of CWF by applying it to restore both simulated andexperimental cryoEM images. Results with experimental datasets demonstratethat CWF provides a good way to evaluate the particle images and to see whatthe dataset contains even without D classification and averaging.Donate to arXiv,statML,d0ab9n1,1,t5_355ac
7464577,0,t3_4hlrjy,Chad HazlettIn the absence of unobserved confounders matching and weighting methods arewidely used to estimate causal quantities including the Average TreatmentEffect on the Treated ATT. Unfortunately these methods do not necessarilyachieve their goal of making the multivariate distribution of covariates forthe control group identical to that of the treated leaving some potentiallymultivariate functions of the covariates with different means between the twogroups. When these imbalanced functions influence the nontreatmentpotential outcome the conditioning on observed covariates fails and ATTestimates may be biased. Kernel balancing introduced here targets a weakerrequirement for unbiased ATT estimation specifically that the expected nontreatment potential outcome for the treatment and control groups are equal.The conditional expectation of the nontreatment potential outcome is assumedto fall in the space of functions associated with a choice of kernel implyinga set of basis functions in which this regression surface is linear. Weightsare then chosen on the control units such that the treated and control grouphave equal means on these basis functions. As a result the expectation of thenontreatment potential outcome must also be equal for the treated and controlgroups after weighting allowing unbiased ATT estimation by subsequentdifference in means or an outcome model using these weights. Moreover theweights produced are  precisely those that equalize a particular kernelbased approximation of the multivariate distribution of covariates for thetreated and control and  equivalent to a form of stabilized inversepropensity score weighting though it does not require assuming any model ofthe treatment assignment mechanism. An R package KBAL is provided toimplement this approach.,statML,d2qocsl,1,t5_355ac
7464583,0,t3_4hlrok,Mario Lucic MesrobI. OhannessianAmin KarbasiAndreas KrauseFaced with massive data is it possible to trade off statistical risk andcomputational space and time? This challenge lies at the heart of largescale machine learning. Using kmeans clustering as a prototypicalunsupervised learning problem we show how we can strategically summarize thedata control space in order to trade off risk and time when data isgenerated by a probabilistic model. Our summarization is based on coresetconstructions from computational geometry. We also develop an algorithm TRAMto navigate the spacetimedatarisk tradeoff in practice. In particular weshow that for a fixed risk or data size as the data size increases resp.risk increases the running time of TRAM decreases. Our extensive experimentson real data sets demonstrate the existence and practical utility of suchtradeoffs not only for kmeans but also for Gaussian Mixture Models.,statML,d2qodom,1,t5_355ac
7542804,0,t3_4inp6r,Xingguo Li TuoZhao RamanArora HanLiu JarvisHauptWe propose a stochastic variance reduced optimization algorithm for solving aclass of largescale nonconvex optimization problems with cardinalityconstraints and provide sufficient conditions under which the proposedalgorithm enjoys strong linear convergence guarantees and optimal estimationaccuracy in high dimensions. We further extend our analysis to an asynchronousvariant of the approach and demonstrate a near linear speedup in sparsesettings. Numerical experiments demonstrate the efficiency of our method interms of both parameter estimation and computational performance.,statML,d2zkwkb,1,t5_355ac
7567054,0,t3_4iz4cq,Mark SchmidtSIERRA LIENS Nicolas LeRoux SIERRA LIENSFrancis Bach SIERRALIENSWe propose the stochastic average gradient SAG method for optimizing the sumof a finite number of smooth convex functions. Like stochastic gradient SGmethods the SAG methods iteration cost is independent of the number of termsin the sum. However by incorporating a memory of previous gradient values theSAG method achieves a faster convergence rate than blackbox SG methods. Theconvergence rate is improved from Ok to Ok in general and whenthe sum is stronglyconvex the convergence rate is improved from the sublinear Ok to a linear convergence rate of the form Opk for ptextless . Further in many cases the convergence rate of the new methodis also faster than blackbox deterministic gradient methods in terms of thenumber of gradient evaluations. Numerical experiments indicate that the newalgorithm often dramatically outperforms existing SG and deterministicgradient methods and that the performance may be further improved through theuse of nonuniform sampling strategies.,statML,d32c5nu,1,t5_355ac
