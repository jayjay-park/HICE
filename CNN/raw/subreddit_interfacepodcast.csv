,controversiality,parent_id,body,subreddit,id,score,subreddit_id
7702439,0,t3_4kpjun,As a programmer I am extremely excited for Viv. Ever since the introduction of Siri I have wanted a Siri API that lets me build new tasksintegrations. Its especially appealing for IoT tasks. A task that I might want to do frequently would be remotely checking in on my dog while Im at work I know Im a helicopter parent. To do that Viv would need to know how to connect to my VPN make an RTSP connection to my webcam and either stream it or grab one or more frames. Here is a request I might use to trigger the workflow gt Show me the most recent frame from my home webcam. Or a little more complicated gtShow me the most recent frame with movement from my home webcam. Or really crazy gt Where is my dog right now?. The wrong answer to this would be at home. Viv needs to be able to infer my intent. What I really asked is Where in my home is my dog right now?Or batshit insane gt What is my dog doing right now?. The last two would require computer vision and machine learning and thats when I start getting really excited and scared about the future possibilities of these assistants.The other way to do this kind of thing would be to set up a little web server at home that I connect to using VLC or a custom app. I guess what you guys were trying to decide in the podcast is which is preferable in this case using an app or using natural language?,interfacepodcast,d3hpsac,3,t5_3e7cl
