,controversiality,parent_id,body,subreddit,id,score,subreddit_id
6229232,0,t3_40wfzc,it worked for me. Check your drive memory. Clean it up. Make sure to have a few GB on C disk.,cs231n,cyxp2g4,1,t5_35p6c
6508237,0,t1_czscnty,And not to mention the absolute joy you get when you write up the computational graph of functions such as batch norm etc and work the gradients up the graph. The back prop as explained by Andrej totally demystified a significant part of deep learning for me. Everything else such as better weight initialization etc seems like ways to make things go better practically. The beauty of understanding this deeply is that now anytime you are struggling with trying to fit best learning rates regularizations etc some part of you can start thinking in terms of can I write up a functional layer that automatically adjusts the learning rate and back prop through it? It might be a totally crazy idea but the germ of the idea wouldnt even arise if you dont understand the nuts and bolts of this technique. I am totally thrilled at how much I am learning in this class. I congratulate and honor the course instructors. ,cs231n,czsi1th,3,t5_35p6c
6959252,0,t1_d152aua,Sorry I looked at this a little bit more carefully. The W argument is not being used at all. You can give f anything as input in this case and you will just get the loss at the starting parameters.I assume this is done so that the evalnumericalgradient can work on functions more generally when necessary.,cs231n,d155zg8,1,t5_35p6c
7006997,0,t1_d19g49m,Why would they not be easy to identify or normalise in preprocessing? ,cs231n,d1al6p9,1,t5_35p6c
8059787,0,t3_4pfq5y,Figured it out I forgot to sum up the dxpadded errors!,cs231n,d4lzv2q,2,t5_35p6c
8352527,0,t1_d5la46a,Sorry but this isnt true. Estimated functions in nets and loss surfaces are identical to multiplicative and additive constants.The benefit comes from as you say promoting diffuse weights  but also importantly from penalising weights being too large and thus very sensitive to inputs. Very large spikes in weights cause great deals of variance in models as you have large activations for specific inputs.,cs231n,d5lmafz,1,t5_35p6c
