,controversiality,parent_id,body,subreddit,id,score,subreddit_id
57496,0,t3_1uracy,This will generate a fake research paper I think that their approach and their result will inspire you but its not quite generic as you would hope no actual learning lots of hard code and rules for specifically outputting research papers iirc.,MachineLearning,cekz5qr,2,t5_2r3gv
64171,0,t3_1utaf9,What the hell is this drivel?,MachineLearning,celqhs7,6,t5_2r3gv
180929,0,t3_1w4f71,Way to slow to use ,MachineLearning,cez1lkf,2,t5_2r3gv
195056,0,t3_1w9fcg,More details about the acquisition here ,MachineLearning,cf0np6y,3,t5_2r3gv
237214,0,t3_1wtuvz,I think using RLink is fine.,MachineLearning,cf5c5by,1,t5_2r3gv
456770,0,t1_cfs60ax,So my input IS continuously fed but for the span of only a few hours.    You make it appear as if k means is process runs only a single time that acts on a finite set of data.    So would a process using kmeans would look like this?        . Collect all my data      .Run it through K means clustering process         .end up with k clusters of classified data?     What would you recommend for growing incredibly large datasets? I dont have to iterate through all of my data to improve on my clusters do I?    Edit I really appreciate your input by the way. ,MachineLearning,cfs6mrr,1,t5_2r3gv
465494,0,t3_1zfhfx,This is definitely a problem where you can use mixtures of PPCA. Check this out ,MachineLearning,cft6fd9,9,t5_2r3gv
494207,0,t3_1zrzr5,I work with similarly heterogenous data in the biomedical field.Feature engineering is a slow process and im not great at it but the first thing I would do is start running random forests and looking at the feature importance. You could just take the top n features and train on them but you may find that you have some surprising things in the top features...leaked data or unique identifiershigh cardinality categorical features that are just leading to overfitting.That will help you get rid of some of the features that may actually be hurting you. There is a bunch of research in improving rf feature selectionimportance scores if you want to go more in depth in this direction.I would alos try some unsupervised things kmeans various matrix decomposition methods making lots of plots on the data and see if there are obvious trends you can encapsulate in a feature.Its also worthwhile to bring in any domain expertise you have to combine features that arent correlated enough to show up in the above but are strongly related example two drugs that are rarely given together but both treat the same thing can be combined with an or.Dealing with groups features separately is worth a try either by combining them before or training models on separately and  then combining. One recent paper I saw on the latter approach  ,MachineLearning,cfwg1h0,4,t5_2r3gv
503927,0,t3_1zu6d5,I think you might be thinking about the reject option. You want to be able to classify dogs and everything else as a dont know. If thats what you are thinking about look into cascading classifiers. ,MachineLearning,cfxjsye,2,t5_2r3gv
566798,0,t1_cg4n70k,QLearning seems promising but there seems to be way too many states to keep all in memory something on the order of  states.,MachineLearning,cg4p7vb,1,t5_2r3gv
575997,0,t1_cg52921,David McAllester has done some PAC Bayes work on dropout That might be a direction you could explore.,MachineLearning,cg5quea,2,t5_2r3gv
576269,0,t3_20q4f4,Stop writing about it and do it,MachineLearning,cg5ryel,2,t5_2r3gv
584319,0,t1_cg6kg53,This is no longer the case  the creation of layerwise pretraining techniques brought neural nets back to the forefront of many areas since that allowed us to stack layers deep enough to outperform SVMs and still train fairly easily. Though layerwise pretraining has since been dropped due to the advent of dropout and rectified linear units to avoid overfitting and ease training. Neural nets are back at the forefront of performance for many fieldsdata types. However I personally believe SVMs are still easier to train on data without published results  one of the tricky bits is getting the correct hyperparameters so that the network a doesnt explode with NaNs and b doesnt perform worse than other methods. This is one of the reasons that automatic hyperparameter optimization is such a key research area for all ML but especially neural networks  if we can get an efficient framework for tuning the many hyperparameters of deep neural networks it will make it much more accessible to researchers who are not neural network experts. Recurrent networks are even harder to train than regular neural networks so these same thoughts extend to there!TLDR This is no longer the case. Deep neural nets rock  tune the hyperparameters until your error decreases make it deep until overfitting or it cant start learning while slightly tweaking hyperparams then add dropout. SVMs have the representative power of a  hidden layer neural network IIRC and we now have techniques to train much deeper nets effectively. However SVMs are much easier to train on unknown data in my experience.,MachineLearning,cg6owcq,4,t5_2r3gv
626503,0,t1_cgbgnhd,I think this stack overflow question helps a bit. But it is specific to Sklearn. ,MachineLearning,cgbhg6q,1,t5_2r3gv
627671,0,t1_cgbl4we,Ive only got an intro class under my belt but when we built our classifiers using a multilayer perceptron we used a curated portion of the Netflix dataset and Nfold cross validation to get the best results. Is there a specific reason that youve separated your training set and your test set?,MachineLearning,cgbm8c2,0,t5_2r3gv
685672,0,t3_220mdh,This is so adorable. Best wishes to them. ,MachineLearning,cgi7nbp,11,t5_2r3gv
721970,0,t1_cgmboui,you can use them for prediction no need to have the outputs binary.,MachineLearning,cgmcauh,1,t5_2r3gv
772486,0,t3_22s9hp,Seriously good tutorial  looks like he added some new stuff from the original ...   I try to read this once a year,MachineLearning,cgs33zs,1,t5_2r3gv
787013,0,t1_cgt5x7y,Do you think deep learning will ever escape the surly bonds of imagespeech tasks and be useful for other tasks?,MachineLearning,cgtqk3h,2,t5_2r3gv
841908,0,t1_cgu7dgp,Appreciate the feedback. Of course that is the introduction to a  part series but the point is well taken nonetheless. Ive posted a second part today but perhaps will refrain from posting it here until Im further along. Im happy to take feedback from anyone following the series.,MachineLearning,cgzz7ef,1,t5_2r3gv
935433,0,t1_cha13tx,thank you very much for such a thorough explanation!,MachineLearning,cham3uc,1,t5_2r3gv
987797,0,t3_25evt3,Darn I was all excited for this to be about an algorithm trying to learn Thiller that ends up overfitting somehow. ,MachineLearning,chgkj8n,60,t5_2r3gv
1004696,0,t1_chiftyb,I found Hierarchical Temporal Memory to be really interesting as a step towards that. Its basically deep learning but the bottom layers tend to be much larger as to form a pyramid the connections between layers are very sparse and you have some temporal effects in there too. There are reinforcement learning algorithms to train these networks by simulating the generation of dopamine as a value function to let the network learn useful things. These may better model the human brain and may better serve to create artificial emotion. Have you looked into this yet? ,MachineLearning,chihprn,11,t5_2r3gv
1012299,0,t1_chin6ea,Generative models? yes. In a way all unsupervised algorithms are generative. Probabilistic generative? probably not.Hobbies Ive always enjoyed designing and building model airplanes and other flying contraptions hacking and just building stuff. I also enjoy music particularly jazz and baroque and sailing. These days I hack microcontrollerbased widgets and play with D printers and CNC machines. Im a maker. I wish I had more time for that.,MachineLearning,chjctcr,4,t5_2r3gv
1042955,0,t1_chmjy4f,TfIDF is very useful but depending on how deep you can afford to make the net it might be worth using lower level features as tfidf carries some assumptions  discards some information.  Geoff Hinton in his well noted Science article uses document specific word probabilities.,MachineLearning,chmu74c,1,t5_2r3gv
1056013,0,t3_2679yw,The best advice probably is to email study advisers or contact persons at universities you think of applying for. In general theyre very helpful and their information will be more relevant than our guesses.But to answer your question I qualified immediately by doing a bachelor in artificial intelligence. To help get an idea here are some requirement pages from Dutch universities UvA VU RUG UU UMI think that with a math BSc. and some programming experience youre in a pretty good spot. Its helpful to show some interdisciplinary experience. Some possible courses that I think would help admission Neurobiology  psychology of the brain Linguistics Philosophy of cognitive science Modal logicObviously you dont need to master any of those topics but an intro course in one or two will be helpful.,MachineLearning,chobng4,2,t5_2r3gv
1062984,0,t1_chp0xly,Hmm at the moment I just dont seem to be getting good results for the dimensionality reduction  compared to the GPLVM for example. The long training times are irritating too ,MachineLearning,chp46k4,1,t5_2r3gv
1074743,0,t3_26dvnm,sigh...  I want a pizza... but there are literally no delivery places in my town .Impossible for me to get regardless of circumstance.  Damn you corporate murica!!!! shakes fist of impotent rage,MachineLearning,chqga6c,1,t5_2r3gv
1107125,0,t3_26sdq8, hours is way too long.Profile your code.Then look for opportunities to parallelize. Matrix operations in particular are great candidates. Using either the BLAS on CUDA or the Intel MKL will help a ton.On intel processors the Intel C and Fortran compilers will also auto vectorize and auto parallelize lots of your code which should give you a huge speed boost. If youre not using C or Fortran they will also run faster without changing the algorithm.,MachineLearning,chu4ynj,6,t5_2r3gv
1114896,0,t3_26uzm5,Honestly I just upvoted because I use the cloud to butt plugin for chrome.,MachineLearning,chv0tqy,4,t5_2r3gv
1125618,0,t1_chw8d0r,Further down on the page gtC. is the successor to ID and removed the restriction that features must be categorical by dynamically defining a discrete attribute based on numerical variables that partitions the continuous attribute value into a discrete set of intervals. ...gtCART Classification and Regression Trees is very similar to C. but it differs in that it supports numerical target variables ...gtscikitlearn uses an optimised version of the CART algorithm.Which means according to their documentation scikitlearn uses CART which supports categorical features bc it it supports more than what C. does which does support categorical features. Their documentation is evidently wrong and my confusion justified given the context. Nothing on the API page gives any indication of such limitations either. gt Because decision trees use a simple splitting threshold they are quite robust to coded categorical values.Any reference on that? Maybe a normal decision tree if only a few options were available  or  for a categorical variable. As the number of options goes up the decision trees decision will become more and more meaningless on categorical features. Randomforest with onehot categorical features would make no sense at all. ,MachineLearning,chw8olv,1,t5_2r3gv
1164771,0,t1_chxkuu5,I had the same feeling. Murphys reads too much like a literature research and not really as a textbook.It mostly tells you what other people have done to solve X or Y problem together with a very superficial description of the methods. If you want to dig deeper you have to look in the references from which most of the examples are taken.In my case I was looking for filtering methods for switching state space models and the algorithm description was very barebones and wrong or at least incomplete depending on how you interpret an undefined symbol in the text. I also noticed several other editing issues...At least Bishops has plenty of original examples and its helpful in learning the concepts.,MachineLearning,ci0nmob,1,t5_2r3gv
1188537,0,t1_ci32xv3,gt They can be realvalued with the column features scaled in the range  . This is a pretty terrible way to use an RBM. It works okay for MNIST because the grey levels are sparse and you can sort of treat them as pseudoprobabilities of inknoink but the RBM is a distribution on binary random variables not variables on the unit interval and deviating too far from this assumption is probably going to result in garbage.Gaussian visible units are a marginal improvement though they spend all their capacity modelling the conditional mean which is not that interesting and spikeandslab or meancovariance parameterizations do a much better job. ,MachineLearning,ci3bv3l,4,t5_2r3gv
1319920,0,t3_2967qh,Note that this distinction is still present in a lot of the field.  In deep learning probabilistic approaches such as RBMs compete with  purely discriminative methods such as dropout.  Deep learning in general is a lot less like a probabilistic model than the very intricate Bayesian approaches to data science proposed by Jordan Tenenbaum etc.,MachineLearning,cii3w9x,1,t5_2r3gv
1337031,0,t1_cijyj1g,Yeah youre gonna want to choose that by cross validation. ,MachineLearning,cik15wu,4,t5_2r3gv
1344723,0,t3_29ghav,Please ask this question on the dataset subreddit.,MachineLearning,cikwb1p,2,t5_2r3gv
1360705,0,t3_29ni0g,gt CAT Dataset  A dataset of  cat imagesReddits favourite,MachineLearning,cimure4,4,t5_2r3gv
1365074,0,t1_cin4lr9,The latter. It doesnt have to be from my domain although I like to work on customer data. In  competition in which I did particularly well I used SQL  of the time. The data wasnt ready to use but rather you had to decide how you want to generate it. In that setting even evaluating model performance was a challenge. So this was more data transformation task rather than modeling.,MachineLearning,cinegsr,2,t5_2r3gv
1560834,0,t1_cjbwapt,I started watching the video on Youtube and dont worry it is perfectly understandable. And with the slides available it is easy to follow. Thanks again! I was currently looking into deep learning meaning representations wordvec and stuff so this is perfect on time for me. ,MachineLearning,cjbx95x,2,t5_2r3gv
1602094,0,t3_2cn8ki,Discussion on Hacker News,MachineLearning,cjh42uc,1,t5_2r3gv
1674002,0,t3_2dhsqq,Good talk  equally applicable to trading the markets using indicators  statistics.  ,MachineLearning,cjq64mv,2,t5_2r3gv
1705534,0,t3_2dwik0,I liken it to being handed a sagging cardboard box full of  years of faded receipts by someone who wants their taxes itemized,MachineLearning,cju57v6,8,t5_2r3gv
1761979,0,t3_2enf1q,Gene Ontology,MachineLearning,ck195uf,1,t5_2r3gv
1822363,0,t3_2ffejw,As far as I remember Im not an expert using a kernel can give your linear classifier e.g. SVM a way to fit nonlinear decision boundaries by transforming your data to another space where its then linear seperable. However a neural network can already fit any function  outofthebox. I can imagine that a kernel COULD make a NN converge faster but then again I am by far no expert on machine learning or NNs.,MachineLearning,ck8s7l7,1,t5_2r3gv
1854283,0,t1_ckcm5wk,well then,MachineLearning,ckcmx4x,1,t5_2r3gv
1921466,0,t1_ckkpplb,Initialized and forced to zero after each weight update. Initializing only made no difference.,MachineLearning,ckkq1cj,3,t5_2r3gv
1952987,0,t1_ckoik6o,Hey thanks for the advice. I actually weighed my options regarding that. I felt that clarity of what Im doing is more important than making the code shorter or more concise. This code might be more readable to people familiar with other languages.,MachineLearning,ckoje91,-5,t5_2r3gv
1960784,0,t1_ckpgm54,Alright sounds like you know what you are doing. Another thing that might be quick to set up is using libsvm to do a regression. Training time will likely be an issue so youll likely have to subsample your space similar to what you did with kNN but Im sure the SVM will give you better results than kNN if you use an rbf kernel.  Also note that by default libsvm runs in a singlethreaded variant designed for sparse matrices. However somewhere on the site I linked you can find a version that is implemented using dense matrices which will give you  boost in performance. Also somewhere in the FAQ of the site it explains how you can add multithreading to the implementation by modifying  lines of code  IIRC speedup is almost linear for the first  cores.,MachineLearning,ckph5q7,2,t5_2r3gv
1967893,0,t1_ckdkhkh,What is the license of that book? Im trying to figure out if it would cause a problem if I went to try and get that book printed so that I can have a physical copy.,MachineLearning,ckqbz04,1,t5_2r3gv
2000583,0,t1_cktzkpf,No suggestion to help advance my idea just blank criticisms?Someone told me reddit was a good place to share ideas I guess they were wrong.,MachineLearning,cku9puw,1,t5_2r3gv
2059942,0,t1_cl1nuap,Yeah this helps a bunch! Is it possible to load a serialized model into a javascript program and then use it? I was using python bindings for WEKA for a project  months ago are there similar bindings for JavaScript?,MachineLearning,cl1nwhk,1,t5_2r3gv
2145578,0,t1_clchfgy,yikes whats with the animosity.i was wondering if you could elaborate on this  I doubt you want anything from top  or something considering HOW Netflix prize was won thats all.. i wasnt the one who downvoted you.,MachineLearning,clcjvej,2,t5_2r3gv
2197874,0,t1_clix6ys,Erik Bernhardsson from Spotify has done some work on using RNNs to take into account the time dimension in music recommendations Although the focus there is on the order in which you listen to the songs and not the time of day  week  month or something like that.,MachineLearning,clj7czt,1,t5_2r3gv
2212861,0,t3_2kgcfb,How about training a bunch of different models on MNIST then ensembling them and aim at . class error?,MachineLearning,cll3w9p,5,t5_2r3gv
2243744,0,t3_2kth6d,Doesnt modeling a Turing machine make it impossible to test for correctness  completeness? Halting problem..  How do you perform crossvalidation?,MachineLearning,clp16mc,0,t5_2r3gv
2273052,0,t3_2l86sk,bro. stop with the spam. ,MachineLearning,clso2cx,1,t5_2r3gv
2310626,0,t3_2lqqn0,I found these slides and paper provide particulary interesting details about the ELM  specifically relating it to a finiteapproximate version of the infinite neural network kernel of C. Williams paperSince the neural network kernel is basically an infinitely wide neural net if you integrate out the weights... wtf? IMO thats the important part... that then is related to deep learning by the Caruana paper Do Deep Nets Need to Be Deep? or just thinking of neural nets as universal function approximators of all things. But I think that is stretching the metaphor pretty thin.I tend to see the ELM as a fixed random affine transform followed by a linear learner. You would think this would be a pretty poor model but the surprising part is how fast it trains and how well it actually works for certain problems. Of course MNIST performance is not the be all end all but doing well on that task is definitely a good sign that something interesting is happening.A similar idea actually used for neural network training is called feedback alignment see the paper That might be the thing that is giving ELM its power in the first place!,MachineLearning,clxb53s,2,t5_2r3gv
2320575,0,t1_clwa7h4,. What are the greatest obstacles RBMDBNs face and can we expect to overcome them in the near future?I shall assume you really do mean RBMs and DBNs not just stacks of RBMs used to initialize a deep neural net DNN for backprop training.One big question for RBMs was how to stack them in such a way that you get a deep Boltzmann Machine rather than a Deep Belief Net. Russ Salakhutdinov and I solved that more or less a few years ago. I think the biggest current obstacle is that almost everyone is doing supervised learning by predicting the next frame in a sequence for recurrent nets or by using big labelled datasets for feedforward nets. This is working so well that most people have lost interest in generative models.  But I am sure they will make a comeback in a few years and I think most of the pioneers of deep learning agree. ,MachineLearning,clyjccv,10,t5_2r3gv
2352311,0,t1_cm2aqb6,My question is about what obligations mentors and mentees can expect when putting their names down. ,MachineLearning,cm2gg7a,2,t5_2r3gv
2353270,0,t3_2m67mi,I am trying to build arrayfire on my mac and am having hard time.It seems there is a  clblas dependency and I am not able to build and install clblas on my mac currently. is there any plan to make this available using homebrew?,MachineLearning,cm2kpqh,1,t5_2r3gv
2355872,0,t1_cm2b09y,I might. I dont have much interest in using LSTMg any more but now that theres a good reference library it should be much easier to do.And Monner at least is interested in me getting that CC hardcoder functional.And youre welcome kkastner!,MachineLearning,cm2wao4,1,t5_2r3gv
2534835,0,t1_cmpd325,Did we just create a DNN Rorschach Test?What do you see here?A puppy?Did you ever have a puppy?Im a robot.,MachineLearning,cmpftsb,11,t5_2r3gv
2547788,0,t1_cmqvjmp,In some problem domains its prohibitively expensive or impossible... to generate datasets with gt samples... These problems arent any less important and even imperfect classifiers may provide enough improvement to warrant investment.,MachineLearning,cmr4sjr,11,t5_2r3gv
2554936,0,t1_cms1kwo,True the graphic isnt about ranking them. It just looks a little weird to me though to see Artificial Intelligence Deep Learning and Machine Learning presented as distinct categories.... when depending on whom you ask there is nontrivial overlap or subset relations going on there.,MachineLearning,cms2fnk,4,t5_2r3gv
2642850,0,t1_cn37gfs,gt I am not familiar with the alpha for Laplace smoothing parameter could you perhaps go into a little more detail? Ill be researching it in the meantime.Thats basically a positive number to avoid that the classconditional probability which is a product of the individual classconditional probability for every term becomes  if you encounter a new word that was not part of your training dataset. In practice I havent seen that much of an effect whether I choose . .   etc. Its just a minor thing to tune in the end. Btw. reminds me of a article I wrote not too long ago Naive Bayes and Text Classification I  Introduction and Theory Maybe youll find some ideas there.,MachineLearning,cn3jwuu,1,t5_2r3gv
2661489,0,t3_2qgtfu,deleted,MachineLearning,cn5zfs4,6,t5_2r3gv
2710057,0,t1_cnacpbf,I guess there are  comments wrapped in one herest  Preprocessing is huge and can make the difference between an algorithm not working and being totally fine. See for example the power of ZCA basically PCA with extra sauce in Alex Krizhevskys Masters thesis nd  Though there is a lot of neat stuff in deep learning and neural nets for many tasks the complexity is not necessary or even plausible given ilittle data. PCA or KMeans  linear learner can be used even with tiny tiny datasets. Bonus points for incorporating your domain knowledge into the transform!rd  These kinds of pipelines make really nice baselines for sanity checking yourself on new tasks and having a dumb baseline can save you a lot of wasted time on algorithms or features that arent that good but show improvements the more they are tweaked. AKA if logistic regression scores  on the validation set it may not make sense to waste time improving an algorithm or feature set that scores in the  range. Especially if you are in a time crunch!thbonus  KMeans and PCA are actually related... There is also some example of using this here.Another example is KMeans  discrete HMM for relatively speedy learning and inference of time series tasks.,MachineLearning,cnc4ezi,1,t5_2r3gv
2713718,0,t1_cnbzatd,so I agree this is how it works in real life in many industries.the main problem I see is when the programmer in this instance goes around and decides to publish his findings in a journal give a talk to others etc. without having done the requisite background research to verify that his assumptions are correct i.e. pore over the theory.  Peer review fails in this case because the people reviewing his work are likely not machine learning people but other programmers.,MachineLearning,cncjey7,1,t5_2r3gv
2740235,0,t3_2reoyx,,MachineLearning,cnfjxa1,1,t5_2r3gv
2763773,0,t3_2rq5mr,If there were a repository for meaningless buzzwords in machine learning this site would be it.,MachineLearning,cni8aqi,3,t5_2r3gv
2776084,0,t1_cnfzjue,Yea I think the representation power is pretty impressive. This and the learning of compressed representations that we can pipe to other layers is what fascinates me about this.gt  Just stripped off the input from the predicted words in the decoderJust to clarify here are you stripping the RNN context from the input during the MSE diff? gt internet being decoded as dataSounds similar to what wordvec might do? ,MachineLearning,cnjmpx6,1,t5_2r3gv
2835589,0,t1_cnp6i5e,THIS. its amazing how people just gloss over you when you tell them hey..if you follow my results we might see a lift in sales and you wont have to chase leads that wont buy .. .,MachineLearning,cnqeih5,1,t5_2r3gv
2923080,0,t1_co0c89j,No problem... I was plotting Steve Zissous next great adventure now who would sign up for that trip? Hinton Ng Bengio LeCun or Schmidhuber?,MachineLearning,co0d32f,1,t5_2r3gv
2936029,0,t1_co1puv3,youre wrong  patents ban the USE of the patented technology whether youre selling it or not.,MachineLearning,co1u7qt,1,t5_2r3gv
2944463,0,t1_co2p2ss,I recently read Data Scientists At Work by Sebastian Gutierrez with a great interview with Yann LeCun. Given his accomplishments and standing in the field this is probably one of the best advices you can getRelevant SectionGoogle Books link,MachineLearning,co2srlf,2,t5_2r3gv
2971132,0,t1_co5r551,scikitlearn seems to be a good starting point. Here is an interesting article ,MachineLearning,co5tzv3,2,t5_2r3gv
3053791,0,t1_coe8ebx,  I went in without a masters so I didnt waste much time before I figured out I didnt want a PhDmasters along the way.  I do research and development at my job too leading a project  year out of school.  ,MachineLearning,coeiuul,3,t5_2r3gv
3121527,0,t1_cokozat,In the end I was able to reproduce their mnist results as much as that small plot allows. Currently testing with the best permutation invariant backprop net I have.,MachineLearning,coljmyi,1,t5_2r3gv
3180886,0,t1_corhpz9,There is a website here that attempts to keep track of all the best published results on a number of datasets.,MachineLearning,corp4rq,2,t5_2r3gv
3184681,0,t1_cormqva,Im also leaving in about months when PhD is all wrapped up. For me data science will have to pay the bills. ,MachineLearning,cos3bol,6,t5_2r3gv
3228387,0,t3_2x3ksr,Random variables are how we express our uncertainty about the world. ,MachineLearning,cowmkvf,2,t5_2r3gv
3241364,0,t1_coxp1e7,That was my thought too. I dont think anything has changed since mnths ago or we it was.,MachineLearning,coxz3j8,1,t5_2r3gv
3258980,0,t1_cozqopg,Whats the purpose of virtualization in your case? Why not install linux as a host system and dont bother with VMs at all?,MachineLearning,cozsz0o,2,t5_2r3gv
3267030,0,t3_2xig3h,deleted,MachineLearning,cp0o24x,-17,t5_2r3gv
3271916,0,t3_2xcyrl,Hello Mr. Schmidhuberfirst of all thanks for doing this AMA. Two  questions. In the community I sense sort of a conflict between the connectionists and Bayesians. Their main critique to neural networks is  that  the inference one does is inconsistent  because of  lack of formulation in terms of prior and likelihood.  Do you think  NNs are a transient tool until there are tools that are as efficent and  usable as NNs but consistent in a Bayesian framework?.  Compared to  symbolic AI  it is nearly impossible to find out what a subsymbolic learning system such as a neural network actually has learned after training. Isnt this a big problem when for example large amounts of stock market trading is done by such systems today? If crashes or other singularities happen we have no idea how they emerged. ,MachineLearning,cp1843e,15,t5_2r3gv
3290214,0,t3_2xcyrl,How on earth did you and Hochreiter come up with LSTM units? They seem radically more complicated than any other neuron structure Ive seen and everytime I see the figure Im shocked that youre able to train them.What was the insight that led to this? ,MachineLearning,cp3b75w,15,t5_2r3gv
3316136,0,t1_cp2t69u,An AI would answer that your perception is reversed. The reason left and right appear to be reversed is because your brain models the mirroryou as part of the same world as the real you and if you went around behind the mirror and faced yourself youd need to reverse your left and right to match the perception of the mirroryou. The reason you dont see the updown reversal is because youre used to travelling horizontally. If you went over the mirror and faced yourself youd then have to reverse up and down instead. So its all in your nonAI head!,MachineLearning,cp69mnp,1,t5_2r3gv
3318100,0,t3_2y5dma,Ive seen mention of this in another post and the original paper but is it really important that batch normalization be done on the inputs to the nonlinearities rather than to inputs to each layer i.e. the outputs of the previous layer.  Also have you seen improvements with BN amp relu or just saturating nonlinearities? Has anyone tried combining BN amp prelu?,MachineLearning,cp6hpnk,1,t5_2r3gv
3340516,0,t3_2yfmij,deleted,MachineLearning,cp91kix,-1,t5_2r3gv
3375033,0,t3_2xcyrl,What are your thoughts on Knowm?,MachineLearning,cpcz8rd,1,t5_2r3gv
3380063,0,t3_2yvscn,I dont know much about nolearn but I feel like theano is much more flexible in what you can acheive whereas nolearn comes with all algorithms prepackaged for theano the equivalent is pylearn.So if youre a newbie both in programming and machine learning maybe nolearn would be better but again I never tried it otherwise I would suggest using theano.theano isnt strictly a neural net library although its primary usage seems to be just that in the sense that it lets you define your own architectures at the vectormatrix computation level rather than at the bit more abstract neural net level which in my opinion is an advantage but might make it harder to learnwork with.,MachineLearning,cpdjvy9,2,t5_2r3gv
3408889,0,t1_cpfsoku,You should know that has nothing to do with memory neural networks. Its at best like a regular recurrent neural network if you are interested in that there is a ton of work on using RNNs to predict text and other sequences.Cireneikuals algorithm is cool but doesnt really have any results yet and I doubt it would do much better than traditional recurrent neural networks. Here is a good reference for that to start with. Any decent neural network library should have some support for RNNs.,MachineLearning,cpgu768,1,t5_2r3gv
3432906,0,t3_2zjkrp,I believe that this is the same as LWTA.  ,MachineLearning,cpjksjs,1,t5_2r3gv
3441091,0,t3_2zladc,Before I set aside  hours to watch this series can someone who has seen these comment on their usefulness? How deeply! are topics covered? How are the slides and the delivery? ,MachineLearning,cpkidki,3,t5_2r3gv
3448765,0,t3_2zovcw,editedTo be even more clearThis doesnt work because secure hash functions are designed to destroy all statistical relationships between their input bits and their output bits.The demo is broken becauseHe starts with  blockheaders    Block      Block     Block     ...He then takes those  blockheaders and for each he generates  random nonces and training labels.  This becomes his new datasetHe then takes  of those blockheaders for each he generates  random nonces and training labels.  This becomes his new dataset      Block  | Random Nonce  | False    Block  | Random Nonce ... | True    Block  | Random Nonce  | False    Block  | Random Nonce  | False    ....    Block  | Random Nonce  | TrueHe then takes the first  which is about  individual blockheaders with  examples each.He now has a training matrix with .mm rows in it.A randomly selected  of those  .mm data points are then held out from training to test the classifier meaning that in the training set of mm rows it would be very hard for there not to be data from the test set for all  block headers.  Even if it misses  or  the test results will look good.Since the classifier sees all most of the block headers it will be tested on along with several examples of a random nonce which is correlated to the value of the true nonce it does really well.Its a classic error in machine learning.That said the real lesson here is DONT EVER TRUST PYTHON PICKLES OFF THE INTERNET.  THEY CAN RESULT IN ARBITRARY CODE EXECUTION.  I disassembled this one and it looks safe but Im lazy so I may have missed something.,MachineLearning,cpldw1p,3,t5_2r3gv
3490797,0,t3_308iyx,Check out  ,MachineLearning,cpq6can,2,t5_2r3gv
3491884,0,t1_cppkimw,gt DAESorry What is this?,MachineLearning,cpqasut,1,t5_2r3gv
3493816,0,t1_cpqi6ts,I am not the author just happened to find this on Github.So I have the same question to be honest.Though in defense of libdeep the Python bindings do provide some pretty tidy examples and I like the idea of writing the net out to a .c source file to be compiled and used however you see fit.,MachineLearning,cpqiq9d,2,t5_2r3gv
3496382,0,t3_30b43k,sounds like a steaming pile of bullshit.,MachineLearning,cpqt9up,5,t5_2r3gv
3512965,0,t1_cpshg4x,Did you mean human? If yes thats an interesting opinion P. Elaborate?,MachineLearning,cpspbkx,1,t5_2r3gv
3551743,0,t1_cpx3vpi,I think the reason is that taking only the sign of the gradients would be way too noisy in the stochasticonline mode.,MachineLearning,cpx4ecc,2,t5_2r3gv
3552725,0,t1_cpx5ept,deleted,MachineLearning,cpx8fud,1,t5_2r3gv
3559945,0,t1_cpy1s7a,Illinformed tripe. Legitimate criticisms exist but not all criticisms are created equal.,MachineLearning,cpy24qv,11,t5_2r3gv
3570005,0,t1_cpyiqmw,I cant think of a single algorithm closer to the core of modern machine learning than logistic regression.,MachineLearning,cpz7i4d,5,t5_2r3gv
3578023,0,t1_cpztkcx,I dont care about good press from the AI community. In fact I welcome the bad press. Thats how I know I struck a sensitive nerve.,MachineLearning,cq04fia,0,t5_2r3gv
3579266,0,t1_cq095gx,Sorry youre right  I am clipping the gradients but my last comment is confusing.  I was just trying to get some intuition using for the difference between the norms.  The gradient matrix will generally have much smaller components.,MachineLearning,cq09j52,1,t5_2r3gv
3582725,0,t3_31dki6,How does this compare to sparse distributed representations? With those modularity is learned at runtime by only activating small groups of neurons at a time. It also has a regularization effect similar to dropout. Perhaps SDRs can be combined with modular connectivity to reduce forgetting even further.,MachineLearning,cq0nqz0,7,t5_2r3gv
3586429,0,t1_cq12vf1,Doesnt this return only square matrices? ,MachineLearning,cq12xbr,1,t5_2r3gv
3586667,0,t1_cq0xaq7,Thanks Foxtrot for the link.  ,MachineLearning,cq13wgi,1,t5_2r3gv
3588175,0,t1_cq153hl,Yeah the physics of the simulation seem to be tuned to make it easier for the AI. Right now at least dont know if its still learning it just jumps at the net when I have the ball then backs up when I serve it then jumps when the ball is coming down in a certain height range. There are some subtleties there to make it serve the ball forward but really its not complex behavior at all. Theres no way to launch the ball faster than the movement speed or at a low angle at a speed fast enough to get past the width of the paddle. Its really just Pong with a paddle thats always the speed of the ball.It did drop one point but that was when it had the serve and it misjudged an angle close to the net. There seems to be no way for the human to make the ball go at that angle so its impossible to exploit.,MachineLearning,cq1a2zr,3,t5_2r3gv
3595548,0,t1_cq1t35k,I see your point. Im making a distinction between parameters and activations. The number of activations to be stored is indeed linear in the length of the sentence but the number of parameters is not. Compare this to the current Caffe master branch where each LSTM parameter would have the weight diffs duplicated  times.The difference between parameters and activations may seem subtle but for RNNs the optimization makes all the difference. Each layers parameters cost far more memory than its activations. For an LSTM layer with  hidden units youll have a weight matrix of size  x  whereas your activations will be closer to  x |batch size|   x . Clearly you do not want to be paying the former cost  times. And indeed before making the code change I could only run LSTMs of size up to  on a  TI. Now I can run LSTMs with  units.So why is this optimization not already implemented? It turns out its pretty tricky to do without making vast changes across the codebase. The obvious approach would be to have each layer use the same pointer to the weight diffs. As you call the Backward on each layer it would add its contribution to the current diff. When you finish the backward pass youll have the correct total diff and can zero out the diffs for the next round. The problem is that most of caffes current layers do not add to the weight diff  they set the weight diff. In fact the entire testing framework assumes that each layer zeros out its weight diff before running the Backward step. So if you dont want to change every layer you need to do something more clever.Now I can finally answer your question. Heres how it worksEach layer is left unchanged. Shared parameter weight diffs are set to point to the same data. For each shared parameter an additional master weight diff is introduced. The total memory usage is now weight.data  weight.diff  master.diff a  increase. The backprop code is then modified so that after each Backward step shared parameters from a layer are added to the master diff. After ever Backward step has completed the master diff has the correct gradient and is used for weight updates.,MachineLearning,cq24atg,4,t5_2r3gv
3647808,0,t1_cq80ljb,The regularization here is controlling something more like expected perturbation of model response with respect to input perturbation. Goodfellow et als work was based on regularizing for worstcase perturbation of model response w.r.t. input perturbation. See also the analogous relationship between stochastic programming  and robust optimization  Youre certainly right though that regularizing to minimize various gradientlike properties of a function approximator has been pretty thoroughly workedover.,MachineLearning,cq82skh,4,t5_2r3gv
3648822,0,t3_2qrbf8,Theres a lot of hype about this class of algorithms currently but perhaps you should try constructing a deep neural network. I am working on building one from scratch in R using mathematical and graphical models as guides.I taken part of Andrew Ngs and Geoffrey Hintons courses on the topic so I have some background but this time around I am planning on implementing using my own data structures and gradient descent algorithm. Its pretty rewarding so far and I am gaining a much deeper understanding pun intended.,MachineLearning,cq86yac,1,t5_2r3gv
3665339,0,t3_32cynp,deleted,MachineLearning,cqa2nia,2,t5_2r3gv
3673606,1,t1_cqazxow,Could be. Id say a fair analogy might be KurzweilTysonLaCunHawkings. Thats probably overstating LaCun by a margin but I think its at least an illustrative analogy,MachineLearning,cqb0jz1,1,t5_2r3gv
3676186,0,t1_cqbajwf,Certainly. We make mistakes all the time. And whenever we realize that we made a mistake we correct it and we move on. Mistake or no mistake its always . No such thing as a  recognition.,MachineLearning,cqbb557,1,t5_2r3gv
3680380,0,t3_32ihpe,I would like to ask where do you see Machine Learning currently heading? I have the impression that it is growing in popularity so I am interested about your thoughts on the current demand for data scientists?Also for Dr. Ng I have just finished your course on Coursera and would like to thank you for this amazing resource. I am interested to know why is Reinforcement learning omitted from the course and are you planning on creating more courses in the near future?,MachineLearning,cqbscti,2,t5_2r3gv
3682502,0,t3_32ihpe,Hi Im wondering what is your guys take on neural tracing? Are you aware of projects focused on this outside of Seung Labs and Eyewirecollaborators? Would you have an idea of how soon we can expect the process to be completely automated?,MachineLearning,cqc12qs,1,t5_2r3gv
3686829,0,t3_32ihpe,Which approach to AI do you think will be more powerful? The neuroscience and biology heavy method or the more statistics oriented approach?,MachineLearning,cqciudu,1,t5_2r3gv
3702705,0,t3_32roqb,This paper  and this one  are probably the closest Ive found to a set of guidelines when talking about deep nets.If youre curious I just did a very informal writeup  on the state of deep learning pretraining architecture selection and initialization with a bit of a focus. In it I summarize those two papers towards the bottom and a half dozen or so others.  May be worth your time to read.Enjoy! ,MachineLearning,cqec00p,4,t5_2r3gv
3705207,0,t1_cqem7m9,OK I will be interested to see how it compares. Maybe the fact that I is a special kind of orthogonal provides lots of the benefit? But either way it seems nice.,MachineLearning,cqem9va,1,t5_2r3gv
3712601,0,t3_32wmes,Unsupervised learning does not model PXY it models PX.,MachineLearning,cqfgmub,2,t5_2r3gv
3724607,0,t3_332557,At my knowledge jobs in software engineering are paid better than what I have seen in BI. So accepting a lower wage is not a option and dont expect to recover this money after you have gained some experience in the field.,MachineLearning,cqgtx1s,3,t5_2r3gv
3729965,0,t3_330hpz,What an elaborate post uBinaryAlgorithmDo you use some widely known agent modelling software or libraries such as NetLogo Framsticks JADE MASON Repast?Do you happen to publish your code on the Internet?If you want to evolve architectures of NNs then you can of course do so. A population generation step could generate agents with different architectures then you could employ some other learning techniques to learn weights. You will easily google some papers on the subject by typing evolving neural network architectureIf you want to encode architecture of the ANN in a genome then take a good look at Framsticks the best genome representation Ive seen so far. You can evolve complex bodies sensors body parts brains etc. and NERO  and You write gt Because I dont program GPUs I have to limit my simulation processing on my PCWell you can of course program your own GPU! It will certainly speed up many computations relevant to neural networks. If you have a GPU that is not older than  years it is very likely that you will have little problems with this. Even my laptops GPU is programmable in such a way.You writegt The brain is active in the absence of inputs. So one of my concerns is helping the Agents be able to save inputs for recall I guess this can be done directly in memory? But how to recall it as a synthetic input without interfering with actual input? and how to make that useful?.Yes brain is active all the time. Inactive brain is a dead brain. You will find many articles code papers on spiking neural networks It makes the net continuously active.As for the memory look at recurrent reural networks RNNs for example LSTM The goto place for links and articles on the subject is Jrgen Schmidhubers page RNNs are awesome for recalling memories. You writegt In other words not just instinctual behaviors but planning requires a sense of timememory. Any ideas?Netwoks playing Atari  games were recently making a buzz on the interwebs You may look at the papers and Qlearning.,MachineLearning,cqhfuz0,2,t5_2r3gv
3732143,0,t1_cqh9ycl,That does not save you from gazillions of parameters necessary to keep in memory though. Also if your output is a softmax over words too only half your problem is solved.,MachineLearning,cqhosa0,2,t5_2r3gv
3761656,0,t1_cql0w4h,K training samples divided between  groups. My final objective is to classify this group of samples not the samples themselves. To classify a group of samples I run my classifier for each one of them then I chose the class that was most present in this group majority voting. I found the majority voting calculations to be particularly heavy on GPU or I didnt find an efficient implementation so Im not doing it for training or validation that is performed all the time but simply computing the classic error rate with individual features for validationInb I am already due to make some simulations with the actual test procedure for validation to see if the negative correlation is still present but that will take time,MachineLearning,cql224a,1,t5_2r3gv
3780667,0,t1_cqn1ii1,deleted,MachineLearning,cqn8273,-1,t5_2r3gv
3785451,0,t3_33op9l,Why not start with a double major and decide which you enjoy more?  If you go to grad school then I imagine that either choice is fine.  If you go to industry some groups will prefer a CS degree and others wont care.  Im skeptical that there are groups that would really prefer a Physics degree.  ,MachineLearning,cqnrrl4,2,t5_2r3gv
3795143,0,t1_cqoos04,I know some of the organizers. I believe this is primarily targeted at the research community. I am pretty certain they very rationally selected the hidden functions and I expect them to unveil them by the end of the competition.,MachineLearning,cqovkoc,5,t5_2r3gv
3851400,0,t3_345cy3,Check out the following videosarticles they may help.Predicting Horse Race Winners Using Advanced Statistical Methods  What are my odds?   Machine learning the hard way  a story about ponies  A new methodology for generating and combining statistical forecasting models to enhance competitive event prediction  ,MachineLearning,cqvamjg,1,t5_2r3gv
3852906,0,t1_cqver3x,Thanks for the answer it was very informative.To summarize youre saying that any choice of q leads to a valid variational objective so we can still think of it in that framework regardless of the arguments of q.  However if we choose a q that is not able to approximate the true posterior then the variational objective will be a poor proxy for px and we will end up with a poor generative model.What are your thoughts on incorporating side information into the variational posterior?  For example if we had labels we could build a model of px using qz|xy as the encoder.  Do you think this is a reasonable way to incorporate side information or would you prefer to include y in the generative model as in your semisupervised VAE?,MachineLearning,cqvgt35,2,t5_2r3gv
3856579,0,t1_cqu7bcn,Since thats nonsense as viewed as maximizing likliehood the thing to reconsider is the assumption of maximum likelihood estimation as the only thing to do. Sum of squares in this context of probabilistic predictions is known as Brier score and there are significant theoretical reasons to consider using it. And see Joyce  quoted above.  Now thats some very technical philosophy which Im not qualified to judge one way or another but its just to show that MSEBrier score is not a priori nonsense in this setting. See also gt  citations on Google ScholarOne very practical reason to not use MLE for some problems is that it responds very heavily and excessively to outright mistakes from your model or just plain wrong data like an incorrect target.That is if your model predicts a very very low probability for some outcome and the outcome did happen the penalty in your target function will be very high for that example.  That may make the modeling process work valiantly to amelioriate a really bad mistake which you werent going to model right anyway at the expense of successfully modeling more of the cases which were reasonable to catch.  Consider a rare event model like looking for a rare disease or network security breaches.  There will always be some outliers which wont be catchable false negatives or false positives.   MSE minimization tends not to overweight them the way MLE maximization can. And finally there are some practical gotchas that may make MLE appear to be better than MSE in an application such as a multilayer perceptronANN but it just turns out that the typical magnitude of gradients is substantially larger with MLE ive seen typical factor of  so it acts as if the learning rate were x larger which may make empirical experiments appear to be better for a spurious reason. Ive been trying to find some reference without success that confirms something I thought I once heard that Brier scoreMSE can be seen as a solution under some reasonable Bayesian assumption the way crossentropyBernoulli likelihood is the usual result from MLE maximization.   I dont know if thats trueif anybody else can help me Id appreciate it. There is Theorem  here  but that seems fairly complicated. There are other practically useful optimization targets for categorical estimation and prediction hinge loss and quadratic hinge loss.  And then you may consider asymmetrical functions which treat positive and negative examples differently if the problem and intended use of the model calls for it.There is always the popular area under the ROC curve AUC and variants thereof but thats not obviously differentiable in parameters the way cross entropy or Brier scoreMSE is or is a sum of individual terms but it has other advantages. Ive trained models deployed in significant commercial settings  MLEcrossentropy is not the alpha and omega for reallife categorical prediction. ,MachineLearning,cqvvw0e,2,t5_2r3gv
3856713,0,t3_34lkg0,You might want to check out the HTS toolkit which allows you to do TTS using Hidden Markov Models. There the problem you mention is handled by training perphoneme HMMs that depend on their context e.g. preceding and following phonemes then similar contexts are clustered using a decision tree so if you dont have a specific context in your training data youll get the next best cluster.You might also want to check out Festival.,MachineLearning,cqvwfrx,1,t5_2r3gv
3865080,0,t3_34piyi,my uninformed current idea is that vanishing gradients problem is reduced because errors can somehow stay on the carousel for a while until forget is triggered. exploding gradients are avoided because sooner or later forget is triggered. it seems to be something like that the effect of the errors is more like a step function that is constant for a long time until the moment of forgetting. whereas without the carousel the trace of the error would typically tend to decay exponentially.,MachineLearning,cqwuq9j,1,t5_2r3gv
3871523,0,t1_cqxkjd4,Yeah I need something smaller. I just downloaded the dataset and its . GB untared. I am looking for something I can let my prospective interviewer work with interactively. ,MachineLearning,cqxl5hl,1,t5_2r3gv
3880987,0,t1_cqyn8ar,When you say that bitfloat multiplication is  times faster you are comparing to a naive implementation right?How many bitfloat multiplications do you do per second when you do matrixmatrix products ?,MachineLearning,cqynzoe,3,t5_2r3gv
3884212,0,t1_cqyvnks,NetinNet used a smaller Network to replace convolutions. This is using an RNN to replace convolutions. At a high level the approaches are very similar. Given that they high level idea has been done before and this paper didnt seem to try and explain anything or try very many configurations  Im having trouble seeing the novelty. ,MachineLearning,cqz17qc,1,t5_2r3gv
3887249,0,t3_34yqig,This one maybe? ,MachineLearning,cqzdp1d,2,t5_2r3gv
3942787,0,t3_35kzlo,Zobrist hashing comes to mind.,MachineLearning,cr5pmcl,2,t5_2r3gv
3968960,0,t1_cr4ug8p,Thanks for the comment it is interesting feedback for me.So artificial reincarnation ,MachineLearning,cr8p175,1,t5_2r3gv
3974210,0,t1_cr8a2zw,deleted,MachineLearning,cr9akhx,1,t5_2r3gv
4008774,0,t3_368r87,Where does the formula for parameter initialization come from? It seems pretty random. I wish citations were included.,MachineLearning,crd8ba1,1,t5_2r3gv
4015012,0,t1_crdvtkn,VR will not hit mass market like the iPhone for another  years. Youve been listening to too much Ray Kurzweil ,MachineLearning,crdxwuu,0,t5_2r3gv
4049727,0,t1_crhweib,ampampamp Law of the unconscious statistician sfw gtgtIn probability theory and statistics the law of the unconscious statistician sometimes abbreviated LOTUS is a theorem used to calculate the expected value of a function gX of a random variable X when one knows the probability distribution of X but one does not explicitly know the distribution of gX.gtThe form of the law can depend on the form in which one states the probability distribution of the random variableX. If it is a discrete distribution and one knows its probability mass function X but not gX then the expected value of gX isgtgtgtInteresting Expected value | List of probability topics | Probability density function Parent commenter can toggle NSFWmessagecompose?toautowikibotampsubjectAutoWikibot NSFW toggleampmessageBtogglensfwcrhwf oror deletemessagecompose?toautowikibotampsubjectAutoWikibot DeletionampmessageBdeletecrhwf. Will also delete on comment score of  or less. | FAQs | Mods | Magic Words,MachineLearning,crhwf13,1,t5_2r3gv
4050064,0,t1_crhr2cm,Congratulations you were finally able to use irony in a sentence. Unfortunately you probably did it by mistake and are still just as clueless about irony as you are about puns and machine learning.,MachineLearning,crhxszb,1,t5_2r3gv
4056075,0,t1_crikueg,BPTT was done for  timesteps and it was kept constant. I never tried varying the number of timesteps during training actually so Im not sure if it would help. Maybe someone with more experience in RNNs could chime in and help? The architecture was quite small because I was training on a CPU and wanted to have some results as fast as possible. I used  LSTM cells per layer and  layers. All songs were generated by the same model but at different stages of training The Boyse was around the th epoch Jimmy OCorlan and Paddy Pilcor around thth epoch and Paddy Mountry Airding Baie after  epochs.,MachineLearning,crimfpv,3,t5_2r3gv
4056494,0,t1_crio44a,In forwardbackward algorithm the next step depends on the last step. Thus the loop cannot be vectorized.,MachineLearning,crio5jr,1,t5_2r3gv
4162651,0,t1_cru98wh,Shouldnt you do a lot better than  if you have a label leak?  What is the human performance on CIFAR?  I often get it wrong just because the images are so blurry and lowresolution.  ,MachineLearning,crur2pc,2,t5_2r3gv
4189764,0,t1_crxt2xf,There is a MOOC on graphical models. It might be on Coursera dont remember but the videos are definitely on Youtube as well. ,MachineLearning,crxu000,4,t5_2r3gv
4190210,0,t3_38sub5,Id want to do a crossdepartmental Ph.D. that combines machine learning with say economics or marine biology.  Right now I think machine learning is too siloed within statistics or computer science departments whereas machine learning plus some other area of domain knowledge will be more useful in the long run. ,MachineLearning,crxvthv,8,t5_2r3gv
4190290,0,t1_crxvq3u,Right that clears it up actually! Thanks.,MachineLearning,crxw597,2,t5_2r3gv
4200982,0,t3_38y1y3,Efficiency can be important as you can use a lot of processing power for training.  Perhaps look at creating a GPU option if you can.Definitely allow the ability to supply activation functions whether step wise or any function they supply. Allow bias and unbiased options for it not every implementation uses bias.,MachineLearning,crz3s6v,1,t5_2r3gv
4207007,0,t1_crzpv4t,Sure! One of my teammates has this on his webpage,MachineLearning,crzscj1,14,t5_2r3gv
4230427,0,t3_39cb8b,To my understanding this is done all the time in deep learning.See ,MachineLearning,cs2gaop,2,t5_2r3gv
4249885,0,t3_39mk4r,What an incredibly misleading headline.,MachineLearning,cs4ocg2,14,t5_2r3gv
4250055,0,t1_cs4niet,deleted,MachineLearning,cs4p1fh,2,t5_2r3gv
4256335,0,t3_39pypu,Here are links to some neural network related ones and others I am reading found by simple Google Search,MachineLearning,cs5en8l,2,t5_2r3gv
4264450,0,t1_cs640oi,deleted,MachineLearning,cs6bl57,-4,t5_2r3gv
4290190,0,t1_cs8hsei,My thought exactly ,MachineLearning,cs98599,-1,t5_2r3gv
4329778,0,t1_cscrlh0,howd you compile all the CUDA stuff without a C toolchain? Most of modern Deep Learning is absolutely not possible with a proper GPU.,MachineLearning,csdp1es,2,t5_2r3gv
4332904,0,t1_cse1je1,deleted,MachineLearning,cse1q0r,-1,t5_2r3gv
4363605,0,t3_3azryw,Are you using TheanoPython for your neural networks? Id recommend grabbing a NVIDIA GPU in that case. I have a GTX and its great. Id pick up a card with at least GB of RAM.Other than that I have a iK and GB ram... Wish I had purchased a larger SSD GB.My rig came in at about  CAD. If you want to spend more Im certain theres a better CPU and multiGPU setup you could go for if you think you could use it.Thats all assuming you want to get a desktop. If you are going for a beefier laptop Im not sure about that..,MachineLearning,cshigvq,2,t5_2r3gv
4363655,0,t1_csh8mid,Yes AFAIK.,MachineLearning,cshio7q,1,t5_2r3gv
4372027,0,t3_3az4qj,I would like someone to buy a GPU farm and project this on an IMAX dome. That would be real trippy. ,MachineLearning,csigo5z,1,t5_2r3gv
4394101,0,t3_3bci3r,Why are chatbots interesting?edit since this is top comment and most of my comments are now .  is it because Im not impressed?  Im trying to be nice and point out their limitations.  it feels like people are too enamored by shiny bells and whistles that they dont realize the triviality of it.  also so its clear  theres a distinction between chatbots and dialogue systems. dialogue systems typically go from understanding gt dialogue state gt generation.  This allows for task state to play into what utterance to generate.   It can and has been a statistical endeavor.  It can and should be trained endtoend.  This is not that.  This is just seqtoseq on movie scripts.  ,MachineLearning,csky8lb,-1,t5_2r3gv
4416442,0,t1_csn2e63,deleted,MachineLearning,csngu0d,1,t5_2r3gv
4426809,0,t1_csoo0rh,If even one person is capable of detecting bullshit it is more powerful than this RNN as constituted. A RNN has no mechanism to reject a training example it is forced to consider it as truth. ,MachineLearning,csoovbe,1,t5_2r3gv
4432605,0,t1_cspedfg,First of all I wouldnt call anything the algorithm does extrapolating. The data set is from Google images I can almost guarantee there are similar pictures of airplane wings labeled as airplane. Even if there werent picking out a wing as part of an airplane would be interpolating not extrapolating.Im also really not sure what you are proposing. Are you saying someone should manually come up with huge lists of these feature points go with this word? Or are you saying it should just always classify guerilla as person to avoid this?,MachineLearning,cspf2e8,4,t5_2r3gv
4437520,0,t3_3bw0h2,Find a machine learning library in the language of your choice and play around with it. Im still learning but that reading scholarly articles helped me the most.,MachineLearning,csq193q,1,t5_2r3gv
4448233,0,t3_3brpre,deleted,MachineLearning,csrdvzu,1,t5_2r3gv
4480433,0,t3_3cgqom,I have tried various approaches to this so far with little success.I tried classifiers on music by band.   I tried adversarial networks.  I tried RNNs.I tried both directly on a waveform wasnt really expecting that to work and on stftd chunked audio.I had most success with an adversarial trained LSTM RNN with a small avoidance force on the state vector to deter the system from getting stuck in a stable loop.Even so results were by no means good  there were odd hints at melody but not much beyond that.In general my STFT system didnt have a good way to embed phase information for the network to process.   I ended up just inputting the real and imaginary components as independent inputs which is obviously suboptimal considering absolute phase compared to the sampling clock isnt relevant in music.Also most music isnt absolute in pitch.  Ie. a tune can be shifted up one octave and still be the same tune.   Since the outputs of a fourier transform are in linear frequency space but octaves are only equidistant in log frequency space the network couldnt learn that relationship.   I couldnt find a good short term logfrequency reversible transform to use.,MachineLearning,csvfcia,11,t5_2r3gv
4521500,0,t1_ct0ki8f,Theoretically you might be able to use the hierarchical softmax of wordvec to hash to a vocab of words. Id be surprised if wordvec ever became that precise but if you wanted to reduce the size of your output matrix that might work.,MachineLearning,ct0kqvt,0,t5_2r3gv
4568848,0,t3_3dm7cc,I always grab it from pip anyway.,MachineLearning,ct6ikb2,1,t5_2r3gv
4569404,0,t3_3dm7cc,,MachineLearning,ct6l2rn,3,t5_2r3gv
4573472,0,t3_3dn9b4,Holy scrolljacking Batman! ,MachineLearning,ct73g39,0,t5_2r3gv
4587938,0,t1_ct7tyjt,Thanks for this  Ive gone through those Oxford practicals and youre right  theyre the best introduction to Torch Ive been able to find. Particularly Practical  really helped me to understand how complex net architectures are assembled. Much appreciated.,MachineLearning,ct8wo28,1,t5_2r3gv
4589613,0,t1_ct93vua,s total.The time comparison was without the compile time because I have builded the model before than I measure only the traintest time...This time on Keras was either using CPU or GPU... It was much worse than simple Weka. Id expected that Weka should be faster but not much faster like what Ive got...,MachineLearning,ct9481u,0,t5_2r3gv
4597200,0,t1_ct9xd0s,Chainer depends on implementations of a forward and backward pass for all operations you want to use. It does autodiff based on these. If you want to use an operation thats not supported it is possible to add new operations adding any simple functions that are already supported by NumpyPyCUDA is trivial. ,MachineLearning,cta2g8p,2,t5_2r3gv
4645960,0,t1_ctg4h9k,Sorry I was in rant mode and youre totally right. I work with neural networks and know they are not linear to their own advantage in most cases! ,MachineLearning,ctg6rdo,1,t5_2r3gv
4646415,0,t3_3ejwx9,deleted,MachineLearning,ctg8tiz,1,t5_2r3gv
4664132,0,t1_cti8inl,Then you are talking about simulation for the sake of understanding  biology.  In that case it totally depends on ones goals and resources.  You can use up arbitrary amounts of compute power if you want to simulate down to the molecular level of cells and then beyond that down into the quantum level.  The key for any practical simulation is hierarchical approximation where you can focus resources on details and scales of importance and relevance.,MachineLearning,ctigwd5,5,t5_2r3gv
4673171,0,t3_3eurqn,I think Andrej Karpathys charrnn repository is the best starting point for a wouldbe hobbyist if youre willing to install Ubuntu to get it to run. The code is simple and open source all of the tools are free text data is very easy to come across store manipulate etc. and the outputs are magical at least to my innocent eyes.,MachineLearning,ctjlrev,1,t5_2r3gv
4682280,0,t1_ctkqknc,Thats a good point thank you. Increasing the number of features in the fake set hence increasing the chance of correlation can lower this number down much further.,MachineLearning,ctkqxcn,2,t5_2r3gv
4685935,0,t3_3f4eoo,deleted,MachineLearning,ctl7gzd,1,t5_2r3gv
4697402,0,t1_ctly2wh,deleted,MachineLearning,ctmnawb,0,t5_2r3gv
4702526,0,t1_ctn2hk3,Hmm I can probably make a shitty first draft of it in just a few days. No guarantee that itll work but I can hack something out of my RNN playground...,MachineLearning,ctnahth,3,t5_2r3gv
4760893,0,t1_cttzc6w,stack RNNs are a much simpler formulation than NTM and I think are targeting similar tasks like copying addition etc. ,MachineLearning,ctunlep,2,t5_2r3gv
4766221,0,t1_ctvbenh,True but I think its more than just a pure numbers game. Tenure track professors in CS make a median salary of k while entering into a scientist position will probably get you k more or even moreless theres a lot of variance.Personally I dont think the salary in academia is commensurate with the stress of managing grad students writing grants teaching classes etc. Id rather work  and make the same amount which is an argument Ive heard from other grad students far more often than paying off debt.,MachineLearning,ctvbqpv,3,t5_2r3gv
4804298,0,t1_ctzetkf,I would recommend torch over keras unless you are very familiar with python and dont want to learn another language right now. Keras still has some limitations.,MachineLearning,cu04e3v,1,t5_2r3gv
4833593,0,t1_cu3l3fz,Yeah this could be very interesting ,MachineLearning,cu3tal4,1,t5_2r3gv
4852507,0,t1_cu66c7i,As I said thats my doubt. However in my unrelated job I have a similar problem and the minimum I find tends to be global or at least a very good solution. Thats why I wanted a proof and since this is also used in NN its subject to the same question.For now I only have that intuition A lot of data  a lot of parameters gt its unlikely to have a lot of different solutions where the gradients are zero for all the parameters. As for NN the next chapter in the book may solve my doubt by using the sigmoid neuron and the crossentropy function the gradient is only zero when the output is exactly the desired one. You can check it out here  really nice book.As for the global minimum theres no way to guarantee it with an optimization method. There are only heuristics such as Global Search genetic algorithms etc.,MachineLearning,cu670ll,1,t5_2r3gv
4858742,0,t3_3hfwjs,Hi guys this is my first post on reddit so please go easy on me. I dont know if this is the right place to ask any question about ML but what Im hoping to get from this place is a community of awesome people with time to drop a line with a comment hint opinion or whatever else might help me make a clearer picture in my mind about this field. Also Im a total beginner with ML amp AI amp everything related to this field so I am ready for some hate  the noob.Im ready to learn so if any of you could point me to the right direction that would be great.Thanks!,MachineLearning,cu6zbe7,1,t5_2r3gv
4894302,0,t1_cubgcwa,Holy shit your suggestion is awesome... Crawl a bunch of repos on GH using the files as training data and then provide it a new snippet and let it guess the language.Thanks man! D,MachineLearning,cubgsc1,2,t5_2r3gv
4895624,0,t3_3hqbhy,no. CNN learns many layers of features. Other methods have done well using random convolutions  kmeans image patches see Coates work Kakades work and others which exploit the same sort of priors about the nature of images but only approaches that rely on extracting many layers of learned features read deep learning with neural networks  nonconvex optimzation on layered models have really been competitive.,MachineLearning,cubmsab,1,t5_2r3gv
4902624,0,t1_cuc1puf, and the associated paper ,MachineLearning,cuciig6,1,t5_2r3gv
4908022,0,t1_cud61tu,gt My intuitive theory is that because images are so highdimensional theres a practically infinite number of different ways to produce fooling images and any particular negative training example can only make the net robust to at most a few different ways.Thats the explanation Ive heard as well. You cant produce and train on all possible fooling images. I think in order to combat the kind of noise that can move an image from one category label to another incorrect one there is some reasonable intuition that if you train with an other category not just noise but anything that doesnt fall into an existing category youd could end up with tighter representations of the categories youre interested in and maybe theyd be less likely to fall prey to those fooling images. But yea it doesnt work that way due to how high dimensional the input space is. Another weirdness about the other category it doesnt really make sense to try to train a single category to catch a large variety of inputs. All images under a certain label should have something in common for the net to find and its not any of the other categories isnt quite good enough.Anyways back to the OP... haha,MachineLearning,cud6yg8,3,t5_2r3gv
4915022,0,t3_3i406k,Nice results. Is there any indication that these in fact are all evenly distributed around one or more canonical antonym vectors in that space? The tnse plot cluster is great  it could be interesting to generate a second one that included other analogies  either meaningful or random  to get a better sense of scale.,MachineLearning,cue2qv2,1,t5_2r3gv
4928713,0,t1_cufnij4,still that sounds like an algorithm problem not a language issue. doesnt all the compilation happen through external programs anyway?,MachineLearning,cufsxrf,2,t5_2r3gv
4933213,0,t3_3igx2m,It is mostly the same as a conditional generative RNN  the big difference is that your conditional information rather than being self generated or coming from the input comes from the previous ground truth as well.You can probably train a model to explicitly do a simple form of this if you have something like a thesaurus that knows valid replacements and treat words as their latent representation then sample the actual word from that.In general though I am biased I think this problem is best handled by an conditional RNN with stochastic latent variables where you hold the latent variables for a given conditional input fixed then generate from them repeatedly.,MachineLearning,cugdcub,3,t5_2r3gv
4938642,0,t3_3igxoe,What would it take to go about indexing another language in this way assuming you already have a corpus?,MachineLearning,cuh20bo,1,t5_2r3gv
4940511,0,t3_3ikkdn,Here are a few plots from my last projectAt the beginning I often use tSNE to visualize the dataI plot performance over topn features to find out how many features I really need in this case I only needed about  out of  features for good performanceTo select my final features I plot importance over feature number and highlight the topn features as found by the previous plotAnd ROC curves with different cutoffs to optionally tune true positivefalse positive ratesI also create plots of cross validation  parameter tuning results to make sure it selects parameters in a smooth area  not an outlier.. I dont have a nice plot of that here right now though.I mostly use Python and R with scikitlearn matplotlib caret and similar libraries and create whatever plots I need myself.. most of them go into IPython notebooks.,MachineLearning,cuhah4s,27,t5_2r3gv
4979061,0,t1_culzm7h,Neat thanks!  Added it to the video description. ,MachineLearning,cum5ccc,1,t5_2r3gv
4987801,0,t1_cun78tm,I think that could explain a few other things too for example if you change the resolution of the image it affects the results significantly. I tried with small images at first only GB on my GPU and it resulted in some overflowsSometimes for really small images it diverges to NaN. This also is making it harder to tweak the hyperparameters they depend on other factors... Going to check the paper for details about normalization.,MachineLearning,cun7qun,1,t5_2r3gv
4992768,0,t1_cunskho,I think there is a place for a Theano like graph based tool that has a lighter C interface. cgt seems to be targeting this in Python but maybe a Julia alternative could explore similar space. You really just need the ability to create nodes do a topological sort combine things and do backpropsecond orderetc. Would be awesome and I have considered taking on the crusade myself.,MachineLearning,cuntcgf,6,t5_2r3gv
5016689,0,t1_cuqp4d8,deleted,MachineLearning,cuqp9ap,1,t5_2r3gv
5025353,0,t1_curpruy,gt this new nomenclature sounds like titles for upcoming M Night Shyamalan movies.The surprise twist in The Human Kernel will be that it does not map to an inner product space.,MachineLearning,curqvpk,2,t5_2r3gv
5044604,0,t1_cuu0qlh,I dont think so my impression is that it has to be hardwired for specific tasks.The extremetech article says thisMoving forward the research team led by Chris Eliasmith wants to imbue Spaun with adaptive plasticity  the ability to rewire its neurons and learn new tasks simply by doing rather than being preprogrammed.Now the tasks they show it performing are perhaps comparable to tasks that could be done by certain deep learning models.  But such ML models are taught to perform these tasks by optimizing certain objective functions they are not programmed to perform them.  I am intrigued and puzzled at how they were able to obtain such behaviors.  It would be virtually impossible to calculate the correct weights in a NN needed to obtain a certain desired behavior other than by optimizing some objective function.This may indicate that the neuroscientists have made more progress in understanding some simple brain functions than I thought but until they can figure out how to get it to learn on its own it is no where near general intelligence.,MachineLearning,cuu2ef4,1,t5_2r3gv
5045317,0,t3_3k1o8v,Faster to train higher performance and... no solid facts. Im looking forward to seeing the result but still rather skeptical that they will outperform the next generation of conv nets at the moment. ,MachineLearning,cuu5ht0,2,t5_2r3gv
5050494,0,t1_cuugntt,. after  epochs.Heres the training curve,MachineLearning,cuurxx0,1,t5_2r3gv
5054779,0,t1_cuv9q8b,gt Sensitivity analysis is a different thing altogether. Essentially its a rate of change analysis i.e. dyjdxi where yj is the and xj a predictor.Thats what it is but thats not why people do it.,MachineLearning,cuvajb7,1,t5_2r3gv
5066085,0,t1_cuwmxm0,First of all thanks for your answer! Im pretty much desperate with this topic and even thinking about quiting this direction of specializationI am not the type of person that likes using frameworks before doing some research on my own so I cannot really make a statement about the MatLAB stuff also I code all my stuff myself in python.to  I always go for traintest with a ratio of  or  and if the model whether generative or discriminative performs good I tend to go for  or even .Also Id suggest computing characteristics of the data to get a feeling for its quality because Machine Learning imho tends to function in a crapin crapout fashion.I computed things like marginaljointconditional probability for single time steps with and without averaged over trials and overall entropy and variance of and artificial bootstrap data set to see if the data lacks quality.to  I was trying to start with rather simple methods instead of going for neural networks because I havent worked a lot with them. I however remember these confusion matrices from my Bioinformatics roughly biological computer science course and my lecturer back then told us that there are plenty of those matrices and none of them really functions or theyre even wrong. However no clue for genome forecasting from my side. I read a lot about things like time series analysis as a starter and got the basics of autoregressive moving average processes and ARIMA models. In the papers I read they also wrote that Markovian Models are probably the way to go because of their simplicity and proper results.One thing I dont quite get is why no one goes for Gaussian Processes or Deep Neural Networks havent worked with DNNs yet. They seem to be helpful when modeling process structure but as I said I am no expert!For the basic terminology I found that the shape of a time series matters more precisely trend and seasonality but I guess that this does not apply to genome sequences as well as it doesnt to my example above. One also does look at the stationarity of a time series as a consequence of trend.In your case Id suggest using a hidden markov model in MatLAB you can write one on your own this should be done fast and try to predict sequences of genomes with a hidden or latent mechanic best understand by looking at its visualization as a graph or over time with a trellis diagram.,MachineLearning,cuwnkty,1,t5_2r3gv
5092786,0,t1_cuzv7si,  it would have made more sense to post this link directly tbh.,MachineLearning,cuzvegl,4,t5_2r3gv
5126449,0,t3_3l5s6t,In the second paragraph of What is a Gaussian distribution can you please change standard deviation to variance? ,MachineLearning,cv3xgu5,1,t5_2r3gv
5134579,0,t1_cv4mjr6,deleted,MachineLearning,cv4wpfz,1,t5_2r3gv
5152667,0,t3_3lkjzv,Here is what I think you should doinput is a pair xin xoutoutput is  or .  if xout is a correct output for input and zero otherwise what you should be predicting is whether a pair are in the correct relationship.,MachineLearning,cv737oq,2,t5_2r3gv
5155446,0,t1_cv7bvjg,Do you mean that in the sense that maybe also alluding to what yggdrasilly says below that there is no unique way of learning from the data? I.e. mere memorization of the training data could just as well be seen as properly learned as much as deducing in my case the rule?If thats the case I guess it plays into my answer to bhmoz below where I was hoping for a learner that learns the simplest simple maybe based on entropy of the model or the like way of describing the data.,MachineLearning,cv7f98d,1,t5_2r3gv
5188160,0,t3_3lxl6i,A basic system like this was developed in the s.  Its the crudest and smallest possible world in which one could use language by Terry Winograd SHRDLU via wikipedia and SHRDLU via stanfords page  theres a whole history attached to the project too.Basically its a block world where you can command a crane arm thing to move blocks.  It can be said to understand the instructions because the world is small.theres a revival of it at brown with source code,MachineLearning,cvbd3gd,2,t5_2r3gv
5245303,0,t3_3mu6oh,Someone call Tom Cruise.,MachineLearning,cvi8xzi,5,t5_2r3gv
5283650,0,t3_3ndh5f,Few issues with this articleThe idea of partial credit in supervised learning is not new.The original article here  just proposes a very specific way of doing this.You need some outside information to define a meaningful way to give partial credit. In other words you need more supervised signal. Exception to this is knowledge distillation which is a really clever way to learn it in an unsupervised manner.,MachineLearning,cvn1q38,2,t5_2r3gv
5297276,0,t3_3njjaw,If it is not for classification but only visualization tSNE is the way to go.,MachineLearning,cvosdi8,1,t5_2r3gv
5315670,0,t3_3nt6gq,Topic of the moment machine learning with neural networks.,MachineLearning,cvr510c,-17,t5_2r3gv
5366859,0,t1_cvxj2e8,gt youve got to interpret there as at hisher current location always.bAbi sure but it is trivial to construct examples in English that do not work that way.,MachineLearning,cvxodod,4,t5_2r3gv
5385563,0,t1_cvzbb2p,Most of the worlds best researchers have PhDs and this has been the case for a while.  There are exceptions like Freeman Dyson.  Perhaps research labs in silicon valley will create a new generation of toptier researchers who dont have PhDs.  Still unless things change dramatically in the future a persons odds of becoming a researcher without having a PhD are quite low.  ,MachineLearning,cw02e4a,2,t5_2r3gv
5480682,0,t3_3q4gwi,ML is unfortunately dividing up into highly doctrinal fiefdoms. As with many topics that begin in academia then move into the commercial world jealous turf wars tend to erupt.,MachineLearning,cwc7ijt,0,t5_2r3gv
5504693,0,t1_cwf85d7,From your linked wiki articlegt Supervised learning The computer is presented with example inputs and their desired outputs given by a teacher and the goal is to learn a general rule that maps inputs to outputs.I see what you are saying MV regression indeed sits in that set.I think my understanding of machine learning is captured in the paper by Breiman  which is linked in the Machine learning wiki article under  to statistics. Breiman splits the functions mapping X to Y into two categories Data Modelling Algorithmic ModellingWhere Data Modelling validates models using GoF tests and analysis of residuals and Algorithmic uses predictive accuracy. My university taught GLMs using the Data Modelling validation approach and methods such as CART and Neural networks using the Algorithmic validation approach. This is why I didnt consider Linear regression as a machine learning technique.When considering the above definitions and the blog author validating using predictive accuracy my original comment is nothing more than a difference in definition. Thanks uarrowoftime.,MachineLearning,cwf9y4q,2,t5_2r3gv
5511838,0,t3_3qk498,I was going to recommend Machine Learning in Python but it suffers from some similar issues. Really would like a ML book in idiomatic Python but its slim picking as as far as I can tell. Most of these authors are professors who dont code in a professional environment.,MachineLearning,cwg6r5m,2,t5_2r3gv
5516799,0,t3_3qmfbz,As someone who watched Jeopardy every day at the time I was profoundly disappointed in Watson. It was clear that they had rigged the questions to be tailorfitted to the kinds of questions Watson is good at. Jeopardy is full of wordplay puzzles puns and that sort of stuff but the questions in the show carefully had any of that cut out. Instead there were a lot of fillintheblank type questions finish these song lyrics put this four facts together a date a country the word kill and the word king to find the most relevant word in that context. Whats more annoying is that a lot of people tuned into Jeopardy that night and were not aware enough of the format to tell that the game had been rigged. I thought the whole thing was dishonest and deceitful.So is IBMs Watson marketing? Yes  that is the stated purpose of Watson! It was invented entirely for media hype. There were some problems with DeepBlue as well that misled a lot of people too so I think they were just trying to do the same trick twice except that in a fair fight with Watson I could beat it at Jeopardy also some time to buzz in might help... whereas I dont think Id ever stand a chance beating DeepBlue at Chess.,MachineLearning,cwgtk78,9,t5_2r3gv
5529079,0,t3_3qtn6o,I want to read it but i dont want to download a random zip file in order to do so. Why not publish it online?,MachineLearning,cwidzxl,3,t5_2r3gv
5532369,0,t1_cwi9cnm,gt This is propaganda. It has no academic content its just bashing Apple.How is this not an entirely fair and reasonable criticism of Apples approach to AI?  They have fallen behind their competitors in this respect and part of the reason clearly is their culture of secrecy  not allowing their employees publish and freely discuss ideas.  The article is spoton.Moreover this is a very important concern for people looking for a job in an AIrelated field i.e. many of the people subscribed to this subreddit.I say if people dont want to read articles like this let those articles get voted down.  Personally I thought the article brought up an important point and it belongs here.,MachineLearning,cwit468,3,t5_2r3gv
5559457,0,t1_cwm3kve,Ill check it out  thanks.,MachineLearning,cwm79lh,1,t5_2r3gv
5564193,0,t3_3rbij0,What do you mean by to average ? Neither weights or subgradients are averaged Instead what you have is a delayed model or subgradients in each worker. Moving subgradients is simpler because they might be compressed better. You may also control frequency of updates and gradients bitprecision Hogwild bit compression,MachineLearning,cwmseff,2,t5_2r3gv
5595929,0,t1_cwpcuh1,You dont consider opensourcing their entire suite of algorithms meaty? ,MachineLearning,cwqq7oi,2,t5_2r3gv
5598083,0,t3_3rq9l1,Set it up to retain X variance rather than setting a specific number of features. Prevents some simple mistakes and easier to work with.,MachineLearning,cwqzuo8,2,t5_2r3gv
5616642,0,t1_cwt9hhd,So Ill ask you what I asked urantana is the GPU mode fully functional yet?,MachineLearning,cwtarih,1,t5_2r3gv
5616771,0,t1_cwt832w,A comment from Yann on this post gives more detailsgt No paper yet on this VQA but Yuandong Tian and his collaborators have tried a number of different methods for this. I believe this particular demo uses a Memory Network on top of a ConvNet but Im told the performance is much more limited by the training data than by the method they all work more or less the same on the standard datasets.,MachineLearning,cwtbcai,5,t5_2r3gv
5624265,0,t1_cwu74sy,what are the thoughts on how to work around it?,MachineLearning,cwu8s2f,2,t5_2r3gv
5642067,0,t3_3se3to,Here is what I knowCt  ft  Ct  it  ChatttHere cell state is not repeatedly multiplied with some matrix. Instead changes that have to made to cell state are calculated and the update is basically a addition step.If the largest eigen vector of the recurrent weight matrix is more than one the nth power of it will have really huge numbers. But more important problem is when eigen vector is less than one. The gradients vanish to be very small signal to noise ratio drops and you cannot train for long range dependencies.If you want to think in terms of gates  error comes in and gets trapped in the cell which distributes it over all time lines. The error flow should be a consequence of your inference forward architecture. Cell is there to store both long range information like open brackets gender of a person in conversation and errors. Also Write and Forget gates could be both  or  at the same time. ,MachineLearning,cwwgb4u,1,t5_2r3gv
5644161,0,t1_cwwphqc,I usually sign up for things like this and yes  are vaporware but the remaining  makes it worthwhile. I found this in another sub and they had an interesting demo using NLP so thought Ill share.,MachineLearning,cwwpo3j,2,t5_2r3gv
5648375,0,t3_3shflv,Or they think autodiff and Theanostyle model flexibility are more important to research productivity than squeezing out the last drops of performance on standard convnet architectures. Of course ideally youd have both but possibly they just havent gotten there yet. Its also worth noting that GPUs in production systems are still a pretty new development e.g. Facebook is apparently still CPUs only so maybe they havent been an optimization priority thus far. ,MachineLearning,cwx8hxm,11,t5_2r3gv
5653481,0,t3_3sh9xh,deleted,MachineLearning,cwxvbfm,1,t5_2r3gv
5656292,0,t3_3sknex,Have you considered going for a Masters or PhD at the Air Force or whichever branch of the armed forces you served in academy? They all do a lot of interesting though sometimes classified research and your background would give you a large leg up there. You also might look at Virginia Tech Virgina University of Maryland and other schools in the defense corridor  you probably already have or have had at least a lowlevel clearance and many schools in the DC corridor need that for some portion of their research.As far as background goes you have basically what every student applying to graduate school has. If you were already an advanced ML researcher with an intense background you probably wouldnt be applying for graduate school in ML! I would apply far and wide including the schools you have decided are top. You may be surprised at the response you get especially with that GPA and a math background. Let them tell you no  if you never apply you definitely wont get in! Someone has to be in that top  after all  why not you?Keep doing projects and try to start a blog of some kind. Writing practice is important and writing is a much bigger part of graduate study than many people realize. You also might consider applying for a PhD. Funding is a huge help and many times you can drop out with a Masters if it turns out the PhD is not what you want. Though I suppose you also have military funding from the GI Bill?,MachineLearning,cwy7v1e,4,t5_2r3gv
5669935,0,t3_3srnzr,A margin loss?,MachineLearning,cwzwt8y,2,t5_2r3gv
5692448,0,t1_cx2oqly,Their reputability among people who know what theyre talking about is next to nil. Its true that a lot of Silicon Valley suits are in love with them and their spiritual successor in the vaporware business Vicarious.,MachineLearning,cx2pc9g,28,t5_2r3gv
5697754,0,t3_3t61ki,Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies by Nishimoto Vu Naselaris Benjamini Yu and Gallant.Youtube video demonstrationThe quality is impressive but not enough to really know what you are seeing.  It also requires an fMRI machine which you cant carry around on your head.,MachineLearning,cx3d0mv,8,t5_2r3gv
5702629,0,t1_cx3x2jg,You can interpret DRAW as a feedforward model. Instead of having one selective attention unit reading and writing over  steps why cant you have  different selective attention units just reading and writing in one timestep. Now under this interpretation DRAW is no longer doing what you call iterative inference.I see attention mechanism in DRAW as an interesting new way of learning representations instead of doing convdeconv.,MachineLearning,cx3yt7y,2,t5_2r3gv
5706161,0,t1_cx2c5g4,Hi thanks for the reply. How would I incorporate features such as punctuation occurrences smileys etc into my word representations? ,MachineLearning,cx4ekvs,1,t5_2r3gv
5717053,0,t1_cx5px1e,I dont buy this. All the learning theory in the world didnt help predict the success of SVM or the deep learning revolution. All real progress in this field comes from empirical results not theory.But overfitting is like the nd biggest idea in empirical evaluation sounds like those people were just idiots.,MachineLearning,cx5r8pg,8,t5_2r3gv
5724773,0,t1_cx6gg9p,Great decision for us breathing folk who no longer need to walk through a smoky casino every day though.,MachineLearning,cx6pq2i,3,t5_2r3gv
5729150,0,t3_3sbzia,It sounds like your on the right path though you have a way to go.I thought those mario videos were pretty cool too though they are not great examples of a robust mario player. They just massively overfit to one specific scenario. If anything about that level changes or if you try a different level it fails.Unfortunately I dont think you are going to be able to get very far without a solid understanding of the math and programming. ML is pretty much just programming mathematical models to do things so you cannot cut any corners there. I also did the codecademy python course which was fine to learn python but I already had a good basis for programming. You will just need to keep with it. You might want to take some linear algebra and calc from some online places as that will come in handy.Something else I like to do often is to check out the code for someone elses project and hack it to do different things. You get a good understanding of what they were doing and can try out some new ideas of your own.Oh and as far as cool videos I thought this one was pretty nice.Good luck and dont give up!,MachineLearning,cx7997z,2,t5_2r3gv
5734069,0,t1_cx71t0b,How do you manage the lack of numpad ?,MachineLearning,cx7v87s,1,t5_2r3gv
5753229,0,t1_cxa7u85,Unless you understand experimental method your models are only meaningful by chance. So no you cant. ,MachineLearning,cxa8rfm,4,t5_2r3gv
5761381,0,t1_cxb4sil,deleted,MachineLearning,cxb942e,1,t5_2r3gv
5772479,0,t1_cxcgde1,Both are continuous. Neither are differentiable at x but thats not terribly important.,MachineLearning,cxcmoe8,7,t5_2r3gv
5795467,0,t3_3uju9s,If you are planning to use graphical models maybe you can have a look this ,MachineLearning,cxfhdl9,2,t5_2r3gv
5838118,0,t1_cxkrepk,Thanks!,MachineLearning,cxkwkbp,2,t5_2r3gv
5856454,0,t1_cxlmm6g,Fair enough the information in the link isnt presented in the optimal way for sure.,MachineLearning,cxnb5gd,2,t5_2r3gv
5867073,0,t3_3vm0ws,Why dont you use SQL to make a program so that you can input your data into an excel spreadsheet,MachineLearning,cxopd4e,0,t5_2r3gv
5878485,0,t3_3vrx9v,Divide mnist digits into two halfs. Input is left half column by column output is right half column by column. Then if that works make it harder by doing a fixed random permutation of inputoutput. Then if that works do it pixel by pixel. Cred to these guys ,MachineLearning,cxq79gt,2,t5_2r3gv
5895734,0,t1_cxr36zg,Actually its a giant pain in the rear  let me know if you want to collaborate!  Ive been trying it with Keras.,MachineLearning,cxsgqt1,1,t5_2r3gv
5916994,0,t3_3sbzia,I have actually taken a hard core machine learning long time ago which I barely passed and not remembering much about and I find Andrew Ngs course is very reasonably structured. He is fairly honest about that you dont have to know or remember certain aspect of what he shows you for instance calculus used in the equations.I am right now three weeks into the course and even math is not my most strongest part I start seeing a lot aha! moment out of me. Yes math he demonstrates is intimidating but things get easier for instance that funky sign sigma LOL is nothing more than a math speak for loop in programming and Octave is fairly simple language the course include some tutorial on it too. and like I said he seems to be good about giving you a shortcut for some of harder math concept.I would strongly recommend keep trying and at least get to week  assignment if you havent gotten to. With course wiki you may be surprised how much you can apply what you learned even if you feel like you havent gotten them very well.Quizes are little harder as it tests you on the mathematical concepts more but given you will virtually have unlimited try its merely an instrument to find out how well you learned and I find it very useful myself.,MachineLearning,cxv8zep,2,t5_2r3gv
5917103,0,t1_cxuxdvr,deleted,MachineLearning,cxv9hxm,1,t5_2r3gv
5949898,0,t1_cxwqnt8,Even in computer vision while everyone seems to jump on the lets apply deep learning to important problem X I think people are starting to have some healthy skepticism about what its actually doing. The vagueness of what visualizing features actually means plus the recent awareness that these methods can be tricked by reverse engineering pictures isnt killing the hype train but is at least slowing it down in academia and focusing more on the theory behind why it happens and works so well rather than figuring out little tweaks to the system that work empirically.,MachineLearning,cxzk8ip,1,t5_2r3gv
5963332,0,t1_cy1aw8o,It is so sad how right you are. This reads like a horror story,MachineLearning,cy1bntz,10,t5_2r3gv
5978632,0,t1_cy3bqwn,so what they do as you know is to generate a random input image then do forward prop calc the error of the Gram matrix wrt the Gram matrix of the original texture and back prop onto the input image.Youll get back variations of the original texture ie displacement and features moved around a bit but by and large the texture you get back after minimizing the loss function will not be a million miles from the original input texture.Whereas what we want is sometimes we might get some islands in the sea.  sometimes some desert. sometimes plains. sometimes rocky mountains etc.  I mean I think?,MachineLearning,cy3bvip,1,t5_2r3gv
5988261,0,t1_cy4cvzd,Do any of these contests have challenges that are more aimed at beginner to intermediate level programmers?,MachineLearning,cy4lc08,2,t5_2r3gv
6004749,0,t1_cy6qtkm,You can use coinbase in the US or a few other services that are similar. Fees are a lot lower than paypal. For the most part I just spend it online though with purse or directly on websites though. I think they are trying to be kinda futuristic in their marketing and thats why they use it I mean global artificial intelligence to predict stock market secret dataset covered by encryption well pay you in cryptocurrency sounds crazy and kinda cool heh. ,MachineLearning,cy6r4bq,3,t5_2r3gv
6008834,0,t1_cy79xzi,Thats an ambitious goal. I suggest you ask yourself where your energy is best spent relative to your natural ability and interests. The bottom line is this you dont need to understand low level machine parts to apply machine learning which is really just applied statistics to specific domain areas. Strictly speaking the computer is used only to implement what can already be done without a computer. If you want to focus your energy on the implementation  understand that it will take away energy from doing what you really want to do that is answer questions as they apply to various domains.,MachineLearning,cy7afcy,1,t5_2r3gv
6019057,0,t1_cy7uifc,I dont agree about the Bayesian LASSO shedding light on the LASSO. If anything it gives the wrong idea of what is going on. The concentration properties of the posterior of the Bayesian LASSO are completely out of sync with the traditional LASSO in the sparse setting. Strange things happen like the posterior mean and mode converging at different rates. The Bayesian LASSO sheds absolutely no light on why the LASSO works the way it does in sparse settings and it has provably suboptimal properties here which is the whole point of the LASSO to begin with. Ive heard several theoretical Bayesians basically say that the uncertainty quantification of the Bayesian LASSO is garbage. To me this reflects on the danger of using Bayesian justifications as substitutes for actually understanding the problem at hand. There are a lot of subtle and not to subtle properties of methods that get swept away by thinking in a fullyBayesian way. ,MachineLearning,cy8mgen,5,t5_2r3gv
6035230,0,t3_3y44yr,You may want to try out which is based on Lasagne Theano?I have written caffe and mxnet version of this network. But the training result was not as good as the paper stated so I am still trying to figure out why. If someone would like to help these could be released on the github too.,MachineLearning,cyaqevq,3,t5_2r3gv
6046850,0,t1_cybpnrf,RNNs the core of NPI are TuringComplete. So I think its a matter of capacity or training before they can sort perfectly. Its however a different datadriven approach for inducing algorithms that we are still beginning to understand.I think Yan Olliviers work is fantastic. It has limitations but its targeting important challenges.Thank you and I hope to see you again soon. ,MachineLearning,cyc8xdq,1,t5_2r3gv
6053950,0,t3_3yfgvr,This looks so s. I thought that the symbolic approach to cognition has fallen out of favor with the triumph of connectionism? This stuff was a dead end already a decade ago.,MachineLearning,cyd67vf,3,t5_2r3gv
6059802,0,t1_cyddqr0,Here is my experimentFirst of all I mannually create classification dataset using sklearn.datasets import makeclassification with  features and  samplesIn order to make the features more nonlinear I create some new features via making multiply substract exponential calculation between the existing features.As a result I got  features.I then take data preprocession which includes remove outlier items and normalization. And then the dataset was divied into trainset and testset which are both used for measure the imputation accuracy.I choose denoising autoencoder model containing  and  structures.The imputation quality was measured by feeding the restored data into already trained logistic regression model.As for my experiments the AE performs not good. In my guess What make this phenomenon are  dataset is not suitable nor complicate for AE  I found many paper use genetic algortihm for training AE but I choose gradien descent this may make its performance worse  the data visualization shows that the gray scale image of the restored dataset is lighter than origin dataset.The thought about the conclusions are just my guess Im not sure which ones are right or all of them are wrong. Let me show some results belowwith LR trained by origin traindatatraindata na fill with zeros error.traindata na fill with means error.traindata na fill with AE error.testdata na fill with zeros error.testdata na fill with means error.testdata na fill with AE error.LR with normed origin data.LR with normed corrupted origin datazero fill.LR with normed corrupted origin data mean fill.LR with normed restored data AE fill.LR with normed test data.LR with normed corrupted test datazero fill.LR with normed corrupted test datamean fill .LR with normed restored test data AE fill.Because the input corruption in the training phase the restored data was generaly higher than origin data.In order to check whether the restored data could improve the classification accuracy I fit the restored traindata into logistic regression model.And the result is belowwith LR trained by origin traindatatraindata na fill with zeros error.traindata na fill with means error.traindata na fill with AE error.testdata na fill with zeros error.testdata na fill with means error.testdata na fill with AE error.LR with normed origin data.LR with normed origin data.LR with normed corrupted origin datazero fill.LR with normed corrupted origin data mean fill.LR with normed restored data AE fill.LR with normed test data.LR with normed corrupted test datazero fill.LR with normed corrupted test datamean fill .,MachineLearning,cydxojc,2,t5_2r3gv
6071769,0,t1_cyfhtgb,And yet here we are.,MachineLearning,cyfhvyz,3,t5_2r3gv
6077419,0,t1_cyg7z7w,gt yes educating yourself is your own responsibility.I dont share your every man for himself view of the world.gt and this is not a subreddit for helping beginners.Which doesnt mean that it cant also help beginners in a controlled way.,MachineLearning,cyg8fn1,3,t5_2r3gv
6091143,0,t1_cyhzg6b,Would I be able to do a real time simulation of Pong using Amazon web services? ,MachineLearning,cyi0008,0,t5_2r3gv
6091826,0,t3_3yzgt6,I like this.  I like this a lot.  ,MachineLearning,cyi2sqq,7,t5_2r3gv
6100539,0,t3_3z3rpl,Batch normalization is arguably the most important new technique for CNN training since  IMO. A lot of the rest are subfield specific e.g.Techniques Batch Normalization Better SGD methods RMSProp ADAM ... STNsNetwork Architectures GoogLeNet VGG MSRA ResNet very newObject Detection FastFasterRCNNUnsupervised Learning GAN Ladder Networks,MachineLearning,cyj2i60,6,t5_2r3gv
6103198,0,t1_cyg7z7w,Whats your definition of educating yourself? Going into google and and finding out how to do it? Well coming on reddit is just another resource for him to be able to learn it. If I was to explain it to him here it would be no different from him finding an explanation elsewhere. This is just another resource for learning in my opinion. If we dont help each other out then how do we advance and help out the next brightest machine learning experts we all start somewhere so please quit being an ass.,MachineLearning,cyjddqm,1,t5_2r3gv
6122237,0,t3_3zb4ne,But why would you apply any dimensionality reduction or PCA before feeding RF?Keep in mind that RF can deal with highdimensional data easily as they select a random set of attributes for each induced tree weak learner so they do harvest the advantage of high variability.Anyways I am very interested in knowing how the performance changes!,MachineLearning,cylj9ay,2,t5_2r3gv
6131034,0,t3_3zig60,Thanks for the link.  I read the first chapter looks good.,MachineLearning,cymj9e0,4,t5_2r3gv
6132072,0,t3_3zet6i,Now it can be made into a scavenger hunt game.,MachineLearning,cymni38,1,t5_2r3gv
6143456,0,t1_cynqhwo,Indeed the true Solomonoff prior is non computable. You can try to approximate it but I dont think its very practical at least at the moment.,MachineLearning,cyny2z3,2,t5_2r3gv
6152028,0,t1_cyovehb,Cool thats sort of what I was guessing. I do a parse with linkgrammar and then have some handmade semantic parsing rules on top of that.,MachineLearning,cyox6wv,2,t5_2r3gv
6156979,0,t3_3zw4p6,This is great list I would add the following two time series dataset for classificationclustering from UCR wikipedia dumps for nlp,MachineLearning,cyphftt,1,t5_2r3gv
6157842,0,t1_cyoo6ev,Your welcome. Let me know if you get stuck. When I started I found ironing but the issues hard than actually the implementation of the machine learning code. ,MachineLearning,cypkzpa,1,t5_2r3gv
6161456,0,t3_3zvuge,Im very skeptical about how much structure the homomorphic encryption preserves. As far as I know learning algorithms are typically sensitive to input features the quality of which affects the classification performance directly.So my guess is that doing machine learning on the unencrypted dataset will probably yield a much better classifier than the one trained on the encrypted dataset. I wonder if anyone inside Numer.ai has experimented and compared results on both datasets?,MachineLearning,cypzspp,4,t5_2r3gv
6171424,0,t1_cyqyvv1,So far FP only for storage for compute they are converted to FP is implemented in Maxwell with full FP coming in Pascal.,MachineLearning,cyr4kxs,1,t5_2r3gv
6175020,0,t3_404r9m,deleted,MachineLearning,cyrjb7h,-7,t5_2r3gv
6177789,0,t3_404r9m,How important do you think cooperation between nationstates is to making AI. safe?. beneficial? In particular is it critical for either or both objectives?How much power ultimately will the scientific community vs. other actors have in determining the impact of AI?,MachineLearning,cyrumja,5,t5_2r3gv
6179018,0,t1_cyrilj5,Cortex has roughly  functionallyanatomically distinct layers but the functional network depth is far higher.The cortex is modular with modules forming hierarchical pathways.  The full module network for even the fast path of vision may involve around  modules each of which is  layered.  So you are looking at around  layers not .Furthermore this may be an underestimate because there could be further circuit level depth subdivision within cortical layers.We can arrive at a more robust bound in the other direction by noticing that the minimum delaylatency between neurons is about  ms and fast mode recognition takes around  ms.  So in the fastest recognition mode HVS human visual system uses a functional network with depth between say  and .However HVS is also recurrent and can spend more time on more complex tasks as needed so the functional equivalent depth when a human spends say  second evaluating an image is potentially much higher.,MachineLearning,cyrznho,15,t5_2r3gv
6186506,0,t3_40axfn,I knew Matlab gradient descent principles and other things. The first assignment was hard as I had to understand the structure. Then it was not so hard.,MachineLearning,cysuafh,2,t5_2r3gv
6196001,0,t3_40dv5v,This is no different from any other academic field but is also the reason why abstracts exist. Read the abstract then either read bookmark or forget.,MachineLearning,cytx3y8,2,t5_2r3gv
6207296,0,t1_cyuz2zk,Its in your hands.,MachineLearning,cyv7bdl,4,t5_2r3gv
6232334,0,t1_cyy00qc,Well its not like games arent good ways to test out the perception and control learners on the research frontier right now.  Its just that many games are also solvable by GOFAI algorithms so we sort of know that the optimal playstructure at which were aiming the learner isnt actually that complex and that a bruteforce strategy will actually perform well.Which is very unlike realworld tasks where we want good learners precisely because bruteforce and analytical methods dont work.,MachineLearning,cyy1r67,1,t5_2r3gv
6248309,0,t1_cyxpq56,In Godel Escher Bach in the s in a chapter AI Prospects. Its presented as his personal guess or opinion.,MachineLearning,cyzv5vb,1,t5_2r3gv
6267680,0,t1_cz1r0zi,gt RFCEr what the hell are these?,MachineLearning,cz22ddf,6,t5_2r3gv
6282877,0,t1_cz3amac,If you want to build something on yourself  to compete with EC g. . Checkout these links Building a Deep Learning Dream Machine . This one is great but note the dream word there which means you can go cheaper than that and have a very powerful build as well  old z architecture with a couple of  TI will humiliate EC instances ..  Hardware Guide Neural Networks on GPUs  this one is very good as wellLuck! ,MachineLearning,cz3sjem,2,t5_2r3gv
6324709,0,t1_cz7xxm2,shh.,MachineLearning,cz8ju2i,0,t5_2r3gv
6327780,0,t3_428cqy,,MachineLearning,cz8wdwu,3,t5_2r3gv
6333689,0,t3_42cxi2,What is preventing you from bringing the robot online?,MachineLearning,cz9kkd6,2,t5_2r3gv
6337353,0,t1_cz9usad,Actually I just read this blog post that claims to be able to perform image classification on the deep belief image sdk using the gpu.  He attached code!,MachineLearning,cz9zjd9,2,t5_2r3gv
6341629,0,t3_42i2gr,Bag of ngrams! That way youll capture things like not good correctly.Pay attention to how you smooth the model though because with bag of ngrams you will start seeing examples in the test data that youve never encountered in the test data.,MachineLearning,czah0yw,1,t5_2r3gv
6357787,0,t1_czbqpwx,There is some work on solving ODEs with GPs. And you might want to check out submodularity for machine learning.,MachineLearning,czcb6c5,2,t5_2r3gv
6401096,0,t1_czh5829,Havent done anything yet. I do have a BIdashboard I whipped up in Excel that gives me a statistical breakdown of a bunch of metrics averages by region i apply to jobs all over the US etc ...I have had quite a bit of phone interviews but when I used ML at my last job for direct marketing at an insurance company I found that the advantage of ML was just cutting out chaff who wouldnt buy our products and not so much getting more people to buy. I think ive applied to about  jobs since October and actually just got an offer on a short term contract. the problem is..having the data points to be predicted are whats important. If ive only had  phone interviews and  applications id have to cut down the negative class in the data set quite a bit and it wouldnt be a good model IMO. ,MachineLearning,czh8mzw,0,t5_2r3gv
6419773,0,t1_czjb2pa,Fair point re impact of time controls I haventa clue either. And def Im pretty confident the team has a lot of lowhanging fruit to pick in order to improve this system. Like I was just commenting on rbaduk its mindblowing to me that this system was never exposed to a single pro game in training at all which facebook used exclusively and got SOTA results or that they had a fairly substantial dataset used in training just one component  fast rollout  but also not used for training move prediction at all. Even though they state even small improvements on move prediction netted them significant improvements in play strength. Or that they used only  day of training and . million games for reinforcement learning from selfplay. Getting to amateur d rank. Used that to produce a dataset of  million games !!! to train position evaluation on picking just  positions from each and throwing the other  away for using all moves lead to overfitting. Can they choose  positons and still avoid overfitting increasing the dataset by an order of magnitude? Can they maybe bootstrap from AlphaGo they have now and create a dataset of  million positions from games played  elo points stronger than their puny dan player and learn positional evaluation on that? etc.,MachineLearning,czjcabt,2,t5_2r3gv
6453246,0,t3_43z5yk,Honestly after seeing tonights matches and your predictions I think this approach is too simple for correct predictions let alone for being profitable. Then again if it was so simple to win at betting everyone would do this. However tweaking and improving your model can be a fun and educational experience.,MachineLearning,czmt2tl,0,t5_2r3gv
6462379,0,t1_cznam7u,There is a slack already created  you need to get invited ask francisco to get you added.  Im making an effort to go through this course   Ill love to online meet up or IRC on this  I am in the chicago area if anyone is interested in a meetup on this.,MachineLearning,cznr5bi,3,t5_2r3gv
6464919,0,t1_czf7mrl,Isnt the problem rather centered around what ArneNx described? regarding the activation functions.,MachineLearning,czo0mz9,1,t5_2r3gv
6474586,0,t1_czp0lq6,Thank you a lot so I should see it going up and down sometimes? but not too often?,MachineLearning,czp0o9d,1,t5_2r3gv
6475111,0,t3_44arz9,Also take a look at Generalized Expectation and Posterior RegularizationThese let you regularize your learning andor inference so that things like XX of output labels are FOO in expectation.These are sort of soft marginal formulations rather than hard constraints but that can be a lot more tractable. Also they sometimes provide a variational distribution whose parameters can then be used for MAP inference.Learning from measurements and constraint driven learning are also in this same vein.Out of those I think that Posterior Regularization is the only one that is used much for regularizing inference rather than learning. We also had some relevant work last year in doing inference with global functions of expectationsEdit Also you can apply a lot of this domain knowledge in the MAP inference regime using Dual Decomposition e.g. ,MachineLearning,czp2ms3,2,t5_2r3gv
6486031,0,t1_czmx0dg,Hi.. do they talk about Method of Moments?,MachineLearning,czq7cfo,1,t5_2r3gv
6521839,0,t3_44u9nb,Why use the weird hybrid loss rather than simply marginalizing the logistic regression loss from SGNS? You have the counts to determine frequency of positivenegative class so marginalization would be trivial and the LR loss doesnt have any problem handling infinite residuals. This point also bothered me in the SGNS as matrix factorization paper from Levy et al.,MachineLearning,cztwr6w,2,t5_2r3gv
6527065,0,t1_czu3hu4,gt Saddle points points that are almost extrema but whos gradient is nonzero in at least one directionThis is not what saddle points are. Saddle points are points where gradient is exactly zero such points are called stationary or critical but if you move in different directions from this point sometimes you increase the loss and sometimes itll go down. They are called saddle because surface of saddles thing for riding horses look this way they increase in one direction along horses body and decrease in another.,MachineLearning,czug8e9,4,t5_2r3gv
6552911,0,t3_45aeui,Theres also the computational linguistics approach with WordNet and a word sense disambiguation algorithm there are lots of them using semantic similarity indicators eg Resnik....,MachineLearning,czx4hz8,1,t5_2r3gv
6554755,0,t1_czx2id5,Set the controls for the heart of the RNN.,MachineLearning,czxbcl2,11,t5_2r3gv
6573046,0,t3_45llog,GJOpen which isnt a prediction market but comes out of Tetlocks Superforecaster work has AlphaGo at  to win Theres a lot of noise in the predictions though. Im not at all convinced that there is any theoretical reason we should believe either forecast.Prediction Markets are supposed to uncover hidden data by allowing those who know more about the field to make money. Unless some people from DeepMind are hoping to make a little on the side I dont see any basis to think that is a methodology that is doing anything more than counting guesses.The Superforecaster methodology could work better here. But I think that the GJOpen site UI has some misfeatures that mean the knowledge of people who know what they are talking about isnt propagated. What is needed is for truly world class Go players and Machine Learning researchers to try to agree on a probability distribution of outcomes. ,MachineLearning,czz7h75,8,t5_2r3gv
6581131,0,t1_d001h5i,True I guess it depends on how bespoke your task is. If its sufficiently general you could just take a pretrained GoogLeNet or GloVe vectorization off the shelf and make tweaks as you go but if youre doing something that requires you to train from scratch you still need lots of data to train a deep net more than you would if you had a more carefully engineered method.,MachineLearning,d001joi,3,t5_2r3gv
6581565,0,t1_czynick,This sounds great because I was getting pushed in too many directions.  I can translate any examples in Oracle PLSQL and I work with a .NET developer who can run the program on that side.  If you knew of good resources where I could look at realworld examples or similar that would be great.  Im going to look for youtube videos on this because what I really need is an education in this.  Thanks!,MachineLearning,d0035wm,1,t5_2r3gv
6598241,0,t3_461w6e,I really dont understand how they can possibly rely on ML since they have absolutely no reason to believe that their training set is at all reliable. I mean Im no expert but theres a difference between sending an email based on a random forest and sending a missile youre always going to get false positives and with severe problems with the data input you wont be able to even quantify that reliably and each false positive is literally putting an innocent life at huge risk.  Seriously how do we make these agencies accountable? This is insane.,MachineLearning,d01t8p8,75,t5_2r3gv
6600887,0,t3_463go9,This is a pretty old diagram from . Heres one of the few source articles I could find with that image Both architectures seem similar where the primary difference appears to be feature engineering with a dash of PR marketing and buzzwords.,MachineLearning,d0233pw,3,t5_2r3gv
6614141,0,t1_d03bfca,In that paper they use policy gradients actorcritic not value iterationQlearning.,MachineLearning,d03gify,2,t5_2r3gv
6620393,0,t1_d0404s1,Is eh brilliant angle hes taken on the whole thing.,MachineLearning,d043th8,1,t5_2r3gv
6627563,0,t3_46fypg,Excellent competition for improving heart health and heart diagnosis through data science. Check out the competition at ,MachineLearning,d04uia2,1,t5_2r3gv
6641817,0,t1_d064gzz,I do not understand your third item as a consequence from this paper and the one they refer.My understanding of their last bullet yours  is Performance of the only last layer trained is a convex proxy of fully trainer so only optimize the last layer when picking an architecture.This strengthens the results of Saxe et al. who    Observed empirical correlation between the randomweight performance and the pretrainedfinetuned performance of any given network architecture.followed by    This method allows us to sidestep the timeconsuming learning process by evaluating candidate architectures using random weights as a proxy for learned weights yielding an orderofmagnitude speedup.No one said that only train the last one.edit typo and changing a some words in the beginning.,MachineLearning,d06bkxh,1,t5_2r3gv
6658276,0,t1_d080t6t,removed,MachineLearning,d080ug1,1,t5_2r3gv
6668231,0,t3_46z8qa,Lines are visible but the towers are almost unmistakeable e.g. this image hope the link works. Perhaps you could train to recognize towers and condition line recognition upon being parallel to and nearby piecewise linear segments between towers. Just a thought. I imagine creating the labelled dataset would be tedious.,MachineLearning,d091wp5,1,t5_2r3gv
6669724,0,t3_46zabh,Keras has a nice feature for this. I really never used but I think its ok.,MachineLearning,d097gc9,1,t5_2r3gv
6683634,0,t1_d09n5px,Ah! thank you for this explanation! I think what confused me was that I didnt connect that that taking the partial derivative of the cost function allows for the individual update of weights. In my head I was still imagining a global derivative.,MachineLearning,d0an9yx,1,t5_2r3gv
6684839,0,t1_d0arnqd,If you can just go with Microsofts Deep Residual nets its pretty much SOTA and the underlying idea isnt that crazy. ,MachineLearning,d0arrh8,1,t5_2r3gv
6695887,0,t3_479zb2,That is verry interesting,MachineLearning,d0bwv7u,2,t5_2r3gv
6722457,0,t1_d0enau2,Somewhat. It does a lot of things for you specifically automatic differentiation so backpropagation is done for you. It also knows several optimizations for said differentiation. That having been said you can get some really cool stuff done with it and in industry youd either use this Theano or Torch maybe MXNet or Caffe. So yes definitely check it out but dont take all the prepackaged stuff and start forgetting the math because ultimately youll be judged on how well you know the math and will definitely have to go into the guts of one of thes  routines to tweak something. ,MachineLearning,d0enrt4,12,t5_2r3gv
6724751,0,t3_47qegp,deleted,MachineLearning,d0ewbsj,-9,t5_2r3gv
6730705,0,t1_d0ejvv0,You can but you may not have to. It depends on how you configure your network. Im also working on a multilabel problem so I tested on both single multidimensional output function vs multiple output functions. I think I dont need it if the output space of multi labels i.e. number of different output vectors are not that big or the classifier with single output function is sparse enough. In my case there was no difference on the performance and Im not sure why there are too many errors on the dataset itself so that it was hard to analyse. EDIT more details. most of them are my opinions so think about it if its really true for you.At the output you can have a singletask classifier that predicts an Ndim vector. Or you might want N classifiers multitask networks each of which output predicts a single value. The latter corresponds to the OnevsRest classifier. This may be required if M the size of set Ndim output vectors in the dataset is large. In a singlelabel classification task there are only N output vectors. In a multilabel one there can be up to N different output vectors. Of course it depends on the dataset and in my case N and M and it doesnt make sense to expect the networks to learn  different output vector independently. It would be okay if the size of dataset is much more HUGE than M but still it would be less efficient in terms of everything.Multitask setting explicitly have different classifier for each dimension so theres no problem due to the multilabel anymore. But singletask networks also can be used if the classifier is forced to be sparse  so that it can be working as if its dealing with each dimension separately. Dropout would be one way to achieve this and there are other methods such as l weight penalisation I end up only using Dropout fyi as l penalty didnt seem to be effective. I think this is what ujfsantos said in the comment. The implementation is the same yes you have to loop through them and run multiple output functions. Check out multitask classification codes out there there are some codes in Keras using graph for example.,MachineLearning,d0fiiim,2,t5_2r3gv
6742603,0,t1_d0gqj9e,Infrastructure on AWS is costly thus we need to require registration in order to add premium features.,MachineLearning,d0gqqv1,21,t5_2r3gv
6747157,0,t1_d0h6zvw,They can easily afford it its more a matter of profit vs expense.,MachineLearning,d0h7nxr,-1,t5_2r3gv
6749911,0,t1_d0he3sj,you can file a patent up to one year after publication so Id take it seriously if I was trying to build a product around it. also their implementation deepart.io is streets ahead of anyone elses results...,MachineLearning,d0hhx4k,4,t5_2r3gv
6750207,0,t1_d0h7nxr,gt They can easily afford itI dont think you appreciate how heavy the computation for something like this is and how much cloud processing power is needed for deploying this to hundreds of millions of people. It takes anywhere between  minutes for a single low res image to kind of converge using a decent GPU you can use a CPU but the time to convergence will jump to  minutes. Now imagine millions of people demanding an image. The wait time for a single image for a single user will not be minutes but weeks  months and processing will cost millions of dollars every day even if you dedicated the whole of AWS only for this particular task.I understand where you are coming from it doesnt have to be realtime and it will be fun but no it just wont work right now no matter how you do it.,MachineLearning,d0hj0wr,4,t5_2r3gv
6772658,0,t1_d0jo61p,The setting you guys are talking about i.e. using the test set as part of your training process is typically referred to as transductive learning. E.g. see the Transductive SVM... There are probably many practical problems where this is a good idea.Some nice slides ,MachineLearning,d0jyo9k,2,t5_2r3gv
6773194,0,t1_d0jz6yb,Yes it doesnt create new data.  However by using regularization methods you should be able to combat this problem.  Essentially you are introducing sparsity.Deep learning isnt a perfect algorithm in the sense that there exists a more likely optimal algorithm but only with better convergence.  Deep learning schemes using maxout for example are in fact universal approximators meaning they can any function given enough parameters and training epochs.,MachineLearning,d0k0vhd,-1,t5_2r3gv
6777272,0,t1_d0kepcg,Ive thought about it more and maybe rich people should just give money for paper sponsorship.  A full sponsorship would include Naming algorithm after sponsor.  ShkrelliStyle A new algorithm for neural artwork.  Including sponsor in examples of generated  reconstructed images.  Adding sponsor as coauthor.  Listing sponsors name  University as bogus second institution i.e. Trump University.  Writing glowing praise for the sponsor in the acknowledgements section.  Only releasing source code privately to the sponsor.  Including obscure hyperparameters that make the word hard to reproduce  giving the sponsor an aura of uniqueness as he can post images on twitter that no one else can match.  Citing private correspondence with the sponsor on major advances often preempting published work.  For example citing Slim  for the discovery of deep belief networks.  ,MachineLearning,d0khk87,4,t5_2r3gv
6783375,0,t1_d0kkf6r,gtThe difference in cost is you get an nvidia engineer to come and help you get started. Its not just the parts. Its also the On boarding.Not in our experience. What you get is a box you install. Then you can call them for help but no one came on site when our org got one.,MachineLearning,d0l6jej,2,t5_2r3gv
6795135,0,t1_d0mfi7o,gt The description needs to be automatically verifiable too.I agree... but I think that thats not enough e.g.  Beware of bugs in the above code I have only proved it correct not tried it.   Donald Knuth,MachineLearning,d0miovo,2,t5_2r3gv
6803078,0,t1_d0nf1ew,Thanks for the tip but I am not looking for journals suggestions. I already know many. As you can read from my main post I have other doubts.,MachineLearning,d0nf7kc,0,t5_2r3gv
6803132,1,t1_d0n54zn,Yes. Itd be a guy from somewhere else. I never questioned the logic of the said statement. Just by the tone of it inferred it to be someone from IDSIA. Nice defence!!,MachineLearning,d0nffjf,0,t5_2r3gv
6805614,0,t1_d0mtn3n,As I remember it this is not a particularly practical algorithm due to its memory requirements. There was a followup paper by the second author but IIRC their approach had the same problem. A few colleagues were looking at making more practical versions with further approximations but I havent heard of anyone making significant progress. The truth is that SGD already does a fairly good job of repelling from saddle points the main useful thing I took away from this body of work was a good explanation of why Newtons method often seemed so hopeless with neural nets saddle points are attractors under Newton dynamics. This had been an empirical observation that I remember Yann LeCun ranting about in  and he probably had said it many times even further back that sophisticated second orderapproximate second order stuff sucked in practice compared to SGD  tricks. Its nice to have a theoretical explanation of why this might be the case.,MachineLearning,d0npm0n,1,t5_2r3gv
6815704,0,t3_493k0s,Very enlightening...,MachineLearning,d0ouvu5,2,t5_2r3gv
6830014,0,t3_49b4fd,The question is how far weve reached in truly modelling a human brain. Are we at that level yet where a machine can be trained on a few instances to learn new things just like a brain would do?,MachineLearning,d0qhdnq,2,t5_2r3gv
6830963,0,t3_49bpid,Although there are many things that annoy me in hypes I still think it is beneficial to AI research. For example the Watson cognitive API makes AI accessible to people that are educated in for instance web development. Are the provided services mind blowing frontier pushing technologies? No. But if more people use AI then more funding for AI becomes available. The best way of making a good impression is to offer those techniques which we know work well and are stabilized in research.,MachineLearning,d0ql98z,13,t5_2r3gv
6832822,0,t3_49civ5,Look up Sutskerver et als work on seqseq. You have an encoder and a decoder. Encoder takes in the input sentence word by word and forms some representation of the sentence in its hidden state. When the whole sentence is fed the final hidden state is taken and fed to the decoder. This decoder is expected to output the targets one by one. As you can imagine this allows for powerful Mapping between input sequences to output sequences regardless of the either sequence lengths. Also Bengios group also developed encoderdecoder architectures not only Sutskever and Co. ,MachineLearning,d0qsus7,6,t5_2r3gv
6832882,0,t1_d0myemx,Fuel by the authors of blocks is excellent for onthefly data prep. See ,MachineLearning,d0qt3mr,2,t5_2r3gv
6843445,0,t1_d0rxued,Now if you could sit here and look into the camera well begin the VoightKampff test ..,MachineLearning,d0s0dii,5,t5_2r3gv
6843990,0,t1_d0s1q3g,I am  familiar with the progression of neural networks in backgammon and the drastic impact it has had on expert play.   Many of the things that Go players take as true will likely be  impacted by even an reasonably strong neutral network based bot.   The change in game play between pre and post bot eras has been extremely entertaining in bg.,MachineLearning,d0s2luq,7,t5_2r3gv
6848007,0,t3_49k54u,Can someone recommend a textbook that deals with using machine learning in finance or marketing? Either in python or R.,MachineLearning,d0sj1eb,10,t5_2r3gv
6850510,0,t3_49lhf8,you might want to target the finance industry. they are more open to people without a strong tech background and a strong statsmodeling background. are you applying much to jobs? look on linkedin for jobs listing statistics and python or java or r as skills. most will not be entry level but throw in your resume anyway as they might have other openings they might want to consider you for. ,MachineLearning,d0st9uw,3,t5_2r3gv
6853561,0,t1_d0t5c5d,According to Wikipedia Go has  possible games while chess only has  so Go is a lot more complex.,MachineLearning,d0t5qzk,3,t5_2r3gv
6854394,0,t3_49k54u,Im working on building a custom dataset and wondering what the best approach is in terms of organising the data and which type of NN would be the best fit. The data is organised in groups where each item in the group is a vector and each item in the group receives a binary classification such that exactly  item in each group is  and the rest . The number of items in each group can vary from anywhere between  amp . Would an RNN fit here? I can see two ways that it might either each vector is treated as a sequence itself or I should pad each group with negative results e.g. all features having value  and the classification  so that the group size is fixed at  and then the sequence would be the group of vectors.Edit if the group was the sequence then the classifier output would need to be  labels one for each vectorrow?,MachineLearning,d0t958c,1,t5_2r3gv
6854794,0,t3_49nuch,So Im going to directly answer your question but I would encourage you to think about critically precisely what it means to have a distribution of labels applied to an image.Assuming you do want to do exactly what you described it should simply be a matter of modifying the label vector to include the labels you want.For example if you have  possible labels and you want to perform logistic regression this means you should have  output nodes each of which is typically bounded between  and  to represent the classifiers confidence in that label. If youre using a softmax output layer the outputs will be normalized so all the output nodes sum to  as well meaning that the output of the network can be thought of as a discrete probability distribution. Normally when training a logreg neural net when you evaluate the error given a single label you typically apply hot encoding meaning that the desired output of the neuron corresponding to the correct label is  and the desired outputs of all the others is . Evaluate the error metric however you like based on that and backprop the error through the system to update weights.To do what you want simply change the desired outputs of each of the output layers nodes to correspond to your labels.Normallypredictedlabel  . . . . desiredlabel      .For what you want to dodesiredlabel  . . .  .Does that make sense?To reiterate however I would encourage you to think about exactly what it means to design a classifier to output probabilities based on these labels. The output represents the classifiers confidence in its labelyou may not get exactly the results you expect if you simply apply a vanilla neural netconvnet with this labeling technique.,MachineLearning,d0tas0w,2,t5_2r3gv
6856791,0,t1_d0t7dhl,I think the person you replied knows the idea behind komi. They were simply questioning how it was derived and whether it is truly set at the fairest value.,MachineLearning,d0tixwj,12,t5_2r3gv
6860554,0,t1_d0twg37,Yeah. Nothing has changed there since the s. Just some new GPUs made everything suddenly magical.,MachineLearning,d0tybgy,1,t5_2r3gv
6862227,0,t3_49q0fx,Question Is he calling RNNLSTMs very deep nets because they appear so after unfolding?If yes thats still different from a regular deep NN because in the case of RNNLSTM the input node receives the next input as soon as the first input crosses the first layer and so on similarly depending on the design the first output is available before all the inputs are fed in. Whereas in regular deep NNs one input one example is processed through all the layers and the weights updated and only then the next input next example is fed.Can anyone expand on that?,MachineLearning,d0u5626,6,t5_2r3gv
6864270,0,t3_49k54u,I am learning about deep learning specifically CNNs and how it typically requires an awful lot of data to prevent overfitting. However I have also been told that the higher capacity  more parameters a model has the more data is required to prevent overfitting. Therefore my question is Why can you not just reduce the number of layers  nodes per layer in a deep neural network and make it work with a smaller amount of data? Is there a fundamental minimum number of parameters that a neural network requires until it kicks in?,MachineLearning,d0udiqp,1,t5_2r3gv
6864583,0,t1_d0uenwt,Gross. ,MachineLearning,d0uesvc,12,t5_2r3gv
6865537,0,t1_d0ucoiz,It doesnt seem to be the pairing. Left guy has nothing to say. He parrots the other guy or says completely meaningless things. He could at least keep their board caught up. ,MachineLearning,d0uip54,1,t5_2r3gv
6865808,0,t1_d0ujh1u,Did you watch the first match? He explained some more basic things in that match.,MachineLearning,d0ujsvy,9,t5_2r3gv
6865926,0,t1_d0uj8lk,Did he really say Im going to do my best to win atleast one game.?,MachineLearning,d0uka6y,34,t5_2r3gv
6866444,0,t1_d0uk7aw,cat,MachineLearning,d0umeb3,1,t5_2r3gv
6876392,0,t1_d0v45z1,Danish. I guess its apt for a computer What makes it hard for newcomers to learn is that  there are no general pronunciation rules  instead each word has its own pronunciation that you need to remember. However the grammar is simpler than English.I think it would be easy enough to get sample text of dyslexic people  the other part is worse because who would admit it?Being new to ML how much sample text would be needed?Great answer by the way ,MachineLearning,d0vr3r5,1,t5_2r3gv
6888368,0,t1_d0x31w5,Yeah but statements likegt So I am one of few people in the world who understands the challenge and benefits of using machine learning in computer chess go and similar board games.andgt ...they exaggerate the role of deep learning. One hidden layer should suffice.show that youre one of the many people in the world whose commentary isnt exactly that valuable for the topic at hand.Machine Learning for board games is an old trick and well published. The distinguishing feature of AlphaGo is that it actually works.,MachineLearning,d0x43kx,33,t5_2r3gv
6889949,0,t1_d0x3vlq,Seems like a very big jump not perfect information game way bigger screen area real timeIIRC StarCraft also has an APM limiter for bots?,MachineLearning,d0xakbd,3,t5_2r3gv
6890000,0,t1_d0xaky3,gtWe dont even know how human level intelligence works.Obviously we dont know how it works right now the point is to try to predict when we will learn.gtThose predictions are educated guesswork which may or may not coincide with reality.Of course.  Same way the people who were predicting computers beating Go champions  years from now based that prediction on educated guesswork and it turned out that prediction didnt coincide with reality.,MachineLearning,d0xartk,4,t5_2r3gv
6891999,0,t3_4a55vh,What Im a little confused about If I meancenter the training data X and normalize it by its standard deviation what will I do with test data X? Do I meancenter and normalize it by its own meanstd or do I save the meanstd of X to apply this processing to the test data?,MachineLearning,d0xiy2l,3,t5_2r3gv
6892907,1,t1_d0xk7bv,I think the value alignment problem is a big insight. But hes not an MLresearcher per se in that hes not working day to day on shortterm improvements to ML problems. Hes trying to solve larger problems that will only become relevant when ML tools are much more powerful. Its like asking what valuable insights environmentalists have provided to the coal mining industry. ,MachineLearning,d0xmnk6,1,t5_2r3gv
6893035,1,t1_d0xjkgh,I am just saying that his expertise isnt relevant. I have nothing against fanfic! It would be like a famous movie star commenting on AlphaGo.,MachineLearning,d0xn6e6,0,t5_2r3gv
6893224,1,t1_d0xmqwq,You dont need to spend any time in a philosophy department though Im not denying that in many cases it would help philosophy does seem to have lost its way in many corners of the world. This guy does it with zero academic qualifications in anything ever. Just the mighty golden shovel.,MachineLearning,d0xny6b,4,t5_2r3gv
6893291,0,t3_4a60vh,Unfortunately its too late.Successful machine learning researchers are identified in elementary school machine learning competitions. Only the most creative innovative and gifted students are selected. If you were never aware of the process then it means that you failed in the secret initial qualifiers and werent even close to earning a place in the program.This process may sound harsh but it would simply be cruel to try to train someone in the art of machine learning if they dont possess the raw talent.,MachineLearning,d0xo81d,19,t5_2r3gv
6896697,0,t1_d0xyxqa,Selfdriving cars are not solved. Thats hype.,MachineLearning,d0y24y9,1,t5_2r3gv
6918330,0,t1_d10h2y6,AlphaGo did not have access to Lee Sedols games. It was trained on amatuer dan level online games then improved drastically by playing against itself millions of times. Even if AlphaGo did have Lee Sedols games it still wouldnt be able to adjust its playing for him  those games would be a couple dozen in a couple million that it trained from.,MachineLearning,d10iky4,15,t5_2r3gv
6935815,0,t1_d1286b7,I agree it would not work as well as with original inputs but its doableBasically what we need is some sort of lowlevel imagevec encoder just to skip the time needed to learn edges and shapes etcIn any case this is would be the first step towards whats the biggest advantage of AI over humans copypaste share of knowledge. While each of us needs to do our minimum  years or so in school AI in theory should only need to do it once.The next challenge after that and a great thesis for any PhD student out there merging two networks into one without loss of learned information from both.,MachineLearning,d12i2cs,1,t5_2r3gv
6953197,0,t1_d14cyqh,Thanks. I didnt go into papers because naming a spreadsheet software ExcelNet made me close tab immediately.,MachineLearning,d14h6q7,1,t5_2r3gv
6962134,0,t1_d155ena,gtYou missed a very important part of the explanation design the robotic AI to have similar base drives an embodied experience and concept formation algorithms similar to humans.Psychopaths have similar base drives an embodied experience and concept formation algorithms.  And yet I would not want to raise a superintelligent psychopath.  Even if raised in a loving environment its still going to be a psychopath.  You could reset to prior states but how do you know the psychopath isnt just deceiving you?The issue is that even achieving the level of commonality of a psychopath through code is a quite difficult goal.  Insofar as it seems easy its due to anthropomorphization.Think of it this way...gtIf a superior alien civilisation sent us a message saying Well arrive in a few decades would we just reply OK call us when you get here  well leave the lights on? Probably not  but this is more or less what is happening with AI. Although we are facing potentially the best or worst thing to happen to humanity in history little serious research is devoted to these issues outside nonprofit institutes such as the Cambridge Centre for the Study of Existential Risk the Future of Humanity Institute the Machine Intelligence Research Institute and the Future of Life Institute.From a piece by Stuart Russell and Stephen Hawking.An alien civilization is a a different species and b a different culture.  So it could have totally different values.  But its still a sentient being.  Lets say members of this species are x as intelligent humans.  So say you take a baby alien and raise it as though its a human... are you willing to bet the human race on this plan working out?  This seems incredibly naive to me.Yes in theory you could try to program your AI to make it think as much like a human as possible... in practice this is very difficult because there are a lot of species  culture assumptions that you have baked in to your brain that you arent very aware of because theyre like the water you swim in.If this is the route you want to take a much more realistic plan is to scan and upload the brain of an actual human.  That way you get a much closer copy and you also dont need to figure out as much since its possible to simulate a brain without fully understanding how it works.,MachineLearning,d15hs3v,2,t5_2r3gv
6985296,0,t1_d17j0k0,The knock is about burying the lede,MachineLearning,d184gdl,2,t5_2r3gv
6992811,0,t1_d18d0a7,Considering they would have access to tons of pro replays and know who was playing whom...Meta would not be a problem. They would outmeta humans before long. Its only a matter of time before this makes it into eports  moneydoto instead of moneyball etc. ,MachineLearning,d18z7dy,1,t5_2r3gv
6993243,0,t1_d190nbh,gtI wasnt suggesting we could actually arrest people based on this binary classificationOh no I didnt think that that was what you meant.gt  thought someone with extensive social network analysis experience would comment if this is feasible but I understand that the idea is pretty vague..There are no robust solutions out there but there has been some work in the direction that youre talking about. For example Magdy Darwish and Weber  investigated the potential for a model to predict ISISantiISIS sentiments on Twitter based on preISIS tweets. And one graduate Enghin Omer recently put out a thesis centered on the problem entitled Using machine learning to identify jihadist messages on Twitter.I have colleagues involved in the fight against ISIS in various capacities and the intelligence gathering is a mix of investigation and ML techniques. The idea is that if you can identify someone as part of a criminal or terrorist network you can use automated analysis techniques to help figure out how deep that network goes and who else is involved. But the initial discovery is often done by a human expert rather than a machine in part due to the huge volume of false positives.,MachineLearning,d190yz2,2,t5_2r3gv
6997078,0,t3_4bhmqb,I think the world definitely needs more machine learning in the biomed domain. Good luck!But please think twice before slapping on the blockchain on anything. It sounds cool but you must realize that the blockchain technology has huge overheads to get its key benefit decentralized security.  You need to do huge amount of computation to keep it running and you are able to store only relatively tiny amounts of data for that computation power.  Is the overhead worth the decentralization?  In some rare cases it is for example to pay for things.  But we havent actually discovered many examples of where the tradeoff is good.Maybe youd like to give way for people to be easily rewarded by bitcoins for mechanical turk style work.  Thats a great idea but can be entirely done by a fund maintained by a trusted third party that runs a website where the work is done and people donating to this third party or multiple third parties.  Not needing that trusted party is nice but is it worth the costs?,MachineLearning,d19gnbt,3,t5_2r3gv
7000301,0,t1_d19gzcb,gt The W power supply so I am guessing  GPUS? I would go for Intel K instead of the Intel .,MachineLearning,d19ttml,1,t5_2r3gv
7015714,0,t3_4bqeuh,Splitting is a nonsolution  no one will migrate why would they when this community is large and established.But OP is totally right about the problem. And the solution is simple  heavy moderation. Dont worry about creating the perfect set of rules just delete the garbage when you see it. No one will be upset except knownothings and they arent contributing anything worthwhile to the subreddit anyway.,MachineLearning,d1bkv6h,8,t5_2r3gv
7032196,0,t1_d1dev48,YMMV but I installed Ubuntu and nvidiadocker to play with all these GPU accelerated libraries and it was totally worth it.  Getting the diverse ecosystem running on Windows was a nitemare.  I just installed a new TB SSD just for Linux and use my bios to choose boot device.,MachineLearning,d1dgd3l,2,t5_2r3gv
7039251,0,t1_d1dp3eh,Id be interested in the paper too. I doubt Ill get around to it any time soon but it seems like an interesting angle to explore a week or two down the line.ThanksJohn,MachineLearning,d1e97n2,1,t5_2r3gv
7039294,0,t1_d1e8ve9,gt This link is truly an ELI Neat! I get a lot more about all these dots Ive seen so far now,MachineLearning,d1e9dyo,1,t5_2r3gv
7040586,0,t1_d1e9zua,non sequitur  fox and the grapes fallacy  your comment.If your comment took a form of a physical mechanism it would not have succeeded in its task when activated.,MachineLearning,d1eeo8p,11,t5_2r3gv
7048843,0,t1_d1f9uwp,You could make it online in the sense that you retrain the model based on new data periodically say every day or week. Essentially you are trying to model a probability distribution over the data and when you see any example that is unlikely according to your distribution its an outlier. To understand RBMs and similar models well I would suggest read a little about probabilistic graphical models. Its a good general framework for thinking about both generative and discriminative models. ,MachineLearning,d1fcehp,1,t5_2r3gv
7050043,0,t1_d1fciu8,Do we have any data on the amount of overhead Keras has over TheanoTensorflow?,MachineLearning,d1fhb0p,3,t5_2r3gv
7055332,0,t1_d1g1iae,Increasing the size of the beam will make the samples more accurate this will not make the samples more varied like you want. You need to sample uniformly from the probabilities not always taking the most probable as beam search does.,MachineLearning,d1g2xka,1,t5_2r3gv
7057082,0,t1_d1g8mpa,Thats referred to as the steady state probability and is calculated using the transaction matrix. It gives you the probability of being in each state given that you have let that chain run for a while which is sometimes called BurnIn. However a unique steady state distribution exists only if the chain is ergodic irreducible and aperiodic. Irreducible means that if it is possible to go from any state to all other sates state while aperiodic means that the transition could occur at every time slice d gt n. In other words each state can transition to any other state at any time. Theres more to it than just that but thats the short and sweet version.Edit Ive thought about you question a little more. If all you want is to define a probability of an event being in a particular state at any time than that would just be a random variable. However if what you want to do is find the probability of being in a state given you have already defined a Markov Chain then you need to calculate the steady state distribution.,MachineLearning,d1ga2lj,2,t5_2r3gv
7065188,0,t1_d1h6yy2,I dont think anyone has done it yet. Im trying to do an implementation now ,MachineLearning,d1h790f,2,t5_2r3gv
7066923,0,t1_d1hdz0b,Good catch.,MachineLearning,d1hecg1,1,t5_2r3gv
7087930,0,t1_d1j9tsk,Is it not a little ironic that researchers are busy trying to create neural turing machines that can work with memory and yet have completely ignored the SNN models where memory and task dependent memory routing just naturally arise out of the model?,MachineLearning,d1js9e3,1,t5_2r3gv
7104948,0,t1_d1lpccg,Well.. That is if youre okay with  accuracy the surprising false positives and false negatives outlined in these imagesThe XKCD comic  year estimate was referring to cleaning up the edge cases specifically these After computers learn to play calvinball better than the best human thats the signal for the leaders to make mandatory computer mental augmentation of all individuals.  There will need to be an effective marketplace of software thoughts and hardware and a Colosseum where the computers can teach us where we are weak and where they are strong so that necessary adaptations can be made to keep up.,MachineLearning,d1lpw73,49,t5_2r3gv
7106877,0,t1_d1f6ty7,Ill give myself a shot. ,MachineLearning,d1lxtpe,1,t5_2r3gv
7124035,0,t3_3vvdly,butWhoWasBee I agree with you this is a good question you could do exactly the same generation task with a FFN. Its actually quite simple to implement and test in Keras. I didnt do it so far though too lazy... The question is why does an RNN work better than a FFN for this task ? I guess its because in texts the correlation between one char and the other surrounding ones is strongly Markovian e.g. xt strongly depends on xt less on xt and so on back in the history. This particular pattern of correlation in such timeseries allow for a very efficient solution to share parameters RNNConversely FFN a priori assumes the same correlation between xtxt and xtxt. Of course with perfect training and infinite dataset they will learn in the end the real pattern of correlation. But in practice we as model designers have a prior knowledge about this actual correlation pattern and use this knowledge to design Markovian models such as the RNN others may also fit HMM CRF... which are best fitted for this task.harponen you say you wrote a RBM as an output layer ? Thats rather nonstandard afaik as RBM are usually used as hidden layers arent they ? Could you share more info about your design please ?,MachineLearning,d1nx531,1,t5_2r3gv
7129312,0,t1_d1o6zhq,gtedit ahh Im so bad at recognizing names... D,MachineLearning,d1oitlz,1,t5_2r3gv
7129988,0,t1_d1okefa,As RNNs can be unrolled to effectively very deep networks with tied weights skipping inputs randomly could have a similiar effect. E.g. skipping characters in an endtoend NLP problem or skipping spectrogram observations in speech processing problems.,MachineLearning,d1ollgq,2,t5_2r3gv
7140093,0,t1_d1pptmp,I would definitely use it... good stuff!,MachineLearning,d1pr2g4,1,t5_2r3gv
7148063,0,t1_d1qfh23,deleted,MachineLearning,d1qntdp,2,t5_2r3gv
7152357,0,t1_d1qzb8o,Yeah just to clarify This is a very small subset of the full leak.,MachineLearning,d1r5gr5,13,t5_2r3gv
7162173,1,t3_4dmblu,Unfortunately its too late as successful researchers are identified in elementary school or something,MachineLearning,d1s9qnq,3,t5_2r3gv
7165042,0,t1_d1slccp,Oh it still does take a while to learn! Something simple like snake doesnt really do anything useful for at least an hours worth of training. Its just a way to get the algorithm to converge to something useful. I guess you are managing it.I believe some of the DQN stuff from deepmind took a week or something crazy to converge fully.,MachineLearning,d1slhxn,1,t5_2r3gv
7169101,0,t1_d1qppbd,ML startups in india?like?,MachineLearning,d1t25kl,1,t5_2r3gv
7169886,0,t1_d1t4yye, features only used  for each tree?,MachineLearning,d1t5dbd,1,t5_2r3gv
7173886,0,t1_d1tkzeo,im curious where you get test data to learn for this,MachineLearning,d1tlqny,2,t5_2r3gv
7182396,0,t1_d1ugghv,FixedBTW I asked the author directly he did not say anything. If he tells me it is not okay I will take it down immediately.,MachineLearning,d1ukq8q,2,t5_2r3gv
7183502,0,t3_4dtbks,I havent tried tensorflow. But there are a known problem with theano scan it does not parallelize if there are no recurrence.Does tensorflow scan parallelize if no recurrence?,MachineLearning,d1up9c9,1,t5_2r3gv
7195607,0,t3_4dyf62,Can someone post up the homework questions so I can understand the context of the solutions provided??,MachineLearning,d1w2y00,2,t5_2r3gv
7196054,0,t3_4e0i4h,I lovingly accumulate them in a big pile.,MachineLearning,d1w4rzf,8,t5_2r3gv
7205068,0,t1_d1w2f9i,    if you have only one life        dont waste your time on things that just are nice like MatlabOctave    else        get to know all nice things,MachineLearning,d1x5q10,3,t5_2r3gv
7237820,0,t1_d20393o,Well you should have a decent idea given that you are working on billion layer nets.If I had to do it I would just katana fold them enough times. The recent trends in ML certainly corroborate that intuition.Res nets for example are something in between regular D feedforward nets and D nets and stochastic depth nets are similar as they can be interpreted as having probabilistic skip links which effectively increases their dimensionality.Stochastic depth nets in particular are a interesting interpretation of sparsity and might point the way how to achieve it in recurrent nets  LSTM and GRUes in particular are not sparse. In regular RNNs there is not much one can do in terms of dropping layers as the connectivity graph might become too severely bottlenecked or even disconnected but as one goes into higher dimensions and higher number of total layers it would become progressively safer to increase sparsity. This would be good as unlike in feedforward nets where sparsity is used as a regularizer and optimization helper here it would have the benefit of decreasing computational requirements.Sparse interconnectivity in high dimensional spaces could also explain puzzling in humans phenomena such as synesthesia. In fact I cannot think of any other reason for it.,MachineLearning,d20vyud,1,t5_2r3gv
7240173,0,t3_4elnqs,The system has converged on a solution to the training data not a solution to the overall problem represented by the test data. ,MachineLearning,d215l6f,2,t5_2r3gv
7241455,0,t1_d21a8ia,Cool thanks for all the info! Yes indeed I am repeatedly testing on all my test data every N iterations. I am playing with the Caffe framework and it doesnt seem to have an option to do validation. ,MachineLearning,d21au5j,1,t5_2r3gv
7249168,0,t1_d21r11e,By default we take control over the entire memory region on the GPU and then suballocate within it so the process looks like it uses all of the memory from the point of view of nvidiasmi.  Some benchmarks show nvidiasmi numbers we dont blame them we dont give them anything else to use yet so it looks like we use GiB of memory for say AlexNet when we actually use less than  GiB active in practice.The way to find out is to turn on the allowgrowth field in our ConfigProto.gpuoptions structure.  It leads to lower memory efficiency due to fragmentation which is why it is default off but is useful in a multitenant environment and gives you a sense of the actual memory needed until we plumb back the stats for that.,MachineLearning,d226gcq,2,t5_2r3gv
7252669,0,t1_d22jwym,Er no that doesnt work.  ,MachineLearning,d22krl7,2,t5_2r3gv
7255548,0,t3_4esdia,Why you should buy his book on Information Theory Inference and Learning Algorithms.,MachineLearning,d22wk18,23,t5_2r3gv
7273943,0,t3_4ezvhn,Guys remember ROKOS BASILISK IS LISTENING.,MachineLearning,d24zypg,6,t5_2r3gv
7314083,0,t1_d29hy23,deleted,MachineLearning,d29k859,1,t5_2r3gv
7345465,0,t1_d2ci3ly,GTX    cores  CUDA compute capability .  GB Currently working on a rocket jumping convnet running on the GPU.,MachineLearning,d2d4n4n,1,t5_2r3gv
7355018,0,t1_d2drjky,I guess what OP calls shuffling labels is basically a permutation test which is a nice way if lowpowered of checking whether the training procedure is actually learning something. Its a nice way of making sure you didnt make any big mistakes with e.g. trainingtest split.I once caught a mistake in my crossvalidation procedure using a permutation test. My samples were not independent but I didnt know that. The crossvalidation procedure was often splitting subsets of heavilycorrelated data into several folds. This led to an effect similar to peeking at test data during training. I thought I got a nice AUC. classifier but when a permutation test also resulted in an AUC. classifier I noticed the problem.,MachineLearning,d2e7r9o,2,t5_2r3gv
7357878,0,t1_d2e0m5l,Lets just all agree on it being the love of Tupic and his love for breasts.,MachineLearning,d2ejgsn,3,t5_2r3gv
7385150,0,t1_d2hfms2,Skflow has been integrated in the master branch of tensorflow.,MachineLearning,d2hn3fk,1,t5_2r3gv
7385925,0,t1_d2hme9t,As long as the kernels do basic math operations like matrix multiply I couldnt care really.Thats like someone complaining that the design of the transistors in a computer arent opensource.   I dont care as long as it serves a single well defined purpose and has fully defined inputs outputs and performance.As soon as people start designing more complex kernels Ill revisit my decision though.,MachineLearning,d2hq9dh,3,t5_2r3gv
7397883,0,t3_4gn77y,Tutorial on implementing an RNN from scratch in TF using scan ,MachineLearning,d2j39s2,2,t5_2r3gv
7421865,0,t1_d2lnyda,Kind of yes but not exactly. While Highway Networks are extension of gating mechanism we see in GRU and LSTM whereby a gate changes the potential path of the network. But as it turns out this has the same effect which learning a residue like in ResNet does i.e go deeper without getting into the problems of vanishing gradient. So far I have only seen application of highway networks in NLP tasks and ResNets in Vision task but I am sure the techniques are not exclusive to the field. From the Highway Networks paper  This is accomplished through an LSTMinspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation.From the ResNet paper  Instead of hoping each few stacked layers directly fit a desired underlying mapping we explicitly let these layers fit a residual mapping.So they are trying to solve same problem  how to train very deep networks but they do it differently.It is strongly believed that training deeper networks can be actually computationally cheaper and efficient. And there is some evidence that human cortex has similar layering structure for information pathway see thisHowever best way to know the difference and similarity is to see the code Resnet and HighwayYou might also be interested in skip layers see the repo for another file for that,MachineLearning,d2ltg3s,10,t5_2r3gv
7431820,0,t3_4h0r33,paper link hereReally nice to see a comparison to unitary RNNs as they have really good memory potential  especially unitary LSTMs  will comment more once I get a good read,MachineLearning,d2my8ip,1,t5_2r3gv
7450247,0,t3_4hcm3g,Artificial Intelligence is a bad name for Artificial Intelligence.  Computational intelligence is a bad name for computational Intelligence.  Machine learning is a good name for machine learning.    ,MachineLearning,d2p1obz,4,t5_2r3gv
7454879,0,t1_d2pkan6,Would you suggest that the paper be new or is it fine if its a bit dated? Also what would the advantages and disadvantages of this be compared to just reading the newest papers on arXiv?,MachineLearning,d2pkm9o,4,t5_2r3gv
7462486,0,t3_4hjof8,Using Overfit models aka how to win at Kaggle!,MachineLearning,d2qfso8,9,t5_2r3gv
7473254,0,t3_4hqwza,mirror?,MachineLearning,d2rnxqe,4,t5_2r3gv
7478551,0,t1_d2rpxil,Orrrrr... The Stanford machine learning class at Coursera is already closedcaptioned.I know because I just finished it and had that turned on because its sometimes hard to understand Andrew Ng.,MachineLearning,d2s9o45,4,t5_2r3gv
7483424,0,t1_d2sa3lr,They always try to work with the schools first. The schools already have processes in place to get videos captioned they just dont or dont communicate this well.Schools are EXTREMELY well aware of  compliance issues its embarrassing this slipped off their radar.A little ironically the technologies Karpathy is teaching will one day be used to very trivially caption these videos were just not quite there yet.But this isnt really a big deal the schools have dealt with this before they have the resources to handle it properly they just need to execute on their preexisting plans.,MachineLearning,d2sto1y,5,t5_2r3gv
7505823,0,t3_4i4j66,You definitely need to get rid of the sliding window approach.Check out these papersYOLOFaster RCNNEdit Also OverFeat,MachineLearning,d2vdgg1,24,t5_2r3gv
7507151,0,t3_4i67cm,Without going through everything to verify your work lets just take a moment to recognize how broken it is that we cant use TeX formatting for math equations and have to rely on ASCII just to write a gradient operator.,MachineLearning,d2viwfk,44,t5_2r3gv
7507770,0,t1_d2vk0lc,Id place them somewhere between RBM and stacked AE. See Bengios paper Greedy LayerWise Training of Deep Networks,MachineLearning,d2vlfw8,1,t5_2r3gv
7512047,0,t3_4i8dd2,Anyone has idea where we can buy it at that price?,MachineLearning,d2w2z5p,3,t5_2r3gv
7522839,0,t1_d2x3h97,that said its fucking expensive and if you dont already know about it I dont know how much youd get from attending.,MachineLearning,d2xb5td,2,t5_2r3gv
7526852,0,t1_d2xo27k,Where are these jobs?,MachineLearning,d2xrkzc,6,t5_2r3gv
7532774,0,t3_4ifz4v,A good question waiting for a good answer.,MachineLearning,d2yftr8,0,t5_2r3gv
7541840,0,t1_d2z8dt9,I actually got it to work just now! It was a really simple mistake on my part... but your response actually gave me the bump in the right direction! Thank you!,MachineLearning,d2zgyk6,0,t5_2r3gv
7554469,0,t1_d30uoin,The vast majority of people dont take calculus in high school including myself ass. Ive never been in a class to understand the chain rule. Youre doing the equivalent of telling someone theyre an idiot for not knowing French when theyve never taken a French class.  ,MachineLearning,d30wnyt,3,t5_2r3gv
7556505,0,t1_d30o7m0,Or TRON.,MachineLearning,d314zi8,1,t5_2r3gv
7560503,0,t3_4ivueg,also allows you to transfer learn weights from wordvec,MachineLearning,d31lcly,2,t5_2r3gv
7581492,0,t1_d33xlrg,I agree and it isnt productive either. But perception isnt a uniquely human concept. Maybe better said it is more of a biological concept than a non biological one. Biological perception as it relates to for e.g. plant behavior could transpose onto mathematical systems no? And inasmuch might offer up a richer account of perception than inputoutput? though I like the simplicity of this model to start.,MachineLearning,d33z8ly,1,t5_2r3gv
7587988,0,t3_4j5az7,Wow interesting post! Its an oldie but its gold.,MachineLearning,d34ptty,1,t5_2r3gv
7590234,0,t3_4j6neb,OP how does one  go from NLP or seqseq to APIs? Can you please point me to some reading on this?i.e. how does it know what API calls to use to answer Where can I get some good tacos nearby?On the whole quite impressive!,MachineLearning,d34z0iw,3,t5_2r3gv
7595051,0,t1_d35eyew,or RTFM,MachineLearning,d35ip0i,0,t5_2r3gv
7633250,0,t1_d39ubam,Formatted    ,MachineLearning,d39uxib,22,t5_2r3gv
7643382,0,t1_d3axi9d,Come on internet make this happen,MachineLearning,d3b0etb,1,t5_2r3gv
7658855,0,t1_d3crbn8,Puh I cant remember any paper right off the bat. The layers are often skipconnected so the network is also able to learn utilization of depth at least in theory.,MachineLearning,d3cro6r,1,t5_2r3gv
7660911,0,t1_d3cx5st,Ah I see so your data is not perfectly monotonic and it is not a lot of samples.  I do think that you have to define monotonicity then which means defining a measure.  One that comes to mind is like my counting idea but lets state it more formallygt A function has a high degree of monotonicity if the probability of increasing is high.From a frequential point of view the probability can be calculated as the count of increasing values over the total number of values.  For your first graph and switching increasing for decreasing in your case this would be    chance of decreasing for the yellow and orange curves.  Compared to    chance of decreasing for the blue curve.  I think this is not a bad measure of monotonicity.  Taking into account the number of samples in a manner similar to the ttest should be able to give you a measure of significance.On the other hand backing up for a second Im not sure monotonicity really matters here.  This is a correlation.  You have a strong correlation between your two axes.  Isnt reporting Pearsons r for plt. enough?,MachineLearning,d3d0360,1,t5_2r3gv
7662124,0,t3_4k82b5,It might be possible to test for nonmonotonicity using the Spearman correlation coefficient You may have to segment the data and then calculate the correlation on each segment.,MachineLearning,d3d51qg,1,t5_2r3gv
7662397,0,t3_4ibv66,In training a NN for binary classification is there any real difference between training a network with a single output node and interpreting the output as a probability versus training a network with two output nodes and applying softmax?  Is one approach more reliable than the other?  Why?,MachineLearning,d3d65xm,1,t5_2r3gv
7662682,0,t1_d3d65xm,Use single output node. The math works out the same and its easier to optimize.,MachineLearning,d3d7bwa,3,t5_2r3gv
7673703,0,t1_d3ee5kl,Thanks for the reply. As for the hosted version unless you offer free academic license since Im just a PhD student I wont be able to get anything.,MachineLearning,d3egdyj,1,t5_2r3gv
7674783,0,t1_d3ee8a8,This was my gut reaction to the title of Murphys book.  Machine Learning a probabilistic perspective. My reaction well is there any other perspective?. ,MachineLearning,d3eksrv,2,t5_2r3gv
7687573,0,t1_d3fm19v,Alright. Im working on something similar on an internship and Im at a similar level of proficiency. I was wondering whether itd be possible to work this out in  months.,MachineLearning,d3g11ok,1,t5_2r3gv
7693660,0,t1_d1m3rcf,Hey there! Did you manage to improve the model ? Hoping it was useful... Best Pascal,MachineLearning,d3gpwzo,1,t5_2r3gv
7700127,0,t3_4kq3jx,Are there any other conferences like NIPS that care about AI machine learning neuroscience and cognitive science so broadly?,MachineLearning,d3hgce3,3,t5_2r3gv
7714882,0,t1_d3j3tta,I like the idea of this kind of thing. Maybe rather than having the benchmarks be in papers which then get out of date very quickly itd be better to basically keep a centralized running record of the best performance crossreferencable between task and algorithm.I guess the issue is if you make an algorithm you probably want to be careful that someone doesnt submit a bad score for it because they used it incorrectly or something like that. But if lots of people submit scores and models that should generally selfcorrect I guess.,MachineLearning,d3j4nwy,2,t5_2r3gv
7738081,0,t3_4lac5r,Hack the NSA,MachineLearning,d3lrhxa,3,t5_2r3gv
7752500,0,t1_d3nefiu,Okay then I suppose we trust him ,MachineLearning,d3negog,5,t5_2r3gv
7852955,0,t1_d3yhrrf,This is great. Could you link the papers for the benchmarks themselves though?Also  and also TrecQA?,MachineLearning,d3yqqg1,3,t5_2r3gv
7871656,0,t3_4n4mbi,TensorFlow has an open source LSTM architecture that does this. However you will need copious amounts of data to generate anything meaningful most NLP papers use like B words. Heres the tutorial ,MachineLearning,d40uhdx,1,t5_2r3gv
7872803,0,t3_4n5ltj,deleted,MachineLearning,d40z4jq,1,t5_2r3gv
7899027,0,t1_d42urp5,gt WTF is this? Download a brain first.,MachineLearning,d43xakc,1,t5_2r3gv
7900341,0,t1_d442j6i,gt Maybe no one has ever made anything original.This would make an excellent Tshirt.,MachineLearning,d442mbj,3,t5_2r3gv
7928822,0,t1_d478yvw,Youre probably being downvoted because of the obvious factual error...  The Titan X has  GB while the GTX  has  GB.  This is  more memory rather than double .,MachineLearning,d479vsi,4,t5_2r3gv
7931694,0,t1_d46ryci,Sadly he passed away recently. He left some great work behind.,MachineLearning,d47ligi,1,t5_2r3gv
7934207,0,t1_d47r3o8,Everyone is in industry so theyre producing the applied research industry wants.,MachineLearning,d47voax,2,t5_2r3gv
7935962,0,t1_d47ogw6,Yeah. Thats exactly what happened and why I abandoned that approach. Not so surprisingly a quadratic fit gave me the best output.,MachineLearning,d482sbf,1,t5_2r3gv
7940896,0,t3_4o0zxg,Sounds a lot like a Do my work for me thread.,MachineLearning,d48mpwa,-12,t5_2r3gv
7941363,0,t3_4o17a1,relevant section from UFDLgtFor large images PCAZCA based whitening methods are impractical as the covariance matrix is too large. For these cases we defer to fwhitening methods. more details to comeAFAIK this is why people pretty much stopped doing it when things moved on from CIFARMNIST. And with the recent stuff like batchnorm and better random initializations I dont think it has much benefit any more even when computationally feasible.,MachineLearning,d48olwh,2,t5_2r3gv
7949663,0,t1_d49l9of,Fixed thanks.,MachineLearning,d49m8h4,2,t5_2r3gv
7952471,0,t1_d495dqo,I disagree I think any use of unlabeled data to improve generalisation  predictive performance constitutes semisupervised learning. Whether this is done through a modification of the loss function  model architecture or a modification to the training procedure e.g. training labeling then retraining on the larger set doesnt really make a difference it is still semisupervised learning.,MachineLearning,d49xlfs,2,t5_2r3gv
7968116,0,t3_4oczzy,Thank you will be checking these out.,MachineLearning,d4bowz4,1,t5_2r3gv
7989968,0,t3_4ompm9,Jeff Hawkins  What are the Hard Unsolved Problems in HTM?Answer. HTM needs some theoretical justification. HTM does not have it. . Comparison with modern AINN literature. Results on MNIST SVNH Machine Translation Speech Recognition...So tldr  they dont have a good theory and they dont have good results. I would start with theory with identifying why exactly HTMs have to be like that why cant it be in some other ways like Neural Networks and what advantages does it provide. ,MachineLearning,d4e5d5h,3,t5_2r3gv
7991282,0,t1_d4e4hf1,Is there a way to download reddit threads? I know there is an offline reddit app but it looks complicated.,MachineLearning,d4eao5p,1,t5_2r3gv
8004783,0,t1_d4fsai3,removed,MachineLearning,d4ftadk,-5,t5_2r3gv
8005158,0,t1_d4ftnkr,Oh my mistake. By AlphaGo I was thinking of a generic system like it in any context  not just Go.,MachineLearning,d4fusz8,1,t5_2r3gv
8021847,0,t1_d4hb8r1,Proof?,MachineLearning,d4hqbty,0,t5_2r3gv
8028326,0,t1_d4idtkj,I appreciate that you take feedback into account. Out of interest how long would you estimate it takes to implement something like this from scratch.,MachineLearning,d4igjaw,1,t5_2r3gv
8033417,0,t1_d4idnfb,Its not a pyramid scheme! Its a reverse funnel system,MachineLearning,d4j15u0,3,t5_2r3gv
8035671,0,t1_d4ilk7x,xeon phi is a game changer after all the hoops of GPGPU approaches. it  my prediction is that the most performant   hybrid super clusters will be built with phi and GPGPU on top of cheap scalable multicore intel,MachineLearning,d4ja9h6,-1,t5_2r3gv
8046049,0,t1_d4ke5e4,What was the source of the raw data?!,MachineLearning,d4kgaan,1,t5_2r3gv
8049000,0,t3_4pftvp,There are some networks pretrained on Places ,MachineLearning,d4ks79m,2,t5_2r3gv
8053095,0,t3_4pikyh,Itd be nice if these project ideas came with data sets or indications of how to get the data sets.,MachineLearning,d4l8s5v,1,t5_2r3gv
8103401,0,t1_d4q8skm,Agreed C is very common in realtime applications. This is why I am trying to avoid python or anything too far from CC. I cant speak for lua as Ive never tried it and cant speak of its speed and convenience compared to C. What I would be curious about is if lua is used in Torch for CV and other AI applications how easy is it to integrate lua with systems that use C specifically and never deal with lua. Not just smart phones but stuff like automation robotics advanced flight controls etc. These applications cant risk stalls or slow responses so going for the fastest languages out there like C seems to be imperative. ,MachineLearning,d4qw8pb,1,t5_2r3gv
8105057,0,t1_d4r2v4o,deleted,MachineLearning,d4r2xa9,-3,t5_2r3gv
8116633,0,t1_d4q5xq7,Alas I would never give my stock market predictions to Numerai unpaid  I would connect to a trading API and use my savings to play. My hunch is that anyone who was serious about fin market prediction would do the same. And if my model is showing simulated returns of    I have an information advantage in the market. I certainly gain nothing by giving Numerai my information advantage for nothing.,MachineLearning,d4sdsdf,1,t5_2r3gv
8130628,0,t1_d4txwct,Yes IBM ML contributions dont seem particularly impressive compared to what Big companies are doing. And also yes at a noname company. Slowly dying etc etc. ,MachineLearning,d4tyexj,1,t5_2r3gv
8131528,0,t3_4qm89y,Maybe a better question Do they publish in any IEEE venue?,MachineLearning,d4u2267,3,t5_2r3gv
8176798,0,t1_d4ywx08,Can you expand on the component collapse problem? Do you mean some latent variables stay around the prior and are useless while others truly learn some useful representation?This paper by DeepMind shows something like that happening on figure A  However they are using a reweighted VAE loss function so Im not sure this applies to the classical VAE.,MachineLearning,d4zne7c,5,t5_2r3gv
8189033,0,t1_d513vpq,The main question is who owns or should own the data  patient or NHS? In that regard anonymizing doesnt change anything.,MachineLearning,d516ini,2,t5_2r3gv
8194914,0,t3_4ri4fw,Theres another problem which has always bothered me but generally gets swept under the rub.  If a word can have two different cases there are no good ways to distinguish those cases.  For example case itself can mean either something to store things in or different meanings of words and the word embedding for those two hypothetical meanings should be vastly different.,MachineLearning,d51wzv7,1,t5_2r3gv
8199618,0,t1_d51rz6m,That is great insight. Thanks a lot for sharing.,MachineLearning,d52i861,1,t5_2r3gv
8202998,0,t1_d52wwoi,Definitely is.  I wish I could claim it as my own.,MachineLearning,d52xfu3,1,t5_2r3gv
8241482,0,t1_d57qksf,Yeah except for the bipartileness. I believe that DeepWalksItemvec model is degenerate by its nature for both items represented as bipartile graphs and graph themselves we have the exact structure. We only have a sample of some language structure in wordvec.,MachineLearning,d57qon7,3,t5_2r3gv
8251157,0,t1_d57071i,Super interesting research. I wonder with this detector running in realtime if it would make sense to provide a sort of speech therapy alongside other kinds of treatment? I can imagine hearing yourself sound less depressed being a very positive thing.Is there a survey anywhere of various speech characteristics associated with different psychological states? Your work investigates reduced vowel space but surely there must be other indicators for other states?,MachineLearning,d58y8ja,2,t5_2r3gv
8276597,0,t1_d5c3408,RNNs are commonly used in robotics for sequencers and I imagine they must be doing some sort of image recognition which is commonly done using CNNs. But of course noone outside of BD and Google really knows.,MachineLearning,d5c4t38,1,t5_2r3gv
8278735,0,t3_4ssm4z,ahh its one of the teachers from stanford CSn!,MachineLearning,d5ceflt,1,t5_2r3gv
8312283,0,t3_4qg8yq,I am creating a prediction model for daily water consumption for households using Neural networks in R. The input parameters would be  Past  days consumption past  days temperature min and max and the forecast day temperature.Suppose I want to create a model that will train on the past  years water consumption data once and then be used for daily water predictions for the next  years. I was thinking of setting a max error threshold at  for  consecutive days and when this is voided the model will retrain itself using the past  months data and then use this for further predictions which should hopefully have better accuracy than before. The reason being that peoples water consumption patterns are likely to change over the next few years due to water conservation methods and awareness plans or just people caring more about water a precious resource.Does this idea of retraining when the error crosses a threshold make sense?Or does it make sense when I have very little past data months and after a couple of years it will never need to retrain itself?Thanks!,MachineLearning,d5glc2c,1,t5_2r3gv
8312355,0,t3_4te5ys,First of all why use neural network?This is pretty straightforward regression problem so a NN seems like an overkill. At first you should check that there is some correlation in your data. Does the chosen parameter data depend on each other and how much? Then you could try to fit some function to your data even if it will be simple Least Squares regression. The least squares regression will perform poorly for longer prediction times. It could be accurate for few days but I doubt it. This problem is periodical in nature and the swings in water consumption during the year will likely break the LS regression model.There are much more powerful models for time series prediction ARIMA springs to mind. Good starting point could be this wiki page,MachineLearning,d5glnpd,2,t5_2r3gv
8313359,0,t1_d5gpf0i,That x is an estimate but probably using cuDNN because thats what everyone uses. Note that some estimates are up to x CPU speed for a GPU.A much bigger issue is that distributed training has significant overheads. It wouldnt surprise me if distributed training dropped the percore performance by  compared to on a single machine.,MachineLearning,d5gq5ua,2,t5_2r3gv
8333107,0,t1_d5j5nm8,Hey AFAIK they plan to publish a paper. I wrote the article. jackclarksf on twitter.,MachineLearning,d5j6zgn,5,t5_2r3gv
8355886,0,t3_4twoo0,This is awesome! Its pretty close to plain english but still detailed and critical of the subject.,MachineLearning,d5m1e8s,1,t5_2r3gv
8362759,0,t3_4u4tgd,Microsofts CNTK  runs on Windows.  If it doesnt have to be C I recommend Theano.  I use Theano in production on Windows servers and have found it to be very robust.Edit Just to add to this.  If you want to get started working with neural networks and experimenting try using a library such as Keras in Python.  Keras runs over the top of Theano or Tensorflow to help you build networks although only Theano is supported in Windows right now without having to worry about the very low level aspects of creating a network.  If you can program in C youll pick Python up in no time.,MachineLearning,d5mwab6,3,t5_2r3gv
8371020,0,t1_d5nf5rg,deleted,MachineLearning,d5nxeft,3,t5_2r3gv
8373475,0,t3_4qg8yq,I created a neural network and it seems to do a good job predicting. I am more interested in determining the relationship between the underlying input variables and the output variable but not using traditional regression methods. I used Oldens weighted connection method to find the relative importance of the the different input variables in the model.A lot of the variables that are important dont appear to have a strong linear relationship with the predicted variable just from eyeballing the scatter plot.I am looking for some suggestions as to what to do when this situation arises? If a variable is important in a model but in a way that is nonlinear and not easy to visualize  how does one go about exploring that?Thanks in advance.,MachineLearning,d5o8fzh,2,t5_2r3gv
8402219,0,t1_d5rohlv,Theyre fairly small since Im training them on a GTXM.For  hidden layers Im using from  to  feature maps for CNNs and  neurons for FC. layers  feature maps for CNNs  neurons for FC.,MachineLearning,d5rtnsy,1,t5_2r3gv
8402850,0,t1_d5rtjc7,Radial basis functions is the usual expansion.,MachineLearning,d5rwhx8,2,t5_2r3gv
8411734,0,t1_d5sgjtw,I am more keen to the  paper Computing Machinery and Intelligence. Note that at that time it is published  the dichotomy between logicbased or connectionist approaches hasnt been formed. People havent started to use words like artificial intelligence neural networks or machine learning as we do today.However the paper is sufficient to infer that Turing did not object to connectionists ideas while he certainly sees many limitations of logicbased approach already from some of his other papers. I suspect these limitations were the reason why he gave it up and start working on chemical and biological based machines which may have contributed to his death four years later  from cyanide poisoning.Here is one excerpt from that paper that seems interesting from the first paragraph of section  We also wish to allow the possibility that an engineer or team of engineers may construct a machine which works but whose manner of operation cannot be satisfactorily described by its constructors because they have applied a method which is largely experimental. I do not believe that in his mindset he is simply talking about the experimental research methodology. Knowing that at the time people do not have the concept of machine learning I would infer that the experimental approaches Turing refers to is something like the training phase of our current machine learning system whereas the goal Turing test is like the testing phase. In that sense Turing test can be understood as an early informal description of probable and approximately correctness one of the main ideas of todayss machine learning theory that describes generalization.,MachineLearning,d5t0eeq,2,t5_2r3gv
8416738,0,t1_d5tl2rd,Note the lack of any neural net.,MachineLearning,d5tmvyo,2,t5_2r3gv
8448939,0,t3_4vcu5d,Choosing appropriate hyperparameters is a completely open problem and most people will start from hyperparameters including the model itself that are known to work on a similar problem then modify. Modifications are mainly based on intuition.I would echo Andrej Karpathys advice in this general situation  it is good to see if you can overfit a model on a small subset of your training data. If it cant produce some kind of results then something definitely needs changing.,MachineLearning,d5xnkc0,2,t5_2r3gv
8466551,0,t3_4vnhyc,This much is obvious. Brains operate on memory. Loopy SpatioTemporal Memory to be exact.,MachineLearning,d5zuuyq,2,t5_2r3gv
8473945,0,t1_d60dx1h,NVIDIA Docker is good if you have a wellsupported system like Ubuntu or CentOS and fairly standard usecases. But Docker is supposed to be relatively hardware agnostic so outside of this you can run into trouble.If running nvidiadockeris somehow a problem but not installing it then you can also try using the nvidiadockerplugin REST API with docker.,MachineLearning,d60s91f,1,t5_2r3gv
8504322,0,t3_4w6tsv,How do you update your production models? What kind of items are on your checklist?,MachineLearning,d64lerk,-2,t5_2r3gv
8531832,0,t1_d67xvfs,Not yet but I am confident in buying things on PayPal. I run a few online businesses and have experienced firsthand how absurdly easy it is for people to get their money back via PayPal so I really have absolutely zero qualms about paying for anything via PayPal. I know if I get something thats not right I will get my money back.Just benchmark and stress test the card just to make sure its not a bad one but it really shouldnt be... unless someone ran it in an extremely hot environment but even then the card will just turn itself off if its in a bad situation.For short term rare training yes youd definitely want to do the math on how many hours you actually might be putting the thing to use and see how that compares.Its certainly not just running a GPU hosting company. Its anything where youre consistently training. Sure if its just for some quick research  months out of the year youd just rent in the cloud but if youre running a business doing ML for companies your own company or even many forms of research that are running  youll quickly find owning a used GPU is better and then just selling it a year later. The one thing I like about the cloud is the stressfree nature of it all plus you can scale quickly. That said AWS the previous main offering for GPU computing is absurdly expensive and you cant just quickly scale to huge portions they only have so much on reserve and you may actually be waiting for that scaling. I cant wait to see how AWS responds to this new Azure offering. I had not considered the whole public funding thing where you have to buy new. Good point there.,MachineLearning,d681kpl,1,t5_2r3gv
8555137,0,t3_4wwack,Just walk on them with bare feet.,MachineLearning,d6aytdk,0,t5_2r3gv
8559991,0,t3_4wydpq, for example is a machine learning startup that automates the process of analysing contracts there are many applications of machine learning in law,MachineLearning,d6bkqoq,1,t5_2r3gv
8564478,0,t1_d6c4inx,Youve misunderstood me and you continue to be put off by something. What is that thing?Who is your perceived public audience here? Who are the people hanging around in an ML forum who need to be told that not every scientist is a professor? I really do not understand what stance you think youre arguing against. ,MachineLearning,d6c4zm4,3,t5_2r3gv
8569302,0,t1_d6cjxlr,Im not saying youre wrong but I find this statement ironic because ML being a subfield of computer science essentially is a form of mathematics.,MachineLearning,d6cqs8f,-2,t5_2r3gv
8572182,0,t3_4x7l36,This is a side project of one of my colleagues. We had a dataset of outbrain titles and its associated clickthrough rate so he trained a small network to predict this rate.He previously tried to predict it with random forest but had a lower accuracy.,MachineLearning,d6d3ryk,2,t5_2r3gv
8591248,0,t3_4xh9qs,Step  download a bunch of virusesStep  remember not to click any of the files you downloaded,MachineLearning,d6fhv9n,3,t5_2r3gv
8604360,0,t3_4xj00x,Did he consider that the effects of ketosis display kurtosis?,MachineLearning,d6h50zf,0,t5_2r3gv
8605975,0,t3_4xnuv2,gt  And I havent seen any paper that showed very good improvements with using them on real world tasks. gt Will the utility of it be in such symbolic tasks only?In fact the attention mechanism can be seen as a readonly version of NTM or an endtoend version of memory network. And it works very well.Whats holding us back might be the memory. Designing a good data structure is hard.,MachineLearning,d6hcb74,4,t5_2r3gv
8627029,0,t3_4y0r8d,deleted,MachineLearning,d6jzddi,1,t5_2r3gv
8637320,0,t1_d6l03oq,gt Ive NEVER found Python to be a limiting factor to scale.Limited ML experience here but thats what I saw as well.  Its usually not difficult to push compute bottlenecks into NumPy or your own CC code and IO is usually limited by the hardware instead of the Python interpreter.,MachineLearning,d6l9ua9,5,t5_2r3gv
8640110,0,t1_d6ld3wf,Im sorry but what? Many prototypes are created before monetization just to see if an idea can be done. See research in every field ever,MachineLearning,d6lmflb,4,t5_2r3gv
8643378,0,t3_4y69um,I like the name feedless. FeelsGoodMan,MachineLearning,d6m16r8,1,t5_2r3gv
8643760,0,t1_d6l301w,Whats the relation of the paper with the PixelRNN in ICML ?,MachineLearning,d6m2wwf,1,t5_2r3gv
8678253,0,t1_d6qe96m,Thanks for explaining!gt which is why vgg would benefit more than a typical resnet with bottlenecksNot sure what you mean here. The original ResNet paper had all x convolutions except the first x and the Wide ResNets paper current best practice I believe is also all x.,MachineLearning,d6qelow,1,t5_2r3gv
8722068,0,t1_d6vqmlk,Yes they talk about differential privacy in the article.  Im still not very impressed with the dance theyre doing around the fine line between the data we store is anonymized and the data we use to train our models is anonymized.  That matters for things like the NSA requesting user info but is not very relevant to machine learning.,MachineLearning,d6vwdy5,1,t5_2r3gv
8731204,0,t1_d6whcgq,No binarycrossentropy assumes a single output in the domain ,MachineLearning,d6x1o57,1,t5_2r3gv
8737185,0,t1_d6xks96,Why not go for denoising autoencoders? At least you learn to denoise your data for the supervised part?,MachineLearning,d6xsp47,1,t5_2r3gv
8742246,0,t3_4zp2f5,Show him the error rates on common benchmarks particularly many class datasets. I was surprised by the fact that commonly reported ImageNet error rates were top when I first found out. Show him some ML APIs. Show him what you get when you put any reasonable length text into ,MachineLearning,d6yflv9,1,t5_2r3gv
8743619,0,t3_4ztc90,Even though the orals seem to be accessible only with a conference code ?!,MachineLearning,d6ylszh,3,t5_2r3gv
8753757,0,t1_d6zt6ja,Thank you. I wasnt aware. The main difference is that they appear to be upscaling from  images to . If I may say so that is a significantly simpler problem than upscaling from x to x.In a  pixel face theres a lot of information about small features such as the shape of the eyebrows nose and lips. In a x pixel face theres a lot more deduction to be made and that requires having much stronger priors.,MachineLearning,d6zvl2r,1,t5_2r3gv
8754595,0,t1_d6yoknw,Thanks for the AIXI papers. ,MachineLearning,d6zzd8z,1,t5_2r3gv
8760464,0,t1_d702owv,Did anything useful come from that Hessianfree paper? Last I heard people couldnt reproduce the results and it doesnt seem to have any recent citations.,MachineLearning,d70pvs6,1,t5_2r3gv
8764471,0,t1_d6z7yk6,is this the fastest way to do it?,MachineLearning,d717ye1,1,t5_2r3gv
8768299,0,t3_5065dv,,MachineLearning,d71p96l,1,t5_2r3gv
8795230,0,t3_50ivm9,Next blog post how a Peruvian longhaul trucker is using LSTM.,MachineLearning,d752fve,22,t5_2r3gv
8795886,0,t1_d752fve,How this Columbian coke lord is using GANs to create traffic across the border.,MachineLearning,d7559oq,19,t5_2r3gv
8810452,0,t3_50srsg,It is easy. TF on windows takes a lot of time to install and run. Theano and Keras has a good community. I work without GPU on Keras and have been running Convnets RNNs comfortably.. Install Conda full Anaconda . Python bitbit. Install  conda mingw libpython. pip install theano. pip install keras You are up and running in Windows without Ubuntu partitioning etc. . This all will take  minutes of your time. Cheers. ,MachineLearning,d76wad0,3,t5_2r3gv
8810818,0,t1_d76diqd,To have the ground truth be public is the rule not the exception for playground knowledge competitions. Lots of academic datasets also provide labels for the test set or require you to build your own testholdout pipeline.In a sense you are saying that it is kinda weird to run a marathon when you can use a car.,MachineLearning,d76xvfi,3,t5_2r3gv
8831753,0,t3_50zolk,I have a dumb question.From section  of the paper While a kiteration ARM k gt  is a multilayer network by itself the parameters of L and L are not independent. For example they both hinge on D in Eqn. . Furthermore L recurs k times with the identical parameters. Therefore the actual modeling power of an ARM is limited.  Fortunately this disadvantage can be overcome by stacking p ARMs each of which has k  di iterations i        p.  Why would stacking another copy of the network on top of itself even if its trained on the output of the previous network guarantee that we produce a different D matrix?  The nonlinearity from the regularization doesnt necessarily mean were going to pick a different D especially given the constraints on ||D||   right?  I mean it seems very likely but I cant figure out why it would be guaranteed.  In fact why bother stacking another layer on top of the original?  Is it just a notational convenience?  It feels like you could get away with more matrix products and sparsity operations in the same network for the same result?  Therefore the actual modeling power of an ARM is limited. ... by stacking p ARMs  Why not just do regularizeDregularizeDregularizeDa?  They mentioned a recursive formulation so how does stacking guarantee that they improve their baseline?,MachineLearning,d79gbma,1,t5_2r3gv
8842622,0,t3_51b2n5,Guys this this is live ,MachineLearning,d7arabm,20,t5_2r3gv
8851657,0,t1_d7b0rat,Leave it to that hacker to get into youtubes mainframe and mess everything up.  ,MachineLearning,d7budzc,2,t5_2r3gv
8851949,0,t3_51gp0y,Though  W  may contain numerous similar columns only one is chosen to represent the input. Ahh not quite  that would be maximal sparsity.  In general the amount of sparsity is variable and depends on the specific choice of sparse regularizer terms used and the typically approx optimizerencoder. What you call Regression Machine is just sparse coding.I sort of lost you at the first eq in the subgradient.  The nd one for the weight update looks right.  Of course you are glossing over the interesting part which is the encoder part the solution to the sparse coding problem.,MachineLearning,d7bvngu,5,t5_2r3gv
8853182,0,t1_d7byzwd,As an investment in research I think it is money well spent. Most research projects dont pan out. The probability of success is very low for any project. But with the recent advancements in computer vision and speech recognition I would say it is the least bad bet among a lot of bad bets.Let me put it this way. If you knew you could get a return on it then there would be no need for government support. The money is to help accelerate us toward that distant future  years off.,MachineLearning,d7c0zey,23,t5_2r3gv
8853545,0,t1_d7c263j,click TEAM,MachineLearning,d7c2jyg,8,t5_2r3gv
8855232,0,t1_d7bds10,A CNN is useful when the word order or a complex relationship between words needs to be captured complex an Ngram model isnt enough however we can see from the visualisations that the model is mostly capturing keywords and that makes sense since programming languages are based on keywords! I think a simple BOW could do the trick it wouldve been great if they compared to a BOW baseline.,MachineLearning,d7c9uzk,3,t5_2r3gv
8861046,0,t1_d7cw3ha,hahaha do you realize that Deep Learning was developed with public funding?? The Government of Canada is not doing anything new just repeating what was very successful in the past. That is the end of the discussion.,MachineLearning,d7cz15f,3,t5_2r3gv
8871177,0,t3_50t9jc,The fix is in! Thanks for your feedback weve moved those annoying social buttons to the bottom so the article should show just fine!,MachineLearning,d7e6v8t,1,t5_2r3gv
8871956,0,t1_d7e9txo,Attaching names to reviews is a possible way to fix it but keeping authors names blind.  More pressure to write a thorough thoughtful review if you know your name will be attached.,MachineLearning,d7ea8ho,6,t5_2r3gv
8878285,0,t3_51us2p,I would be wary of charging too much since this can lead to you being priced out by cloud services. ,MachineLearning,d7f1ml9,-2,t5_2r3gv
8878884,0,t1_d7f3rxc,The experiments on VGG are hard to parse. A lot of the intro material is somewhat readable potentially some of it novel. I dont get why people are questioning the acceptance of this paper the review process is not meant to catch fraud it would be impossible. Would you really have rejected this paper if you were a reviewer? I mean seriously what would your review be like recommending rejection? ,MachineLearning,d7f47xe,9,t5_2r3gv
8884726,0,t1_d7fmwp6,Poisson is a good option its only caveat is that the mean is also the variance for that distribution. The Negative Binomial NB is nice because the Poisson is a special case of the NB and doesnt have the meanvariance restriction.Itd probably be a fun exercise to code up the NB loss function.,MachineLearning,d7fthe6,2,t5_2r3gv
8884997,0,t1_d7fstyh,True but if they are a scientist of any merit they should value a thoughtful critique of their work.  I like to work with people that can cut through my BS and give deep critiques.    ,MachineLearning,d7funkt,1,t5_2r3gv
8893780,0,t1_d7gvt1g,Sure thats why I used bigger sample database and might  . We wont know until someone makes an experiment and I just wanted to say that it wont automatically be good.Besides if you want affordable classical concatenative systems are already good enough even for singing see e.g. this vocaloid song from  with several concatenative voices. If you want best currently it seems like you want WaveNet. Figuring out various tradeoffs between these two wont be that interesting I guess especially as Im pretty sure WaveNet will quickly become cheaperit is after all just the first attempt at using this architecture.,MachineLearning,d7gwm1h,1,t5_2r3gv
8900115,0,t1_d6dirok,And one month later deepmind has now come closer to making what you described here possible!,MachineLearning,d7hnzvf,1,t5_2r3gv
8903170,0,t1_d7hgysy,It came out in November . It doesnt really matter that it is old as lots of the material is still relevant and probably well taught.I imagine that this along with Andrews course are good foundations before doing the modern Stanford courses  .,MachineLearning,d7i16p6,2,t5_2r3gv
8911689,0,t1_d7irech,As a student doing this the slow way I am not bothered in the slightest. Most of it was hardly his spare time he worked hard and got results. I cant think of a reason to critisoze someone for that particularly when it was at times hr coding days. I also think this particular author highlighted how he isnt that great. My main takeaway was to not be afraid to push your boundaries with real work as a way of continued learning. He admits he isnt an expert just an individual who has enough knowledge to give a problem a go.,MachineLearning,d7j1zbh,2,t5_2r3gv
8911950,0,t1_d7hxxjt,ahem feedback propagation,MachineLearning,d7j33yh,1,t5_2r3gv
8916310,0,t1_d7jl2dg,Keep in mind that concatenating isnt the only option. If you take the point of view that the numeric output value of a feature map is proportional to the probability that that feature is present at a given location it makes sense to do elemwise sums means see FractalNets or even maxouts see Competitive Multiscale Convolution which allows you to do some level of gettingmultipledownsamplingmethods without having output dimensionality blow up at each hidden layer. You still incur the increased computational cost but it just doesnt stack between layers when you concatenate outputs.,MachineLearning,d7jlyrx,1,t5_2r3gv
8918370,0,t1_d7jtesl,There are definitely differences but obviously also huge overlaps between the feature sets of all these toolkits. PaddlePaddle allows for variable length sequences without padding in RNNs. I think thats pretty cool.,MachineLearning,d7juvrg,3,t5_2r3gv
8920371,0,t1_d7jd8dz,Can you pm your exploit tutorial? ,MachineLearning,d7k3jhl,2,t5_2r3gv
8922783,0,t3_52g7uy,gt What Epstein has done is conflated computer with computation. A mistake easily forgiven in a layperson not so much someone with Epsteins credentials.  Great writeup.,MachineLearning,d7kdytg,7,t5_2r3gv
8929533,0,t1_d7l6k7l,Are they extra new? I mean gb videochips were released last yearthis year. If people plan the budgets a year ahead they just bought these.,MachineLearning,d7l7589,1,t5_2r3gv
8935342,0,t1_d7lrpnn,deleted,MachineLearning,d7lw9mn,1,t5_2r3gv
8949454,0,t1_d7n5vq5,Thanks for your feedback . I think you are right I should have added baseline performance. Will add it soon. Goal of this project was to get familiar with these recent multimodal Neural Nets. and so I thought I will try out yesno type data only which can be easily extended to multiple categories. ,MachineLearning,d7nlazq,2,t5_2r3gv
8960501,0,t3_52zr5y,Weve just launched a new course on Tensorflow Creative Applications of Deep Learning with TensorFlow | KadenzeUnlike other courses this is an applicationled course teaching you fundamentals of Deep Learning with Tensorflow as well as stateoftheart algorithms by encouraging exploration through the development of creative thinking and creative applications of deep neural networks. Weve already built a very strong community with an active forum Github and Slack where students are able to ask each other questions and learn from each others approaches on the homework. I highly encourage you to try this course. There are plenty of GREAT resources for learning Tensorflow. But this is the only comprehensive online course that will both teach you how to use Tensorflow and develop your creative potential for understanding how to apply the techniques in creating Neural Networks.Course InformationThis course introduces you to deep learning the stateoftheart approach to building artificial intelligence algorithms. We cover the basic components of deep learning what it means how it works and develop code necessary to build various algorithms such as deep convolutional networks variational autoencoders generative adversarial networks and recurrent neural networks. A major focus of this course will be to not only understand how to build the necessary components of these algorithms but also how to apply them for exploring creative applications. Well see how to train a computer to recognize objects in an image and use this knowledge to drive new and interesting behaviors from understanding the similarities and differences in large datasets and using them to selforganize to understanding how to infinitely generate entirely new content or match the aesthetics or contents of another image. Deep learning offers enormous potential for creative applications and in this course we interrogate whats possible. Through practical applications and guided homework assignments youll be expected to create datasets develop and train neural networks explore your own media collections using existing stateoftheart deep nets synthesize new content from generative algorithms and understand deep learnings potential for creating entirely new aesthetics and new ways of interacting with large amounts of data.SCHEDULECourse runs July    December  Dates are suggested deadlines only and you can take the course at your own pace.  A new set of suggested dates will follow.Session  Introduction To Tensorflow July   Well cover the importance of data with machine and deep learning algorithms the basics of creating a dataset how to preprocess datasets then jump into Tensorflow a library for creating computational graphs built by Google Research. Well learn the basic components of Tensorflow and see how to use it to filter images.Session  Training A Network W Tensorflow August   Well see how neural networks work how they are trained and see the basic components of training a neural network. Well then build our first neural network and use it for a fun application of teaching a neural network how to paint an image and explore such a network can be extended to produce different aesthetics.Session  Unsupervised And Supervised Learning August   We explore deep neural networks capable of encoding a large dataset and see how we can use this encoding to explore latent dimensions of a dataset or for generating entirely new content. Well see what this means how autoencoders can be built and learn a lot of stateoftheart extensions that make them incredibly powerful. Well also learn about another type of model that performs discriminative learning and see how this can be used to predict labels of an image.Session  Visualizing And Hallucinating Representations August   This sessions works with state of the art networks and sees how to understand what representations they learn. Well see how this process actually allows us to perform some really fun visualizations including Deep Dream which can produce infinite generative fractals or Style Net which allows us to combine the content of one image and the style of another to produce widely different painterly aesthetics automatically.Session  Generative Models September   The last session offers a teaser into some of the future directions of generative modeling including some state of the art models such as the generative adversarial network and its implementation within a variational autoencoder which allows for some of the best encodings and generative modeling of datasets that currently exist. We also see how to begin to model time and give neural networks memory by creating recurrent neural networks and see how to use such networks to create entirely generative text.,MachineLearning,d7ox1s2,4,t5_2r3gv
8962123,0,t3_5306ns,deleted,MachineLearning,d7p41zg,1,t5_2r3gv
8972021,0,t1_d7qab45,Whoops yeah I meant per hour,MachineLearning,d7qausn,1,t5_2r3gv
8981637,0,t3_53aptf,deleted,MachineLearning,d7rgemk,1,t5_2r3gv
8993862,0,t1_d7sw37d,what is your question?,MachineLearning,d7sx8fk,1,t5_2r3gv
8998510,0,t1_d7tgxzu,I calculated all stats for all players and then through inference and trial and error I use different stats for different roles. For example supports do get the kills or cs stat.,MachineLearning,d7thcaz,1,t5_2r3gv
9020521,0,t3_53trd3,And the other one  Im curious if anyone could explain what FlowMachines actually are. Sony CSL mentioned style transfer but without details. Anyone attended the NIPS workshop and could share some info?,MachineLearning,d7w4it6,2,t5_2r3gv
9027133,0,t1_d7uztfu,gt Computer scientists are the ones that care how a human learns all these skillsThat would be cognitive scientists actually .,MachineLearning,d7wx46y,3,t5_2r3gv
9045383,0,t3_545qei,Ive got a python class that wraps an arbitrary function in a python process and any calls to the function batches the inputs and runs the network in batch then passes the result back to the original processes. Then I use a rest interface to handle requests.,MachineLearning,d7z3zgx,2,t5_2r3gv
9045855,0,t1_d7z4tt1,Why dont you use the reward defined in the paper? I dont think your rewards are Markovian. Rewards based on time are not really a good idea unless the state is parameterized by time and even then it makes the problem way harder because Qxyt ! Qxyt. Without this parameterization the agent has no concept of how long it is taking to find the prey. If it takes  steps to find the prey whatever it did would look exactly the same as taking  steps to find the prey. It would not know why it got lambda in one instance and lambda in the other. You should just make the reward a constant for catching the prey. Reinforcement learning should naturally encourage the predator to find the prey as quickly as possible. Edit Also your states might also be nonmarkovian. The agent should also know where it is so that it doesnt bump into walls.,MachineLearning,d7z611h,1,t5_2r3gv
9051192,0,t3_53t4h8,Hi. I currently work at a company Lumiata that does exactly this. My background is in medicine never practiced and computational modeling. Send me a pm if youd like to chat.  ,MachineLearning,d7zt550,1,t5_2r3gv
9057214,0,t1_d80caxy,I used a simple stack of bidirectional LSTMs as a baseline... In this case I prealigned the train pair with DTW. It works fairly well although the naturalness of the output its not great... With seqseq and attention it performs even better during training when Im using the ground truth equivalent to the true tokens in NMT. However when i try to use the models predictions instead to generate the sequence the error propagation is overwhelming for the network and it produces something that is not even perceptible. Using some kind of vector quantization might mitigate this issue but it can create audio artifacts as well...,MachineLearning,d80j5yh,1,t5_2r3gv
9057901,0,t3_54btqc,Convolutional neural networks CNNs,MachineLearning,d80m4rh,-2,t5_2r3gv
9067255,0,t3_54fw7g,removed,MachineLearning,d81qjyx,1,t5_2r3gv
9074734,0,t1_d82ff7y,It was quasiacademic but not hard science. Anything soft never gets a good response.,MachineLearning,d82mwhc,1,t5_2r3gv
9085287,0,t1_d80z5wc,How so? The premise of numpy is for efficient implementations of linear algebra operations not algorithmic implementations of models.Edit  Misinterpreted your comment . But a spline based library? If anyone knows of any please let me know!,MachineLearning,d83wjn9,1,t5_2r3gv
9087662,0,t1_d83ymrj,Hmm Id like to start off by saying Im definitely no expert on the matter however from what Ive been learning thus far Image classification does have tons of feature engineering steps. For example I spent the better part of last month learning about how linear classifiers can be trained on features computed from the raw pixels i.e. HOG to capture the texture of the image Color Histogram to capture color SIFT to detect edges etc. To your point about widthdepth increase giving marginal returns in performance  again that all depends on your train dev and test errors. If your model is suffering from high variance then getting more data maybe through some smart augmentation can be very beneficial and get you a large increase in performance. By the way I replicated the graph with some functions on my own so I may have exaggerated the gap between the different model sizes. My point is that I wasnt trying to be precise the graphs intent was more to capture this trend of bigger model with more data gt better performance. I dont have sufficient experience to answer your last question but it would certainly be interesting to know.,MachineLearning,d846v16,2,t5_2r3gv
9088583,0,t1_d83u9m5,Facebook Research might as well rename itself as Facebook Engineering if it continues to publish papers like this...,MachineLearning,d84aufs,1,t5_2r3gv
9092587,0,t1_d84pnh3,lol well you could use the ltmetergt tag P,MachineLearning,d84s6sc,1,t5_2r3gv
9113352,0,t1_d873rky,... speaking of making strong claims from anecdotal evidence...,MachineLearning,d87a0bs,2,t5_2r3gv
9120713,0,t1_d87hxa5,Statistical Physics deals with systems at or near equilibrium but they dont describe the trajectories of the dynamical systems as DST does. In particular DST is needed to understand how information or signals passing between interacting dynamical systems can be used to synchronise couple and control one or both of them and especially how embedding theorems can be exploited to automatically model and simulate one dynamical system inside another. These ideas are the main reason for the effectiveness of any recurrent network but they are entirely missing from the literature. ,MachineLearning,d885t2d,2,t5_2r3gv
9146752,0,t1_d8bdiew,Your problem is you need a scalar cost to optimize you cant train with respect to three outputs at once. So if your output is a vector that you hope will approximate some data you can set your cost to the mean squared error cross entropy or some other differentiable function of the output vector and ground truth target vector.If your vector is three probabilities from softmax and you have labels for the correct class you slice your output matrix with the vector of indices of the correct class using your librarys equivalent to numpys advanced indexing. Then you maximize your likelihood product of probabilities by minimizing the sum of the negative logs of those probabilities over your minibatch.,MachineLearning,d8bf0vt,2,t5_2r3gv
9155118,0,t3_54x09q,I have to put together a project for my entrance exams. In short its like doing a thesis but at a lower level finding novel thing is obivously not requiered but searching and learning and having a scientific mindset is. Its supposed to be finished in months or so.Id love to have machine learning taking a big place in it since Id like to work in that field later. At least in AI.What I thought could be feasible is to take a simple game easy to understand rules since Ill have in total mins to present it and implement a neural networks and probably finding weights with evolutiongenetic. I know its quite vague thats the start. Im currently reading about it every day and will implement the simplest things first. My report would include different methods to train one and how itd compare to other heuristics AI and players.What are the common pitfalls? Im sure Im not the first trying to use ML to create an AI for a school project. Any suggestion for first steps interesting games to look for?  I have more questions but theres already enough text Ill wait.,MachineLearning,d8ch6wq,1,t5_2r3gv
9169751,0,t1_d8dr5zo,Cool thanks for the info. I really had no idea theres been a shift in trends since Ive really mostly relied on publications at this point and havent had the opportunity to pick up on that yet.,MachineLearning,d8ec1lo,1,t5_2r3gv
9174133,0,t3_55xis6,Since a single game can vary a lot in appearance but has some distinctive features the HUD being one for instance or the characters a bag of words method could work.You sample descriptors densely or on interest point detectors or both on you rimages build a dictionary out of it with Kmeans. Then for a new image you sample the descriptors find their nearest neighbor in the dictionary and build a histogram with it if you have N words in the dictionary the histogram is thus a N vector. The histogram is what you try and classify with any other method.This is more handengineered but it can handle small details in a big image whereas your CNN will probably start from a small image where we hope distinctive features will still be there to be picked up during training.,MachineLearning,d8ew0ib,2,t5_2r3gv
9209440,0,t1_d8jcxpr,No reason to get upset. I agree that making it himself is subjective.However while running a precompiled executable shows you possible results its far from creating nor does it require any knowledge or understanding of what the program does. It is a good way to take baby steps though.Edit I played a video game the other day. Could I claim I made a game myself?,MachineLearning,d8jd5fa,1,t5_2r3gv
9218398,0,t1_d8j9ljw,Done example post here ,MachineLearning,d8khzw5,2,t5_2r3gv
9231956,0,t3_56st2s, deep learning is almost always a bad idea unless you know that there is structure in your data which you can architect a neural network to take advantage of.  if you havent architected information like this in a neural network will generally underperform compared to gradient boosting. its also a bad idea if you know something about your data  underlying model which deep learning doesnt match as well as another model e.g linearity or some other known interaction. its also bad if you are under time constraints and your chosen architecture will take too long to train. Example k class problem on  million text tokens. Naive bayes will train much faster probably just as good depending on the type of classes. when you dont have very much data youre going to overfit while something linear or a random forest or SVM will have less of a chance when you dont know wtf youre doing you can waste WEEKS or MONTHS playing around with neural nets with subpar results and have no clue as to the fact if youre a noob while someone skilled can walk in with linear regression or a random forest and smoke you in a matter of hours. . Ive seen this happen A LOT.,MachineLearning,d8m7uwg,8,t5_2r3gv
9245178,0,t3_56059z,I have been reading a Information Theory tutorial and it has formulas for calculating the entropy of both a discrete and continuous distributions. Can entropy be calculated when we dont know the underlying distribution of a sample? I know that one could use kernel density estimation but that feels too heuristic.,MachineLearning,d8nw6kd,1,t5_2r3gv
9276360,0,t1_d8rl42d,Its the same setup they use in the paper adopted for x. Test error at . with RMSPropdefaults.,MachineLearning,d8rufft,2,t5_2r3gv
9329456,0,t1_d8vtx8c, link is broken. Could you fix it? Id like to read that! Thanks.,MachineLearning,d8ykrly,1,t5_2r3gv
9329502,0,t1_d8w3l3r,I agree  there are some other articles on that site that are wellvisualized.,MachineLearning,d8ykz4x,1,t5_2r3gv
9366323,0,t3_58s02z,Especially for something like recycling which is really costsensitive I think some questions to consider are To what level of specificity must you classify materials? Is it enough to separate metals  glass  plastics? Or are you better off working within one of those categories? e.g. separating goldplatinumetc from cheaper metals At what part of the materials  recycling handling process would you implement this? AFAIK many recycling plants only deal in a particular class of material ewaste scrap industrial metal paper etc. So you might think about if you should work at that level or upstreamdownstream from that.Intuitively I suspect that the hardest part wont be deciding on ML models but moreso deciding on which implementation can lead towards a feasibletoimplement final result in terms of dollars saved.Anyway I think the core idea fine all that said. The research group that Im a part of deals with a lot of similar stuff so its certainly a valid approachproblem.,MachineLearning,d938yis,5,t5_2r3gv
9374711,0,t3_58xjq2,udacity deep learning ,MachineLearning,d94b7gy,-4,t5_2r3gv
9438225,0,t1_d9a2g7k,Id rather stay in the US make tons of money and advance state of the art AI further than Canada has...,MachineLearning,d9cd0xv,1,t5_2r3gv
9440478,0,t1_d9bv2y6,,MachineLearning,d9cnb52,2,t5_2r3gv
9443598,0,t3_575jz8,This is a terrible idea. Its going to further fragment an already niche subreddit. ,MachineLearning,d9d1ka4,5,t5_2r3gv
9474861,0,t1_d9gxck3,deleted,MachineLearning,d9gzbuw,1,t5_2r3gv
9504457,0,t1_d9kf1ly,deleted,MachineLearning,d9kmkqf,1,t5_2r3gv
9512131,0,t1_d9lk8gl,Why is that better than the number of apples simply bring the largest index?,MachineLearning,d9lkm93,2,t5_2r3gv
9518562,0,t1_d9lzsf6,removed,MachineLearning,d9md5zp,1,t5_2r3gv
9586070,0,t1_d9ul33b,It will be great if you can show that an evolved agent on one set of levels can generalize and play another set of levels that it has never seen before.  That has been one of the criticisms of GA in the previous work.What is nice about Flappy Bird is due to the simplicity every game is unique and randomized.  The evolved agent is forced to generalize well to the environment and not memorize a sequence of predetermined pipe locations to win.,MachineLearning,d9uog37,8,t5_2r3gv
9588978,0,t1_d9uvnql,Thank you for answering. It seems to me that you were very close to doing some feedback on the sensorimotor rhythm which is often mixed or overlaps the alpha range. SMR based neurofeedback is known to work well for BCI and other applications I guess you were on the right track. Further if you are able to separate and remove other sources that contaminate your parietal or central SMR you could get better results. In this case in addition to artifacts removing the strong alpha power from parietal sources. Followup curiosity question what headset and electrode type did you use ?,MachineLearning,d9v1csm,1,t5_2r3gv
9601415,1,t1_d9wk89o,how many programmers do you know that use assembly languages? each new abstraction layer each new programming language makes harnessing computational power easier and more accessible. its a naturalinevitable evolution. there will be programmers but they wont know how code or logic works. theyll be house wifes and children giving vauge commands that layers of abstraction will turn into viable code. no other field has as many automation experts working in it as computer science. ,MachineLearning,d9wkk3q,-2,t5_2r3gv
9601703,0,t1_d9wl78e,this nonprogrammer programmer reality is absolutely post agi.as for ui there are already projects that aim to automate the process of a human usable interface. imo it will be one of the less challenging aspects of automated programming.,MachineLearning,d9wlu4p,4,t5_2r3gv
9615700,0,t3_5cksd8,Why is he using Python . and not Python .?,MachineLearning,d9ybzr1,1,t5_2r3gv
9632838,0,t1_da00frk,gt I also think that it is obvious that regularisation techniques are only responsible for small increases in generalisation.Dropout is pretty successful at this no? I mean hard to argue rigorously over what counts as small increases but it feels to me like dropout is capable of substantial increases in generalization when training data is scarce.,MachineLearning,da0g1j2,2,t5_2r3gv
9633803,0,t3_5cwfb6,I do not agree with cluelessscientist in the following aspects I do not agree that the paper is somewhat pointless simply because it weights coevolution features so high. This paper has pointed out that the depth of the network is also very important in addition to coevolution information. The deep network structure can improve the accuracy by more than . over MetaPSICOV a method using a network of only  hidden layers.The reason why the deep architecture works is that it can capture well protein contact occurring patterns which is information orthogonal to coevolution.This is further confirmed by the fact that the method outperforms the pure coevolution methods even when proteins in question have a very large number of sequence homologs not to mention that on average the method approximately doubles the accuracy of the pure coevolution methods. Although deep learning has been tried on contact prediction starting from  but previous methods have not shown any significant advantage over a shallow network.As far as I know this paper is the first one showing that deep learning actually works very well on protein contact prediction much better than a shallow network.Further this paper uses a very different network architecture than previous methods.,MachineLearning,da0kc3g,4,t5_2r3gv
9652653,0,t1_da2o70z,,MachineLearning,da2w1ky,3,t5_2r3gv
9666644,0,t1_da4kdks,But you couple this  with speechtotexts error rate and...,MachineLearning,da4m78x,8,t5_2r3gv
9689672,0,t1_da79n60,iirc they were UIUC kids,MachineLearning,da7ggy6,2,t5_2r3gv
9698185,0,t3_51kgiy,Great post. The notes really helped me in learning and training GANs. What I first realized after reading your post was the pre training for my D was over trained!  My main takeaways from your notes are observation of the loss and gradients different learning rates for the D and G use of lekyrelu for D batch norm for G. Thanks for taking the time to write it up!,MachineLearning,da8iai1,1,t5_2r3gv
9711460,0,t3_5e1q8o,The answer is its hard to say.. Model based RL and model free RL are solving the same high level problem but the actual functions that they are learning are very different. I dont think there is a definite way to say which function is easier to learn for any given domain but you can certainly construct examples for each domains. For some problems learning a model is just way more complex than learning a good policy whereas for others the model may be nice and smooth while the optimal policyQ function is making a ton of jumps. I dont think either is true for atari games in general. Traditionally model based RL methods are much more sample efficient and current RL methods employed on these kind of games generally arent at all. Even something as complex as video prediction may end up being more efficient if it can be made to work.,MachineLearning,daa5a8i,1,t5_2r3gv
9720596,0,t1_dab9snl,Interesting. But shouldnt the moving average affect your prediction at subsequent timesteps? I cant see how it will affect the predictions if you dont send it as an input to the LSTMAlso any reason why you just chose to store the negative moving average of all of the previous distributions and not anything more?,MachineLearning,dab9wvq,2,t5_2r3gv
9749786,0,t3_5er9qe,Extremely cool project! Would be interesting to put a language model on top.,MachineLearning,daevi7w,3,t5_2r3gv
9772807,0,t1_daggttm,Thanks! The LVAE paper seems really interesting. But can you elaborate a bit on the part that structured mean field still isnt enough? I guess if NN can separate XOR via increasing and then decreasing dimensions we should be able to generate distributions with complex modes via DLGM...,MachineLearning,dahpqnf,1,t5_2r3gv
9784903,0,t3_5fb49m,Afaik transfer learning is more about transfering features than transfering the ability to learn them.  The features you learned from task A are used for task B. This is interesting for many reasons  there is no overfitting on B since Bs model doesnt even see As data. It also suggests that the features are somehow universal since they also help for a task they werent designed to solve.Prelearned features help in many situations    When you dont need a endtoend system and would prefer fast training. Example  pretrained word vectors.   When you dont want to overfit.   When you have a lot of data for task A but not much for task B.   ...Maybe it helps,MachineLearning,daj7atn,4,t5_2r3gv
9794420,0,t3_5ffgts,C tipThe use of pragma once in your headers sample is redundant with the header guard I suggest you to remove pragma once since it is not part of the standard. More info,MachineLearning,dakdhdu,2,t5_2r3gv
9822537,0,t3_5fvt2o,deleted,MachineLearning,danwacc,1,t5_2r3gv
9823209,0,t1_danq86s,deleted,MachineLearning,danzg2s,2,t5_2r3gv
9897880,0,t3_5h6aio,OMG not ... its .It will be presented at a workshop tomorrow.,MachineLearning,daxptsi,1,t5_2r3gv
9901547,0,t1_daxx1b2,,MachineLearning,day70iq,1,t5_2r3gv
9947877,0,t1_db46541,arXivGitHub  What a great concept! Thanks for the link.,MachineLearning,db48fbm,3,t5_2r3gv
9963831,0,t3_5i23wt,I wonder if you could build a higher stack  GANs for increased resolution. Is there a ceiling you gonna hit Training time doesnt converge ...?,MachineLearning,db6baeh,1,t5_2r3gv
9965485,0,t1_db6baeh,GPU requirements?,MachineLearning,db6j1j4,1,t5_2r3gv
9968351,0,t1_db6vcgs,Good point I dont think the Nash equilibrium argument actually considers these added penalties.,MachineLearning,db6whyw,1,t5_2r3gv
9994006,0,t3_5iof4c,deleted,MachineLearning,dba8wb5,-1,t5_2r3gv
10057597,0,t1_dbic984,I was going to add a link to Detexifier too to But on reading further I think what is being attempted here is something conceptually akin to a math OCR followed by a reverse mapping to LaTeX which makes it suitable candidate for deep learning hierarchical feature mapping. Unlike detexifier it works on typeset math which is much more regular not mousedrawn or handwritten math. However it seems to be a software port of an existing scheme imlookup Deng et al to the Tensorflow platform so it would seem there is no new technology here.,MachineLearning,dbij2cs,1,t5_2r3gv
10061342,0,t1_dbic984,Oh wow! I always wanted something like this. Would it be possible to translate a picture into LaTeX with this?,MachineLearning,dbj0ml7,1,t5_2r3gv
10065588,0,t1_dbiv70q,Been in EA for half a million years? Ive backed games like that for sure...,MachineLearning,dbjkial,3,t5_2r3gv
10095157,0,t3_5kdgru,intractable inference,MachineLearning,dbnex0e,2,t5_2r3gv
10111381,0,t3_5knlk4,Go check out the python pandas package and its documentation see for usage example here most kernels of kaggle competition the ones not using deep learning for image recognition  especially line  the pd.merge operation is probably what you are looking for.,MachineLearning,dbpitx3,2,t5_2r3gv
10137402,0,t1_dbsi2zt,Thats what I said lol. So what is the link above? It says its a network that predicts saliency. That doesnt make sense lol.,MachineLearning,dbswoll,1,t5_2r3gv
10140763,0,t1_dbsy85f,I suppose impossible is too strong a word but I know of a couple of groups that tried and gave up. In our lab at least there were many hypotheses about why it wouldnt work e.g. its due to weightsharing or its due to threeway interaction etc. We suspected there was some fundamental incompatibility.,MachineLearning,dbtcgig,1,t5_2r3gv
10165529,0,t3_5j3525,I have an idea  I will make a bet that you fail to convince any state to release the data in a useful form.  ,MachineLearning,dbwbhis,1,t5_2r3gv
10178490,0,t1_dbxr8o7,Yes this is similar to the use of embeddings in the MILA paper on kaggle taxi predictions referenced by ukkastner in another comment.,MachineLearning,dbxsdnt,1,t5_2r3gv
10180856,0,t3_5lq48b,The wording is a bit confusing. By increase in dimensions the paper is referring to the increase in depth number of channels not the spatial size. The alternatives to match depth are by adding padding layers or via x convolutions with number of filters matching desired output depth. Decrease in spatial size is handled by using stride  convolutions.,MachineLearning,dby21da,3,t5_2r3gv
10181470,0,t1_dbwltbe,No you wouldnt write code like that usually. The keras version uses loops etc for repeating blocks the slim version does not for no good reason you can do it. The inception structure pretty much asks for having functions to build the different building blocks but neither of those two do it ... first thing I did when playing around with inceptionv in TF was refactor that function and also make it not be dependant on anything like the tf.app.flags ...,MachineLearning,dby4jr1,1,t5_2r3gv
10209982,0,t1_dc1cvhz,does somebody need the wambulance,MachineLearning,dc1cwxq,-1,t5_2r3gv
10233618,0,t1_dc2tgqu,What is free will? Free from what  physical laws? The concept of free will is based on dualism the idea that spirit is some kind of separate reality from matter and free from material constraints. But dualism is flawed as even philosophers figured out not just AI researchers.What you call free will I call a neural network with injected noise. Just because we cant find an easy deterministic cause of an action doesnt mean it is free from causality.,MachineLearning,dc41dkd,1,t5_2r3gv
10263820,0,t3_5mydx8,why? when would anyone actually use this for anything. even if the other  headeronly NN libraries didnt already exist.,MachineLearning,dc7gkeh,-1,t5_2r3gv
10270111,0,t3_5n2hm2,Title Estimation and Inference of Heterogeneous Treatment Effects using Random Forests  Authors Stefan WagerfindstatauWagerSall Susan AtheyfindstatauAtheySall  gt Abstract Many scientific and engineering challenges  ranging from personalized medicine to customized marketing recommendations  require an understanding of treatment effect heterogeneity. In this paper we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breimans widely used random forest algorithm. In the potential outcomes framework with unconfoundedness we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge this is the first set of results that allows any type of random forest including classification and regression forests to be used for provably valid statistical inference. In experiments we find causal forests to be substantially more powerful than classical methods based on nearestneighbor matching especially in the presence of irrelevant covariates.  PDF link,MachineLearning,dc8690i,3,t5_2r3gv
10276787,0,t1_dc8x96n,TIL   GTX  sufficient?    Also how significant is the speedup?,MachineLearning,dc8xgux,2,t5_2r3gv
10291669,0,t3_5ndjf6,This is some really excellent work! ,MachineLearning,dcam79k,11,t5_2r3gv
10303671,0,t3_5njryo, ipython notebook ,MachineLearning,dcbz5u4,3,t5_2r3gv
10351086,0,t3_5nlzp3,Cant remember many details but the one from ages ago where a outdoor selfdriving bot is fed a D camera with labelling of the dataset as traversable based on the height of objects which I think was helped by laser scan rather than processed out of the D camera as in typical CV approach learning that part.,MachineLearning,dchctge,1,t5_2r3gv
10353428,0,t3_5o31jb,Can we download the dataset somewhere for own experiments?,MachineLearning,dchmdyp,1,t5_2r3gv
10376701,0,t1_dck7tib,In a very simple highly contrived case lets say you have feature A and feature B.Feature A can take on values   and B can take on   .If you take a subset of the data where all of your examples have a value of  for Feature A it is calibrated. So the average predicted expectation on all your examples for A will be . At least thats how I read it.,MachineLearning,dck9gno,3,t5_2r3gv
10381990,0,t1_dck5zrk,your comment about recursive networks isnt quite right.  if you want nonlinear access and to use theano  you have to do some fairly intense things.  examples SPINN and neural tree grammars Jacob Andreas also has neural model networks which leverages ApolloCaffe a caffe branch that caches layers to allow for dynamic use sacrificing certain optimizations and not something theano or tensorflow couldnt do.,MachineLearning,dckv2i5,1,t5_2r3gv
10395272,0,t3_5ou9as,removed,MachineLearning,dcmdcmm,1,t5_2r3gv
10395942,0,t1_dcm22i8,I see didnt know that. In which dynamic framework you see such speed up and compared to which one?,MachineLearning,dcmg2wo,4,t5_2r3gv
10405760,0,t1_dcm6xp4,Interesting.  Thanks for the insight!,MachineLearning,dcnk6j8,1,t5_2r3gv
10408154,0,t1_dcn8ic7,I dont think so in the case where the defects which occur in the future may look totally different than the defects which were present in the training data. ,MachineLearning,dcntyqo,4,t5_2r3gv
10418071,0,t1_dcmwnps,This discussion was interesting. The approximation does not have a great justification. The stepsize should be chosen related to the magnitude of H such as the spectral norm. Effectively approximating H is the goal of quasiNewton approaches. In general it is expensive and difficult to approximate H. Firstorder methods use a much coarser approximation with a firstorder Taylor series expansion  O|| x  y|| . You can then imagine that there is simply some constant in front of ||xy|| to reflect the magnitude of this approximate term and then solve as you would if you really had the Hessian.,MachineLearning,dcoyg44,1,t5_2r3gv
10427914,0,t3_5pan11,How can they prove their results are useful and better than simple methods?Simple fingerprint similarity searches using the active compounds from their training set on the PubChem data would probably also find a lot of active cancer drugs. Do they actually gain anything?,MachineLearning,dcq2mnp,8,t5_2r3gv
10460075,0,t3_5ptqix,Title Bringing Impressionism to Life with Neural Style Transfer in Come Swim  Authors Bhautik Joshi Kristen Stewart David Shapiro  gt Abstract Neural Style Transfer is a striking recentlydeveloped technique that uses neural networks to artistically redraw an image in the style of a source style image. This paper explores the use of this technique in a production setting applying Neural Style Transfer to redraw key scenes in Come Swim in the style of the impressionistic painting that inspired the film. We document how the technique can be driven within the framework of an iterative creative process to achieve a desired look and propose a mapping of the broad parameter space to a key set of creative controls. We hope that this mapping can provide insights into priorities for future research.  PDF link  Landing page,MachineLearning,dctr7b9,1,t5_2r3gv
10461034,0,t1_dctu39j,In the internship I had only one server with one GPU so yeah.,MachineLearning,dctv4qy,2,t5_2r3gv
10466058,0,t1_dcu4vbo,Just for anyone wondering a human is around  billion synapses.But on the other hand computers are around  million times faster. ,MachineLearning,dcufn6c,3,t5_2r3gv
10498352,0,t3_5qbjz7,Line counts are a bit inapplicable no? Is it to create clickbait for people that dont comprehend the relevance of line numbers?,MachineLearning,dcy3keb,67,t5_2r3gv
10518076,0,t3_5qlshp, for considering buzzfeed ToI as clickbait. Can you write a tutorial model building of your project for novice like me to understand?,MachineLearning,dd0c5sr,15,t5_2r3gv
10583532,0,t3_5re2pw,I think its somewhat important to understand why its harder than the other two types of learning. In RL and deep Q learning especially you have a temporal aspect to contend with. I train the network using values estimated from the network. This means convergence can be harder to achieve. And its predecessors are worthwhile reads as is David silvers course on YouTube. It can seem tempting to just throw a neural network at the issue but I found really focusing on the fundamentals and learning about function approximation in a more general sense quite helpful. ,MachineLearning,dd7lyns,3,t5_2r3gv
10602333,0,t1_dd9imu1,Yeah it seems to fall short of its rather lofty goal pretty quickly.Its pretty clear now that if we want to work out the learning algorithms the brain is using examining it at a systems DeepMind andor neurophysiological level Numenta is far more fruitful.At any rate thanks for the exchange.,MachineLearning,dd9jstm,0,t5_2r3gv
10619294,0,t1_ddb97xv,Some papers use differential geometry specifically Riemann manifolds geodesics Lie groups and for  differential geometry you need to know basics of differential topology  manifold definition tangent bundle etc and if you doing deep learning Whitney embedding theorem is essential for understandingConvex analysis on top of functional analysis help too.  ,MachineLearning,ddbat4v,3,t5_2r3gv
10637056,0,t1_ddcuxzm,Damn I looked all over and must have missed that one. Glad to see Im not the only one effected. I was hoping for a cleaner solution but if it works it works. Thanks!,MachineLearning,ddd4pv7,1,t5_2r3gv
10648399,0,t1_dde84ak,Havent heard good news about this guy.,MachineLearning,ddeati7,-21,t5_2r3gv
10701222,0,t1_ddjof1v,Thanks for the feedback!We cannot reproduce the NaN error usingth main.lua dataset cifar bottleneckType resnextC depth  baseWidth  cardinality  weightDecay e batchSize  nGPU  LR . nThreads  shareGradInput trueThe code has been tested on CentOS Ubuntu . CuDNN . CUDA.. Nvidia Tesla Titan X and M on  GPUsI feel like your experience might be somehow machinedependent. Were you be able to run fb.resnet.torch code without a problem? Im also collecting feedbacks from other users. Feel free to let me know if anyone has similar issues.,MachineLearning,ddjr2df,1,t5_2r3gv
10773232,0,t3_5u341a,Most of them require some setup and if you want GPU with that prepare to wrestle your Linux system to the ground in the process.There is a nifty online handwriting demo at  and if you look at some of the prebaked stuff there is some absolute hilarity out there. One of the lines that really sold me on deep learning was the following even though I didnt end up pursuing languagebased models MMMMM Recipe via MealMaster tm v.       Title BEEF MARINADE Categories Dips      Yield  Servings           Bay leaf       tb Unsweetened chocolatechopped           Salt and pepper Chopped       ts Powdered sugar       ts Salt       tb Cornstarch       ts Cinnamen chopped celery       tb Chopped fresh basil       tb Cornstarch     c  Water       ts Baking powder     ts Salt       ts Salt   In a mixing bowl beat butter until smooth.  Drain on both sides of the refrigerator. In  a lightly floured bowl beat the eggunsalted water   minutes. Mix together the water and the  oil in a mediumlightly blender. Make a little and cooked or all  to the cherry and ice cream and slice the crumbs and fresh sauce.    From Electric this unpeelerset Beans Kinch Bar Suet Brightor  Nanc and Information To you by Depth Chefs copyrighted by  ISBN  Microwave Optional Collection of Shellies MMMMMDrain on both sides of the refrigerator had me rolling for several minutes. The whole thing is a gas I dare you to try to read it out loud to a few buddies might be a good competition idea there. Try to read the recipes as seriously as possible no smiling etc. allowed. The mock severity alone can cause lots of laughter and the content makes it harder ,MachineLearning,ddr6if6,0,t5_2r3gv
10775918,0,t3_5u37t4,How did you come up with key researchers? looks quite arbitrary at places.And how come Recurrent Neural Network Models has only  papers when they are like half of modern DL??,MachineLearning,ddrgi8w,5,t5_2r3gv
10783208,0,t1_ddquz6p,But whats the state of the art tool set for the labeling?  OpenRefine? Excel? ,MachineLearning,dds7jpm,1,t5_2r3gv
10796832,0,t1_ddt9zmp,Trying out another loss function might be a good idea. Could help me see if the loss function is the bottleneck. sounds pretty good. What kind of input did you use and how did your network look?,MachineLearning,ddtm6xm,2,t5_2r3gv
10809247,0,t3_5ulf4n,Anyone have any experience of this?,MachineLearning,dduwbki,1,t5_2r3gv
10852793,0,t3_5tz5y9,Arent walkers the usual solution for solving this problem? I havent read it yet but I imagine the results are similar.,MachineLearning,ddze07k,1,t5_2r3gv
10889139,0,t3_5vkn2c,How will this coexist with TensorFlow Fold? Its scary to invest in one of these new repos as it feels like Google could pull the plug at any time.,MachineLearning,de34y6o,1,t5_2r3gv
10925255,0,t1_de4ox08,I dont think this is what he asked for but still interesting.,MachineLearning,de6v3ve,1,t5_2r3gv
10927291,0,t3_5w3q74,Ive been meaning to do a project in tensorflow so I can make a candid threeway comparison between TheanoLasagne PyTorch and Tensorflow but I can give some rambling thoughts here about the first two. Background I started with TheanoLasagne almost exactly a year ago and used it for two of my papers. I switched over to PyTorch last week and have reimplimented two of my key current projects which were previously in Theano.API The way Theanos graph construction and compilation works was a bit of a steep learning curve for me but once I got the hang of it everything clicked this took about two months but I was still learning python and basic neural net stuff so take that with a grain of salt. Lasagnes API to me is elegant as Catherine the Great riding an orca into battle which is to say I love it to death. Ive always said that its the library I would write if I knew ahead of time exactly how I wanted a theano topper library to work and it drastically eases a lot of the gruntwork.PyTorchs API on the other hand feels a little bit more raw but theres a couple of qualifiers around that which Ill get to in a moment. If you just want to do standard tasks implement a ResNet or VGG I dont think youll ever have an issue but Ive been lightly butting heads with it because all I ever do is weird weird shit. For example in my current project Ive had to make do with several hacky workarounds because strided tensor indexing isnt yet implemented and while the current indexing techniques are flexible theyre a lot less intuitive than being able to just use numpystyle indexing. The central qualifier about the is that they literally just released the friggin framework of course not everything is implemented and theres still some kinks to work out. Theano is old and wellestablished and I wasnt really around to observe any of its or Lasagnes growing pains. Newness aside my biggest complaint with pytorch is basically that things arent put together the way I would have put them together on the neural net API side. Specifically I really like Lasagnes layers paradigmbut a little bit of critical thinking should lead you to the conclusion that that paradigm is specifically and exactly unsuited to a dynamic graph framework. Im completely used to thinking and optimizing my thought processes around static graph definition so making the switch APIwise is a minor painpoint. This is really criticalIve spent so long thinking about Okay exactly how would I define this graph in Theano because I cant just write it as I would a regular ole program with my standard flow control that Ive become really strong in that avenue of thinking. Dynamic graphs however necessitate an API which is fundamentally different from the definerun and while I personally dont find it as intuitive in the last week alone the ability to do definebyrun stuff has as CJ said opened my mind and given me half a dozen project ideas which previously would have been impossible. I also imagine that if you do anything with RNNs where you want to say implement dynamic computation time without wasted computation the imperative nature of the interface is going to make it a lot easier to do so.Speed So I havent done extensive benchmarks but I was surprised to find that PyTorch was out of the box  faster at training time than theanolasagne on singleGPU for my current project. Ive tested this on a  and on a Titan X with two implementations of my network which I have confirmed to be identical to within a reasonable margin of error. One. Hundred. Percent. Literally going from in the simplest case  minsepoch to . minsepoch on CIFAR and in some cases going down to  minsepoch i.e. more than twice as fast.This is with identical boilerplate code using identical data fetchers I cant unironically say fetcher without thinking DIE FETCHER! identical everything else other than the actual code that trains and runs the network.This surprised the hell out of me because I was under the impression that Theanos extensive and agressive memory optimizations which in this case you pay for with several minutes of compilation time when you start training meant that it was crazy fast on single GPU. I dont know what leads to the improved speed either because theyre both using the latest version of cuDNN Ive explicitly checked to make sure this is so so all those gains must be in the overhead somewhere but sweet christmas I have no idea where.Relatedly Ive never been able to get multiGPU or halfprecision floats working with theano ever. Ive spent multiple days trying to get libgpuarray working and Ive tinkered a bit with platoon but each time Ive come away exhausted assuming I can even get the damn sources to compile which was already a pain point. Out of the box however PyTorchs dataparallelism single node  GPUs and halfprecision pseudoFP for convolutions which means its not any faster but it uses way less memory just...worked. I was stunned by this as well.Dev Interactions My interactions with the core dev teams of both frameworks have been obscenely pleasant. Ive come to the Lasagne and Theano guys with difficulties and questions about weird stuff many many times and theyve always promptly and succinctly helped me figure out what was wrong usually what I didnt understand. The PyTorch team has been just as helpfulIve been bringing up bugs or issues I encounter and getting nearimmediate responses often accompanied by sameday fixes workarounds or issue trackers. I havent worked in Keras or in Tensorflow but I have taken a look at their Issues dockets and some usergroups and just due to the sheer volume of users these frameworks have it doesnt look like its possible to get that kind of individual attentionit almost feels like Im going to Cal Poly where the facultystudent ratio is really high and you rarely have any more than  students in a class while looking over at people in a  people lecture hall at Berkeley. Thats not at all to condemn the Cal kids or imply in any way that the analogical berk doesnt work but if youre someone like me whos into  nonstandard neural net stuff were talking Chuck Tingle weird then having the ability to get quick responses from the guys who actually build the framework is invaluable.Misc The singular issue Im worried about and why Im planning on picking up TensorFlow this year and having all three in my pocket is that neither Theano nor PyTorch seem designed for deployment and it doesnt look like thats a planned central focus on the PyTorch roadmap though I could be wrong on this front I vaguely recall reading a forum post about this. Id like to practice deploying some stuff onto a website or droid app mostly for fun but Ive been crazy focused on research and I think it would be a real useful skill to be able to actually get something I made onto a device and Im just not sure that the other frameworks support that quite as well.Relatedly PyTorchs distributed framework is still experimental and last I heard TensorFlow was designed with distributed in mind if it rhymes it must be true the sky is green the grass is blue brb rewriting this entire post as beat poetry so if you need to run truly largescale experiments TF might still be your best bet. TLDR Im not really trying to recommend one framework over another I love Lasagne to death and beyond but Ive been finding that the flexibility of dynamic graphs and the sheer incomprehensible speed gains Ive been getting with PyTorch just in the last week alone and with very little relative time invested into learning the framework mean that Im making the switch and Im not likely to look back. I dont know much about TensorFlow yet but the individual attention I can get from the pytorch devs is a big point for me as I look to do weird researchy stuff but Im also likely to pick up tensorflow for some projects later in the year. This post is pretty rambly but hopefully if youre reading it you can pick up some impressions. Please take this for what it is my experience not a hardandfast this is how it is you will definitely feel the same way.,MachineLearning,de72nnr,170,t5_2r3gv
11001902,0,t1_dedla5k,I agree with uVelveteenAmbush. Also by throttling upgrades they are also throttling use for machine learning which they have to know is going to be a HUGE market in the foreseeable future. ,MachineLearning,def17oe,2,t5_2r3gv
11015573,0,t1_deg9gui,OriolSchmidhuber P,MachineLearning,degl1ly,8,t5_2r3gv
11038279,0,t3_5xk2vr,While this doesnt answer you question directly I would recommend two ML books which straddle the boundary between formalism and practical applications. They both make a compelling effort to show how one relates to the other.. Introduction to Statistical Learning. Deep Learning Book,MachineLearning,dej644w,3,t5_2r3gv
11060175,0,t1_deljoo2,Though Ive learnt from this thread that  works really well tested in on a few articles.,MachineLearning,delnijy,2,t5_2r3gv
11068580,0,t1_demm17h,Thanks I appreciate that! ,MachineLearning,demm2xx,1,t5_2r3gv
11095732,0,t1_deotf4c,Yeah definitely a good idea! ,MachineLearning,depowl8,1,t5_2r3gv
11149424,0,t3_5z72m9,That seems like something very helpful indeed !,MachineLearning,devrx2z,2,t5_2r3gv
11155916,0,t1_dewhn1e,I think this should be with the source filtered.I think its the total tag count but only applied to tags that are used on danbooru    dbmirror select        tid.tagid        tid.tagcount        dbtags.tag    from        dbtags        INNER JOIN                    SELECT                distincttagsid AS tagid                count AS tagcount            FROM                dbtagslink            WHERE                releasesid IN                                    SELECT                        id                    FROM                        dbreleases                    WHERE                        dbreleases.sourceDanbooru                                GROUP BY                tagsid            ORDER BY                count DESC            limit         AS tid ON tid.tagid  dbtags.id         tagid | tagcount |         tag               |    | girl          |    | solo          |     | long hair         |     | highres         |     | short hair           |     | blush          |     | breasts          |     | smile          |     | multiple girls          |     | blonde hair         |     | blue eyes          |     | open mouth          |     | brown hair          |     | hat         |     | skirt          |     | thighhighs         |     | red eyes          |     | girls         |     | black hair        |     | bad id          |     | looking at viewer          |     | ribbon          |     | twintails          |     | school uniform         |     | boy          |     | brown eyes          |     | underwear         |     | hair ornament           |     | bow         |     | gloves          |     | dress         |     | monochrome          |     | blue hair         |     | translated          |     | green eyes          |     | panties           |     | animal ears          |     | large breasts         |     | comic         |     | sitting         |     | navel         |     | cleavage          |     | weapon         |     | simple background         |     | closed eyes         |     | nipples         |     | purple eyes          |     | hair ribbon         |     | jewelry         |     | purple hair         |     | pink hair         |     | ponytail          |     | very long hair         |     | wings         |     | hair bow         |     | yellow eyes          |     | tail          |     | bare shoulders          |     | green hair         |     | shirt          |     | red hair        |     | absurdres         |     | swimsuit         |     | flower         |     | silver hair         |     | hairband         |     | white background          |     | glasses         |     | black legwear         |     | pantyhose         |     | braid          |     | ass         |      | translation request         |      | boots         |      | detached sleeves          |      | one eye closed         |      | nude          |      | ahoge        |      | japanese clothes          |      | artist request          |      | multiple boys         |      | barefoot         |      | necktie          |      | serafuku         |      | tears        |      | heart         |      | lying          |      | sword         |      | censored        |      | male focus         |      | bikini         |      | girls         |      | food         |      | midriff         |      | white hair          |      | d          |      | long sleeves         |      | sweat         |      | shoes          |      | striped     rowsObviously I need more subqueries.,MachineLearning,dewif21,1,t5_2r3gv
11163192,0,t3_5z8110,gt Ph.D. in Mathgt any job I wantgt k starting,MachineLearning,dexc3v8,2,t5_2r3gv
11164487,0,t1_dex61ag,Agreed. Im waiting for the day a good computational framework is released. Until then Julia is relegated mostly to my not deep learning work.,MachineLearning,dexhe3s,1,t5_2r3gv
11164733,0,t3_5zetab,Source code Ive learnt about it from a discussion thread Reading arXiv preprints on an ereader?  rMachineLearning  HT uklarh.,MachineLearning,dexie72,1,t5_2r3gv
11171557,0,t1_dexuhgp,The shattered gradients paper got things to work with CReLUs Wmaxx  Wmaxx but not with PreLUs that start out linear. I think this paper can explain why.Starting out linear would help with the vanishing gradient but not with breaking symmetry. Subtracting the two blocks from the CReLUs will anchor them to each other thus breaking the permutation symmetry because exchanging two units in one block will no longer produce an equivalent representation. ,MachineLearning,deya8bi,3,t5_2r3gv
11177677,0,t1_dexh1gp,Original DQN paper appears to have been trained for  million frames and the DQN results from the AC paper were originally taken from  which doesnt seem to say,MachineLearning,deyz6qi,1,t5_2r3gv
11230552,0,t1_df4fvsu,Are you already in a PhD program or going to apply soon?? What you said about failure and correction is totally true. I think the freedom to fail in academia is something that I like I a lot. If you fail in a company youll get fired. If you fail at something in academia you learn from it and retry again. As for me I like both theory and application. What I dont like is the idea of being  years behind the cuttingedge research implementing some algorithm so I can help some sleezy company sell stuff to people. P ,MachineLearning,df4yv2l,1,t5_2r3gv
11266840,1,t3_60sier,Anyone else notice the giant kill switch on the center console? lol,MachineLearning,df92vuy,-2,t5_2r3gv
11268241,0,t1_df951ga,presence ! location,MachineLearning,df98lbr,1,t5_2r3gv
11303782,0,t1_dfd11pe,My knowledge of evolutionary algorithms is genetic algorithms and this seems to be different. In GAs you have a population of solutions which you improve through crossover and mutation. Here it seems like they have one solution. Each iteration they generate a large pool of mutated candidates. Then they move the solution toward the better candidates.Overall seems very expensive computationally which is not different from traditional GAs but easily scalable to a large number of cheap computers. In other words you need many more computations than backpropagation but each is much cheaper.,MachineLearning,dfd9m9v,5,t5_2r3gv
11345939,0,t1_dfi15fo,Wouldnt that be a tough comparison since it would partially depend on the expertise of a person at training a particular architecture.,MachineLearning,dfi1ixq,1,t5_2r3gv
11349001,0,t1_dffh7gh,Thanks for the really nice reply! Thats totally fine the internet is a big place really... its hard to know whats going on in different corners  Ill send you a message now.,MachineLearning,dfidzs0,2,t5_2r3gv
11355698,0,t3_622cqd,Title Towards Evaluating the Robustness of Neural Networks  Authors Nicholas Carlini David Wagner  gt Abstract Neural networks provide stateoftheart results for most machine learning tasks. Unfortunately neural networks are vulnerable to adversarial examples given an input x and any target classification t it is possible to find a new input x that is similar to x but classified as t. This makes it difficult to apply neural networks in securitycritical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network and increase its robustness reducing the success rate of current attacks ability to find adversarial examples from  to ..   gt In this paper we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with  probability. Our attacks are tailored to three distance metrics used previously in the literature and when compared to previous adversarial example generation algorithms our attacks are often much more effective and never worse. Furthermore we propose using high confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.  PDF link  Landing page,MachineLearning,dfj5bhk,1,t5_2r3gv
11360234,0,t1_dfjjyn6,You can minimize a scalar function of the Jacobian and never form a gigantic secondorder jacobian matrix.Still bottleneck will be Jacobian computation. Nn frameworks theano tensorflow does not support higherorder derivatives. So Jacobian will be computed for each output unit in cycle approximate slow down N output units,MachineLearning,dfjntvf,1,t5_2r3gv
11393094,0,t3_62dud6,Jrgen Schmidhuber pronounce youagain shmidhoobuh pun intended,MachineLearning,dfne31a,3,t5_2r3gv
11409497,0,t3_62ro8x, in the future we will test on more complex datasets like MNIST ,MachineLearning,dfp9512,26,t5_2r3gv
11458318,0,t3_63jwog,Aiight! Alma Mater represent!,MachineLearning,dfusjdr,2,t5_2r3gv
11484450,0,t3_63vfst,Can some body tell me what is the eligibility for Google Brain Residency program? Do we require mastersPhD in math stats comp sci etc or candidates from any other background can send their application.Is google brain residency program only available for those living in america legal citizen or worker ?,MachineLearning,dfxre3v,1,t5_2r3gv
11498238,0,t1_dfz6rjr,gt I hope your joking. Anything would be an improvement over Keras. A literal pile of trash would be better than Keras.gt ouchalso maybe get off that high horse? Different tools for different needs personally Im all for different levels of abstraction. Would you say a web programmer is a wannabe cryptography researcher because they use TweetNaCl.js? I dont know jack about web dev so I googled that library but Im going to assume it proves my point perfectly. You know I think most researchers dont give a shit about what libraries anyone uses they probably actually care about the research. Maybe then again that might be because theyre mentally older than a twelve year old with a superiority complex.,MachineLearning,dfzbpim,5,t5_2r3gv
11510184,0,t1_dfzrqmb,I think it is more about the fact that to predict the next characters and therefore words in a sentence it helps to know the intended sentiment of the sentence? If the last few sentences were all personal attacks than the next sentence is much more likely to contain more words related to personal attacks.Not quire sure what youre hinting at beyond that.,MachineLearning,dg0oix2,1,t5_2r3gv
11510517,0,t1_dg0ccq9,It just means that theres something that effects the state transition that isnt immediately local.  For example chess is an MDP if the entire board is the state  but starcraft is a POMDP because most of the board is hidden until you explore it.  I think that people usually frame it as a POMDP.  ,MachineLearning,dg0pvus,3,t5_2r3gv
11513727,0,t1_dg0jn3l,Oh Im not saying that I expect the music to literally sound like a Bach chorale. Its just that I hear some really really weird artifacts in the output that are quite puzzling to me.  For an indication of what we might expect from a nearSOTA model trained from Bach chorales alone and yet arguably not trying to create something that is Bachlike but more like an original music composition see the BALSTMjsb files from here which relate to Daniel Johnsons tied parallel networks model.  You are right that getting anything remotely musical out of these models is pretty difficult but ISTM that we are on a wholly different level here.  As an aside that model does seem to generate both pitches and durations in a closedloop way.,MachineLearning,dg12zh7,2,t5_2r3gv
11513903,0,t1_dg123f4,Yes that could be implemented within a database and the annotation software UX but each of these features I listed above are not trivial to implement e.g.  ability to revert a single older commit without reverting all future changes and have not been implemented in any package that I know of I could even argue that doing them properly would take more work than implementing the actual annotation core youd need to write half of git to get there.E.g. version control on single DB records works if each single DB record is something thats atomic and semantically meaningful to user  like customer data in a CRM app. Its not a good fit for annotation data. If your DB record is an annotated sentence sometimes done for reasons explained later then youd need to write diffpatchmerge code for that data which is hard. If your DB record is a single annotation atom e.g. a single label or a single link then often a single user change a click in editor will affect multiple atoms so you need to figure out how to link them together. Furthermore dumping it to a file when needed generally is always  any and all actual use of the data is going to be outside the system by some other tools the DB or files are only for the human annotation workflow but the majority of code interacting with it wouldntcouldnt use a DB meaningfully anyway youd either have some efficient inmemory data structure or a feature extraction process that would mangle it all to a bunch of vectors. If youre exchanging data people theyre going to be using other tools and thus semistandard semireadable flat file formats work much better than e.g. DB dumps. And you want the ground truth data format to be independent on that annotation system since youll want to mix that data with similarbutdifferent data from other sources e.g. automatically annotated data. Also relation databases are a really really poor fit for many styles of annotation. E.g. you can encode something like  to RDFtriplet like structures for graph edges but its a pain to decodeencode. For simpler data e.g.  you can get all kinds of benefits if you encode the actual column structure in the database but then youre locked in to that structure and need a change if youd e.g. add a named entity annotation layer to it and the people buildingmaintaining an open source tool for annotation wont do an adjustment for every particular group of users so a table structure for data like that anyway tends to look like sentenceid text jsondatablob.Id argue that if youre treating each fragment of annotated text as an opaque blob of jsonxmlwhatever you dont get much benefits from a proper DB youd need more work to add differences branching for public releases history and blame ability to revert an older change without reverting all future changes to a DB rather than write the file management  especially because youd need to write the serializationdeserialization code anyway for file importexport to DB. For client code if all youre ever doing is select  from data and transforming it for your needs then you might as well do git pull and run the same transformation you just get to reuse githubgitlabbitbucket UI for version management instead of writing half of the same thing from scratch.Okay Ill stop ranting probably my point is that if somethings not done using a proper database its not because of lack of trying but because of a multitude of practical reasons. Often actually by implementing it with a database first because DBs are simple everybody knows them and theyre the default approach but then throwing it away and rewriting it differently.,MachineLearning,dg13pce,1,t5_2r3gv
11577172,0,t1_dg7zsfs,Sorry for the length. We want to break it up into multiple parts next time we do a post this large.,MachineLearning,dg8a1hd,4,t5_2r3gv
11617521,0,t1_dgcpq4o,Bigger batches hurt generalization source? I had been taught bigger batches worked better. Also couldnt it just be a noisy dataset?,MachineLearning,dgcuu6d,16,t5_2r3gv
11627857,0,t3_65wrdp,This is interesting thanks for sharing! Im wondering how locality as approach compares to depersonalization  differentiated privacy. ,MachineLearning,dge139x,2,t5_2r3gv
11639006,0,t3_63ue4v,deleted,MachineLearning,dgfam45,1,t5_2r3gv
11671481,0,t1_dgiuqcp,The most common beginner level applications of machine learning are usually done on toy datasets. The Iris dataset is perhaps the most common. If you want something more advance theres the MNIST dataset  for number recognition. Otherwise you can selfassemble things such as facial recognition a bit more advanced  youd have to either do a sliding window or similar or text recognition.Generally Id recommend starting with MNIST if this is your first crack at it  the numbers is already centered and theres easy tweaks you can perform to get better recognition such as rotation the image leftright to get numbers that arent perfeclty updown.,MachineLearning,dgiz89q,1,t5_2r3gv
11718765,0,t1_dgochho, Audiobook reading with your favourite voice Cartoons Videogame voicesThis was just a small scale demo but we will open the API for everyone soon.,MachineLearning,dgocn0k,20,t5_2r3gv
11747154,0,t3_67lgvw,Would neural Turing machines count?,MachineLearning,dgrkiep,8,t5_2r3gv
11755580,0,t1_dgsczme,See my edit,MachineLearning,dgsiw97,3,t5_2r3gv
11762316,0,t1_dgsxo47,Features should be scaled when using SVMs too. Featuring scaling is generally a good idea for most applications like this.,MachineLearning,dgtadk0,1,t5_2r3gv
11764875,0,t3_67ufwo,Word movers distance has worked for me,MachineLearning,dgtku17,12,t5_2r3gv
11780043,0,t1_dgv65cu,These tutorials are great Small Nit You should recommend TFServing for inference. Its going to be much more performant than a Python API for a number of reasons It can batch multiple API requests into a single forward pass of the graph Doesnt force data through the python layer. gRPC supports streaming and is generally going to be lower latency than something like a Flask server.If you do use TF Serving Graph freezing from checkpoints will not for exporting instead you should use tf.savedmodel example code here  Disclosure I work at Google on TF but not TF Serving.EDIT Added some details and clarification.,MachineLearning,dgvb0dh,15,t5_2r3gv
11790473,0,t1_dgw3dw0,Thats why a lot of people use Keras instead of raw TF.,MachineLearning,dgwhn5v,2,t5_2r3gv
