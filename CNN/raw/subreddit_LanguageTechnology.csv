,controversiality,parent_id,body,subreddit,id,score,subreddit_id
2403263,0,t3_2mvznv,It may be difficult to keep syntactical sentences using only POS tagging. I have seen some work where sentenced were compressed by pruning the parse tree it might be a direction that you want to explore.I suggest you search for sentence compression on Google Scholar if you have not already done so ,LanguageTechnology,cm8r33j,2,t5_2rkr2
2790970,0,t1_cnkoyur,If by acronyms you mean e.g. U.S.A. I would suggest the rule that a period after a single alphabetic character never indicates the end of a sentence. No manually defined vocabulary is then necessary. A rare exception to that is You and I.  Perhaps my definition of rules differs from yours?,LanguageTechnology,cnlbn7v,1,t5_2rkr2
3346694,0,t3_2yg3fx,You might try using MEAD It incorporates LexRank as one of its features in deciding on summaries.  Of course MEAD may struggle with summarizing a single email as it was tuned for multidocument summarization.,LanguageTechnology,cp9qyjp,2,t5_2rkr2
4921325,0,t1_cue4cw9,Ok I will search more about regression model instead then I also looked that with similar problems in training model they dropped the  stars reviews and postulate that lt stars was negative and gt stars a positive review but then would only classify in . or . stars or  and  and never would show up the  star or even the  stars or also divide train set in  smaller datasets drop the  stars again and then run a training for  or  stars where  star is negative and  is positive and then another run for  and  stars where  is negative and  is positive but this aproach doesnt seems very quite suitable to me what you think?,LanguageTechnology,cuevdq8,1,t5_2rkr2
4929662,0,t1_cuf4f5t,Just a small hint. Youll probably want to use at least bigrams. Otherwise reviews like this product is not good will break your model and be mistrained. ,LanguageTechnology,cufx90n,1,t5_2rkr2
5349436,0,t3_3o8j9x,deleted,LanguageTechnology,cvvg8h6,1,t5_2rkr2
7082516,0,t3_4cl0wu,Your challenge comes down to training vs. classification imo. You could train on known rumor tweets but that sounds like moving backward to me. Or at least using too much effort to do the exact same thing but it might be the only way.Or you could classify what makes something a rumor. Its a very good exercise to ask yourself how you work.Ask yourself What makes something sound like bullshit?. Already sort of a hard question but Id start with things called weasel words. Like studies show or etc. There are a lot of things that people say when they are unsure of something or when they have little evidence and they begin to conflate the sentence structure with a load of crap. That might be indication for a rumor but its sort of barbaric. It might end up with all of the results following a rigorous structure based on how the sentence is formed. My concern is essentially    bundled together but there is hope because of no.  the way I see it. If you validate your predictions using the news or CDC info regarding where a threat has been located you can put weights on the system and alter the rumor detection module this way. My suggestion because of how complicated rumors are more than people simply spreading bullshit sometimes they believe what they are saying and there is no way to tell from their language that they are wrong is to train a ton of data on situations that fall under step . Collect massive amounts of data and find out which ones were true.Analyze those that were false for rumorlike language and qualities. Also maybe think about supervision. You may notice things that a computer wont about the way the tweets are written amount of adjectives etc. You can build multiple models of the language that work together once youve trained on a lot of real conclusions.Also research rumors and human behavior. Worth noting that people who are obviously scared are more likely to report information too early or without evidence. Things like that about human nature will be worth learning about and making a list of.,LanguageTechnology,d1j637i,2,t5_2rkr2
7771255,0,t3_4lp5vp,It would help if you further refined the task. Are you looking for entailment? Check out any of the models here For sentiment analysis or something similar? Try a recursive architecture  The original paper is here It looks like the live demo is down but the front page of this course has a picture of it working In general if you want to handle negations a tree structure has what you want. It can handle things like negation because it will join not with the whole to be negated clause.If you really want an outofthe box solution then AFAIK you should either use paragraph vector as hdgdtegdb said or you could try MetaMind which youll have to pay for. You may get better results if you roll your own though.,LanguageTechnology,d3pj28w,3,t5_2rkr2
9726907,0,t1_dac1n1p,Often Java  many companies license Stanfords CoreNLP library which is open source and GNU licensed so your free to mess around with it for personal or research stuff.That said Ive found that most things are proprietary and built inhouse. A good understanding of the algorithms that the NLTK implements is certainly more important that familiarity with the NLTK itself.,LanguageTechnology,dac1z3f,2,t5_2rkr2
10786717,0,t3_5u7lui,These algos already exist as standard PyPI packages at least all I checked soundex fuzzy metaphone. Some of them even seem presumably rather efficient C implementations. What was your reason to reinvent them?,LanguageTechnology,ddskldy,1,t5_2rkr2
11176988,0,t1_deyuq4l,And I have already edited the post in case of more misunderstanding. Thanks.,LanguageTechnology,deywdhr,1,t5_2rkr2
11269039,0,t1_df93epq,deleted,LanguageTechnology,df9bujv,1,t5_2rkr2
